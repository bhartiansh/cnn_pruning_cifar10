{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNR+2QGGJ4SZPDlq41Eb6eG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhartiansh/cnn_pruning_cifar10/blob/main/pruning1(lth).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bhartiansh/cnn_pruning_cifar10.git\n",
        "%cd cnn_pruning_cifar10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpAQGoNN2oos",
        "outputId": "aef27b43-7dcc-43f3-aaad-015eb0f463c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cnn_pruning_cifar10'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 50 (delta 14), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (50/50), 21.16 KiB | 1.17 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/cnn_pruning_cifar10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)  # Optional: For debugging\n",
        "\n",
        "# Confirm eager execution is on\n",
        "print(\"Eager Execution:\", tf.executing_eagerly())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Xyg_IaGO5R",
        "outputId": "bbba1c07-8f0a-4900-fdd5-2d633967e9a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager Execution: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from models.resnet56_baseline import build_resnet56\n",
        "from data.cifar10_loader import load_cifar10_data\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "MzJqhJf52qnC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "CHECKPOINT_PATH = './checkpoints/lth_resnet56_cifar10.keras'\n",
        "FINAL_MODEL_PATH = './models/lth_resnet56_cifar10_final.keras'\n",
        "INITIAL_WEIGHTS_PATH = './models/initial_resnet56_weights.npy'"
      ],
      "metadata": {
        "id": "9A6ZKa66_o2u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "INITIAL_WEIGHTS_PATH = './models/initial_resnet56_weights.npz'\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "def save_initial_weights(model):\n",
        "    weights = model.get_weights()\n",
        "    np.savez(INITIAL_WEIGHTS_PATH, *weights)\n",
        "    print(\"Initial weights saved.\")\n",
        "\n",
        "def load_initial_weights(model):\n",
        "    data = np.load(INITIAL_WEIGHTS_PATH)\n",
        "    weights = [data[f'arr_{i}'] for i in range(len(data.files))]\n",
        "    model.set_weights(weights)\n",
        "    print(\"Initial weights loaded.\")"
      ],
      "metadata": {
        "id": "lRJT4C6Z_spL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_model(model, pruning_fraction=0.2):\n",
        "    weights = model.get_weights()\n",
        "    pruned_weights = []\n",
        "\n",
        "    for w in weights:\n",
        "        if len(w.shape) > 1:  # Prune only weights, not biases or BN params\n",
        "            k = int(np.prod(w.shape) * pruning_fraction)\n",
        "            threshold = np.partition(np.abs(w.flatten()), k)[k]\n",
        "            mask = np.abs(w) > threshold\n",
        "            w = w * mask\n",
        "        pruned_weights.append(w)\n",
        "\n",
        "    model.set_weights(pruned_weights)\n",
        "    print(f\"Model pruned with {pruning_fraction * 100:.1f}% sparsity.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "ydFG2SCI_xbR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lth_model():\n",
        "    train_gen, val_gen = load_cifar10_data(batch_size=64)\n",
        "    model = build_resnet56(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "    if not os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        save_initial_weights(model)\n",
        "    else:\n",
        "        load_initial_weights(model)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train initial model (or load pre-trained weights)\n",
        "    print(\"Training initial/pruned model...\")\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=CHECKPOINT_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    # Load best weights and prune\n",
        "    model.load_weights(CHECKPOINT_PATH)\n",
        "    load_initial_weights(model)  # Reset to initial weights\n",
        "\n",
        "\n",
        "    #change for change in sparsity[0.4,0.6,0.8]\n",
        "    model = prune_model(model, pruning_fraction=0.2)\n",
        "\n",
        "\n",
        "\n",
        "    # Recompile and retrain the pruned model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Re-training pruned model from initial weights...\")\n",
        "    history_pruned = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    model.save(FINAL_MODEL_PATH)\n",
        "    print(\"LTH pruned model saved.\")\n",
        "\n",
        "    # Load initial weights if available\n",
        "    if os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        load_initial_weights(model)\n",
        "    else:\n",
        "        save_initial_weights(model)"
      ],
      "metadata": {
        "id": "jcpDHNYc_zf9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_lth_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdOVtJIh_1Ta",
        "outputId": "7c0024c3-ce09-4739-9e8a-532c60f7c689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights saved.\n",
            "Training initial/pruned model...\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3438 - loss: 1.9568\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45920, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 57ms/step - accuracy: 0.3439 - loss: 1.9563 - val_accuracy: 0.4592 - val_loss: 1.6808\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5653 - loss: 1.2008\n",
            "Epoch 2: val_accuracy improved from 0.45920 to 0.59130, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.5654 - loss: 1.2008 - val_accuracy: 0.5913 - val_loss: 1.1975\n",
            "Epoch 3/30\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6581 - loss: 0.9663\n",
            "Epoch 3: val_accuracy improved from 0.59130 to 0.67750, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.6581 - loss: 0.9662 - val_accuracy: 0.6775 - val_loss: 0.9265\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7100 - loss: 0.8223\n",
            "Epoch 4: val_accuracy did not improve from 0.67750\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7100 - loss: 0.8223 - val_accuracy: 0.6367 - val_loss: 1.0984\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7442 - loss: 0.7304\n",
            "Epoch 5: val_accuracy improved from 0.67750 to 0.75750, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.7442 - loss: 0.7304 - val_accuracy: 0.7575 - val_loss: 0.7108\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7713 - loss: 0.6626\n",
            "Epoch 6: val_accuracy did not improve from 0.75750\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7713 - loss: 0.6626 - val_accuracy: 0.6931 - val_loss: 0.9397\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7851 - loss: 0.6130\n",
            "Epoch 7: val_accuracy did not improve from 0.75750\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7851 - loss: 0.6130 - val_accuracy: 0.7539 - val_loss: 0.7461\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8113 - loss: 0.5426\n",
            "Epoch 8: val_accuracy improved from 0.75750 to 0.80480, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8113 - loss: 0.5426 - val_accuracy: 0.8048 - val_loss: 0.5790\n",
            "Epoch 9/30\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8177 - loss: 0.5265\n",
            "Epoch 9: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8177 - loss: 0.5265 - val_accuracy: 0.7645 - val_loss: 0.7103\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8303 - loss: 0.4858\n",
            "Epoch 10: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8303 - loss: 0.4858 - val_accuracy: 0.7660 - val_loss: 0.7400\n",
            "Epoch 11/30\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8381 - loss: 0.4643\n",
            "Epoch 11: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8381 - loss: 0.4643 - val_accuracy: 0.7439 - val_loss: 0.8211\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8492 - loss: 0.4365\n",
            "Epoch 12: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8492 - loss: 0.4365 - val_accuracy: 0.7550 - val_loss: 0.8247\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8539 - loss: 0.4193\n",
            "Epoch 13: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8539 - loss: 0.4193 - val_accuracy: 0.8046 - val_loss: 0.5835\n",
            "Initial weights loaded.\n",
            "Model pruned with 20.0% sparsity.\n",
            "Re-training pruned model from initial weights...\n",
            "Epoch 1/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.3562 - loss: 1.9760\n",
            "Epoch 1: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 53ms/step - accuracy: 0.3563 - loss: 1.9755 - val_accuracy: 0.4938 - val_loss: 1.4123\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5769 - loss: 1.1771\n",
            "Epoch 2: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.5770 - loss: 1.1770 - val_accuracy: 0.5134 - val_loss: 1.6327\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6719 - loss: 0.9368\n",
            "Epoch 3: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.6719 - loss: 0.9368 - val_accuracy: 0.6874 - val_loss: 0.9140\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7185 - loss: 0.8032\n",
            "Epoch 4: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7185 - loss: 0.8032 - val_accuracy: 0.6173 - val_loss: 1.2990\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7513 - loss: 0.7102\n",
            "Epoch 5: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7513 - loss: 0.7102 - val_accuracy: 0.7040 - val_loss: 0.8915\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7793 - loss: 0.6333\n",
            "Epoch 6: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7793 - loss: 0.6333 - val_accuracy: 0.7108 - val_loss: 0.8744\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7974 - loss: 0.5837\n",
            "Epoch 7: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7974 - loss: 0.5837 - val_accuracy: 0.7420 - val_loss: 0.7948\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8134 - loss: 0.5430\n",
            "Epoch 8: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8134 - loss: 0.5430 - val_accuracy: 0.7472 - val_loss: 0.7758\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8234 - loss: 0.5072\n",
            "Epoch 9: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8234 - loss: 0.5072 - val_accuracy: 0.7294 - val_loss: 0.8556\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8357 - loss: 0.4738\n",
            "Epoch 10: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8357 - loss: 0.4738 - val_accuracy: 0.7962 - val_loss: 0.5898\n",
            "Epoch 11/30\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8427 - loss: 0.4503\n",
            "Epoch 11: val_accuracy did not improve from 0.80480\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8427 - loss: 0.4503 - val_accuracy: 0.7882 - val_loss: 0.6607\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8533 - loss: 0.4194\n",
            "Epoch 12: val_accuracy improved from 0.80480 to 0.81990, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.8533 - loss: 0.4194 - val_accuracy: 0.8199 - val_loss: 0.5785\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8622 - loss: 0.4003\n",
            "Epoch 13: val_accuracy did not improve from 0.81990\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8622 - loss: 0.4003 - val_accuracy: 0.7883 - val_loss: 0.6624\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8673 - loss: 0.3851\n",
            "Epoch 14: val_accuracy did not improve from 0.81990\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8673 - loss: 0.3851 - val_accuracy: 0.8129 - val_loss: 0.5679\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8742 - loss: 0.3613\n",
            "Epoch 15: val_accuracy did not improve from 0.81990\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8742 - loss: 0.3613 - val_accuracy: 0.8129 - val_loss: 0.5837\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8806 - loss: 0.3429\n",
            "Epoch 16: val_accuracy did not improve from 0.81990\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.8806 - loss: 0.3429 - val_accuracy: 0.8060 - val_loss: 0.6061\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8850 - loss: 0.3322\n",
            "Epoch 17: val_accuracy improved from 0.81990 to 0.82260, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.8850 - loss: 0.3322 - val_accuracy: 0.8226 - val_loss: 0.5570\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8905 - loss: 0.3153\n",
            "Epoch 18: val_accuracy improved from 0.82260 to 0.84600, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.8905 - loss: 0.3153 - val_accuracy: 0.8460 - val_loss: 0.4925\n",
            "Epoch 19/30\n",
            "\u001b[1m781/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8953 - loss: 0.3008\n",
            "Epoch 19: val_accuracy did not improve from 0.84600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8953 - loss: 0.3008 - val_accuracy: 0.7966 - val_loss: 0.6880\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8982 - loss: 0.2912\n",
            "Epoch 20: val_accuracy did not improve from 0.84600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.8982 - loss: 0.2912 - val_accuracy: 0.7559 - val_loss: 0.8942\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9028 - loss: 0.2736\n",
            "Epoch 21: val_accuracy did not improve from 0.84600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.9028 - loss: 0.2736 - val_accuracy: 0.8193 - val_loss: 0.5829\n",
            "Epoch 22/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9077 - loss: 0.2639\n",
            "Epoch 22: val_accuracy did not improve from 0.84600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.9077 - loss: 0.2639 - val_accuracy: 0.7928 - val_loss: 0.7249\n",
            "Epoch 23/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9083 - loss: 0.2580\n",
            "Epoch 23: val_accuracy did not improve from 0.84600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9083 - loss: 0.2580 - val_accuracy: 0.8365 - val_loss: 0.5293\n",
            "LTH pruned model saved.\n",
            "Initial weights loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparsity40"
      ],
      "metadata": {
        "id": "mp-jSNoWuSdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lth_model_sparsity40():\n",
        "    train_gen, val_gen = load_cifar10_data(batch_size=64)\n",
        "    model = build_resnet56(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "    if not os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        save_initial_weights(model)\n",
        "    else:\n",
        "        load_initial_weights(model)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train initial model (or load pre-trained weights)\n",
        "    print(\"Training initial/pruned model...\")\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=CHECKPOINT_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    # Load best weights and prune\n",
        "    model.load_weights(CHECKPOINT_PATH)\n",
        "    load_initial_weights(model)  # Reset to initial weights\n",
        "\n",
        "\n",
        "    #change for change in sparsity[0.4,0.6,0.8]\n",
        "    model = prune_model(model, pruning_fraction=0.4)\n",
        "\n",
        "\n",
        "\n",
        "    # Recompile and retrain the pruned model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Re-training pruned model from initial weights...\")\n",
        "    history_pruned = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    model.save(FINAL_MODEL_PATH)\n",
        "    print(\"LTH pruned model saved.\")\n",
        "\n",
        "    # Load initial weights if available\n",
        "    if os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        load_initial_weights(model)\n",
        "    else:\n",
        "        save_initial_weights(model)"
      ],
      "metadata": {
        "id": "AIOfHJZ_kRHg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_lth_model_sparsity40()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXiAGbEzlrop",
        "outputId": "540d2e2f-744b-41d7-8431-05b288a8a626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights saved.\n",
            "Training initial/pruned model...\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3684 - loss: 1.8503\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46230, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1288s\u001b[0m 2s/step - accuracy: 0.3685 - loss: 1.8499 - val_accuracy: 0.4623 - val_loss: 1.7135\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5832 - loss: 1.1690\n",
            "Epoch 2: val_accuracy improved from 0.46230 to 0.55720, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1262s\u001b[0m 2s/step - accuracy: 0.5832 - loss: 1.1689 - val_accuracy: 0.5572 - val_loss: 1.5803\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6858 - loss: 0.9002\n",
            "Epoch 3: val_accuracy did not improve from 0.55720\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1253s\u001b[0m 2s/step - accuracy: 0.6858 - loss: 0.9002 - val_accuracy: 0.5336 - val_loss: 1.6659\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7294 - loss: 0.7810\n",
            "Epoch 4: val_accuracy improved from 0.55720 to 0.70440, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1251s\u001b[0m 2s/step - accuracy: 0.7294 - loss: 0.7810 - val_accuracy: 0.7044 - val_loss: 0.8771\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7599 - loss: 0.6876\n",
            "Epoch 5: val_accuracy did not improve from 0.70440\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 2s/step - accuracy: 0.7599 - loss: 0.6876 - val_accuracy: 0.6928 - val_loss: 0.9541\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7830 - loss: 0.6284\n",
            "Epoch 6: val_accuracy did not improve from 0.70440\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 2s/step - accuracy: 0.7830 - loss: 0.6284 - val_accuracy: 0.6792 - val_loss: 1.0518\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7981 - loss: 0.5870\n",
            "Epoch 7: val_accuracy improved from 0.70440 to 0.71130, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1242s\u001b[0m 2s/step - accuracy: 0.7981 - loss: 0.5870 - val_accuracy: 0.7113 - val_loss: 0.8463\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8128 - loss: 0.5402\n",
            "Epoch 8: val_accuracy improved from 0.71130 to 0.73140, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1253s\u001b[0m 2s/step - accuracy: 0.8128 - loss: 0.5402 - val_accuracy: 0.7314 - val_loss: 0.8153\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8237 - loss: 0.5040\n",
            "Epoch 9: val_accuracy did not improve from 0.73140\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1236s\u001b[0m 2s/step - accuracy: 0.8237 - loss: 0.5040 - val_accuracy: 0.7106 - val_loss: 0.9161\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8353 - loss: 0.4724\n",
            "Epoch 10: val_accuracy improved from 0.73140 to 0.75140, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1261s\u001b[0m 2s/step - accuracy: 0.8353 - loss: 0.4724 - val_accuracy: 0.7514 - val_loss: 0.7620\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8427 - loss: 0.4487\n",
            "Epoch 11: val_accuracy improved from 0.75140 to 0.79560, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 2s/step - accuracy: 0.8427 - loss: 0.4487 - val_accuracy: 0.7956 - val_loss: 0.6148\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8525 - loss: 0.4287\n",
            "Epoch 12: val_accuracy did not improve from 0.79560\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1240s\u001b[0m 2s/step - accuracy: 0.8525 - loss: 0.4287 - val_accuracy: 0.7846 - val_loss: 0.6326\n",
            "Epoch 13/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8619 - loss: 0.3988\n",
            "Epoch 13: val_accuracy improved from 0.79560 to 0.80160, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1236s\u001b[0m 2s/step - accuracy: 0.8619 - loss: 0.3988 - val_accuracy: 0.8016 - val_loss: 0.5980\n",
            "Epoch 14/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8683 - loss: 0.3827\n",
            "Epoch 14: val_accuracy improved from 0.80160 to 0.81050, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1239s\u001b[0m 2s/step - accuracy: 0.8683 - loss: 0.3827 - val_accuracy: 0.8105 - val_loss: 0.5533\n",
            "Epoch 15/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8710 - loss: 0.3704\n",
            "Epoch 15: val_accuracy improved from 0.81050 to 0.81220, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1265s\u001b[0m 2s/step - accuracy: 0.8710 - loss: 0.3704 - val_accuracy: 0.8122 - val_loss: 0.5781\n",
            "Epoch 16/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8798 - loss: 0.3440\n",
            "Epoch 16: val_accuracy did not improve from 0.81220\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1237s\u001b[0m 2s/step - accuracy: 0.8798 - loss: 0.3440 - val_accuracy: 0.7795 - val_loss: 0.7118\n",
            "Epoch 17/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8834 - loss: 0.3340\n",
            "Epoch 17: val_accuracy improved from 0.81220 to 0.82860, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1243s\u001b[0m 2s/step - accuracy: 0.8834 - loss: 0.3340 - val_accuracy: 0.8286 - val_loss: 0.5203\n",
            "Epoch 18/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8868 - loss: 0.3232\n",
            "Epoch 18: val_accuracy improved from 0.82860 to 0.83970, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 2s/step - accuracy: 0.8868 - loss: 0.3232 - val_accuracy: 0.8397 - val_loss: 0.4948\n",
            "Epoch 19/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8963 - loss: 0.3000\n",
            "Epoch 19: val_accuracy did not improve from 0.83970\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1246s\u001b[0m 2s/step - accuracy: 0.8963 - loss: 0.3000 - val_accuracy: 0.8343 - val_loss: 0.5188\n",
            "Epoch 20/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8984 - loss: 0.2912\n",
            "Epoch 20: val_accuracy improved from 0.83970 to 0.84490, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1282s\u001b[0m 2s/step - accuracy: 0.8984 - loss: 0.2912 - val_accuracy: 0.8449 - val_loss: 0.4567\n",
            "Epoch 21/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9026 - loss: 0.2725\n",
            "Epoch 21: val_accuracy did not improve from 0.84490\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1248s\u001b[0m 2s/step - accuracy: 0.9026 - loss: 0.2725 - val_accuracy: 0.8195 - val_loss: 0.5626\n",
            "Epoch 22/30\n",
            "\u001b[1m394/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m10:03\u001b[0m 2s/step - accuracy: 0.9084 - loss: 0.2615"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparsity60"
      ],
      "metadata": {
        "id": "-YFVlln7rGSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lth_model_sparsity60():\n",
        "    train_gen, val_gen = load_cifar10_data(batch_size=64)\n",
        "    model = build_resnet56(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "    if not os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        save_initial_weights(model)\n",
        "    else:\n",
        "        load_initial_weights(model)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train initial model (or load pre-trained weights)\n",
        "    print(\"Training initial/pruned model...\")\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=CHECKPOINT_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    # Load best weights and prune\n",
        "    model.load_weights(CHECKPOINT_PATH)\n",
        "    load_initial_weights(model)  # Reset to initial weights\n",
        "\n",
        "\n",
        "    #change for change in sparsity[0.4,0.6,0.8]\n",
        "    model = prune_model(model, pruning_fraction=0.6)\n",
        "\n",
        "\n",
        "\n",
        "    # Recompile and retrain the pruned model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Re-training pruned model from initial weights...\")\n",
        "    history_pruned = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    model.save(FINAL_MODEL_PATH)\n",
        "    print(\"LTH pruned model saved.\")\n",
        "\n",
        "    # Load initial weights if available\n",
        "    if os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        load_initial_weights(model)\n",
        "    else:\n",
        "        save_initial_weights(model)"
      ],
      "metadata": {
        "id": "RoqcoxA4t82T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_lth_model_sparsity60()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4o9TYGYt-GA",
        "outputId": "c11dd1c2-4edd-4f04-9c90-fd1e0044dba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Initial weights saved.\n",
            "Training initial/pruned model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3555 - loss: 1.9610\n",
            "Epoch 1: val_accuracy improved from -inf to 0.47080, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1283s\u001b[0m 2s/step - accuracy: 0.3557 - loss: 1.9604 - val_accuracy: 0.4708 - val_loss: 1.5816\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5904 - loss: 1.1380\n",
            "Epoch 2: val_accuracy improved from 0.47080 to 0.65020, saving model to ./checkpoints/lth_resnet56_cifar10.keras\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1299s\u001b[0m 2s/step - accuracy: 0.5904 - loss: 1.1379 - val_accuracy: 0.6502 - val_loss: 1.0119\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6807 - loss: 0.9008\n",
            "Epoch 3: val_accuracy did not improve from 0.65020\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1321s\u001b[0m 2s/step - accuracy: 0.6807 - loss: 0.9008 - val_accuracy: 0.5638 - val_loss: 1.4266\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7330 - loss: 0.7706\n",
            "Epoch 4: val_accuracy did not improve from 0.65020\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1256s\u001b[0m 2s/step - accuracy: 0.7330 - loss: 0.7706 - val_accuracy: 0.6195 - val_loss: 1.1942\n",
            "Epoch 5/10\n",
            "\u001b[1m386/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m10:18\u001b[0m 2s/step - accuracy: 0.7583 - loss: 0.6899"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lth_model_sparsity80():\n",
        "    train_gen, val_gen = load_cifar10_data(batch_size=64)\n",
        "    model = build_resnet56(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "    if not os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        save_initial_weights(model)\n",
        "    else:\n",
        "        load_initial_weights(model)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train initial model (or load pre-trained weights)\n",
        "    print(\"Training initial/pruned model...\")\n",
        "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=CHECKPOINT_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "    earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=30,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    # Load best weights and prune\n",
        "    model.load_weights(CHECKPOINT_PATH)\n",
        "    load_initial_weights(model)  # Reset to initial weights\n",
        "\n",
        "\n",
        "    #change for change in sparsity[0.4,0.6,0.8]\n",
        "    model = prune_model(model, pruning_fraction=0.8)\n",
        "\n",
        "\n",
        "\n",
        "    # Recompile and retrain the pruned model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"Re-training pruned model from initial weights...\")\n",
        "    history_pruned = model.fit(\n",
        "        train_gen,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=[checkpoint_cb, earlystop_cb]\n",
        "    )\n",
        "\n",
        "    model.save(FINAL_MODEL_PATH)\n",
        "    print(\"LTH pruned model saved.\")\n",
        "\n",
        "    # Load initial weights if available\n",
        "    if os.path.exists(INITIAL_WEIGHTS_PATH):\n",
        "        load_initial_weights(model)\n",
        "    else:\n",
        "        save_initial_weights(model)"
      ],
      "metadata": {
        "id": "H7oCFl1EX8rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_lth_model_sparsity80()"
      ],
      "metadata": {
        "id": "Xlis_CkNYAt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRaIomzAX9_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparsity80"
      ],
      "metadata": {
        "id": "b4SA7HtXrGKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AYxTknNorGCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eqTgJNI_rF40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7oV0yFHGrFna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qE1rP-aJC1Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "-7rh3LwahCzF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "def build_prunable_resnet56():\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # Wrap convolutional and dense layers with pruning\n",
        "    x = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        layers.Conv2D(16, (3, 3), padding='same', activation='relu')\n",
        "    )(inputs)\n",
        "\n",
        "    # Add the rest of ResNet blocks with pruning as needed\n",
        "    # Dummy blocks here — replace with real ResNet-56 blocks\n",
        "    for _ in range(3):\n",
        "        x = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "            layers.Conv2D(16, (3, 3), padding='same', activation='relu')\n",
        "        )(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    )(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ph4EowXHhFGz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "# Define your sparsity levels\n",
        "sparsity_levels = [0.4, 0.6, 0.8]\n",
        "\n",
        "# Store results\n",
        "val_accuracies = {}\n",
        "test_accuracies = {}\n",
        "\n",
        "# Create directory to save models\n",
        "os.makedirs(\"lth_models\", exist_ok=True)\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Build and store weights before starting pruning loop\n",
        "base_model = build_resnet56()\n",
        "initial_weights = base_model.get_weights()\n",
        "\n",
        "for sparsity in sparsity_levels:\n",
        "    print(f\"\\n🔧 Starting LTH iteration with {int(sparsity * 100)}% sparsity...\\n\")\n",
        "\n",
        "    # Reload initial model and weights\n",
        "    model = build_resnet56()\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "    # ✅ Define optimizer before compiling\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Apply pruning\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(sparsity, begin_step=0)\n",
        "    }\n",
        "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "    pruned_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = pruned_model.fit(\n",
        "        x_train, y_train,\n",
        "        batch_size=128,\n",
        "        epochs=10,  # Use a lower epoch count to conserve Colab units\n",
        "        validation_split=0.1,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = pruned_model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"🔍 Test Accuracy at {int(sparsity*100)}% sparsity: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "FtFcDGFzIVHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "973413b1-5c7a-4649-ef8f-fba4e25c5fd9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Starting LTH iteration with 40% sparsity...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Functional.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-957e34bc1e48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m'pruning_schedule'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstantSparsity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     }\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpruned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_low_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpruning_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpruned_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/keras/metrics.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FAILURE_LABEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/keras/metrics.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUCCESS_LABEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py\u001b[0m in \u001b[0;36mprune_low_magnitude\u001b[0;34m(to_prune, pruning_schedule, block_size, block_pooling_type, pruning_policy, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpruning_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPruneLowMagnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;34m'`prune_low_magnitude` can only prune an object of the following '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m'types: keras.models.Sequential, keras functional model, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Functional."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "HLdVu7VqTm7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64366f5-2767-4803-a7ba-b400d4ac1bd1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ju5Wwq1foCb",
        "outputId": "b477779d-9528-4388-fca4-ee9120986154"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'keras.src.models.functional.Functional'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(isinstance(model, tf.keras.Model))  # Should be True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeQcvGligzyY",
        "outputId": "5f167ef3-4fce-4c46-a1cb-9e3442905330"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Function to build ResNet56 model\n",
        "def build_resnet56():\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # Example layers, you can add more blocks as needed\n",
        "    x = layers.Conv2D(16, (3, 3), padding='same', activation='relu')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Function to build pruned ResNet56 model with pruning applied\n",
        "def build_prunable_resnet56():\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # Wrap the layers you want to prune with prune_low_magnitude\n",
        "    x = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
        "        pruning_schedule=tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0)\n",
        "    )(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        pruning_schedule=tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0)\n",
        "    )(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    outputs = tfmot.sparsity.keras.prune_low_magnitude(\n",
        "        layers.Dense(10, activation='softmax'),\n",
        "        pruning_schedule=tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0)\n",
        "    )(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "# Function to apply pruning for a given sparsity level\n",
        "def apply_pruning(sparsity, x_train, y_train, x_test, y_test, initial_weights):\n",
        "    print(f\"🔧 Starting LTH iteration with {int(sparsity * 100)}% sparsity...\")\n",
        "\n",
        "    # Pruning scope is not needed when pruning the whole model\n",
        "    model = build_prunable_resnet56()  # Create pruned model\n",
        "    model.set_weights(initial_weights)  # Reset weights to initial\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks for pruning updates\n",
        "    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
        "\n",
        "    # Train the model with pruning\n",
        "    model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=callbacks)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ... (rest of your code remains the same) ...\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Initialize the base model to get the initial weights\n",
        "base_model = build_resnet56()\n",
        "initial_weights = base_model.get_weights()\n",
        "\n",
        "# Set the sparsity levels for pruning\n",
        "sparsity_levels = [0.4, 0.5, 0.6, 0.7, 0.8]  # Example sparsity levels\n",
        "\n",
        "# Loop over sparsity levels\n",
        "for sparsity in sparsity_levels:\n",
        "    model = apply_pruning(sparsity, x_train, y_train, x_test, y_test, initial_weights)\n",
        "    # Save the model after each pruning iteration if needed\n",
        "    # model.save(f\"pruned_model_sparsity_{int(sparsity * 100)}.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "XIGSdQjYhL8_",
        "outputId": "fc8f1d28-7276-4461-ebae-8b052c0a3246"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Starting LTH iteration with 40% sparsity...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Conv2D.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f3ff31417569>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Loop over sparsity levels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msparsity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msparsity_levels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_pruning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Save the model after each pruning iteration if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# model.save(f\"pruned_model_sparsity_{int(sparsity * 100)}.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-f3ff31417569>\u001b[0m in \u001b[0;36mapply_pruning\u001b[0;34m(sparsity, x_train, y_train, x_test, y_test, initial_weights)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Pruning scope is not needed when pruning the whole model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_prunable_resnet56\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create pruned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reset weights to initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-f3ff31417569>\u001b[0m in \u001b[0;36mbuild_prunable_resnet56\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Wrap the layers you want to prune with prune_low_magnitude\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     x = tfmot.sparsity.keras.prune_low_magnitude(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpruning_schedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparsity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstantSparsity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/keras/metrics.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FAILURE_LABEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/keras/metrics.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUCCESS_LABEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py\u001b[0m in \u001b[0;36mprune_low_magnitude\u001b[0;34m(to_prune, pruning_schedule, block_size, block_pooling_type, pruning_policy, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpruning_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPruneLowMagnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_prune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;34m'`prune_low_magnitude` can only prune an object of the following '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m'types: keras.models.Sequential, keras functional model, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Conv2D."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWPQqzKyiFD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}