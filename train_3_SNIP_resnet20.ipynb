{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from models.resnet20 import build_resnet20\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# -----------------------------\n",
    "# SNIP Pruning Function\n",
    "# -----------------------------\n",
    "def snip_prune(model, x_batch, y_batch, sparsity):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    snip_scores = [tf.abs(w * g) for w, g in zip(model.trainable_variables, grads) if g is not None and 'kernel' in w.name]\n",
    "    all_scores = tf.concat([tf.reshape(score, [-1]) for score in snip_scores], axis=0)\n",
    "\n",
    "    k = int((1 - sparsity) * tf.size(all_scores).numpy())\n",
    "    threshold = tf.sort(all_scores)[k].numpy()\n",
    "\n",
    "    masks = [(tf.abs(score) > threshold).numpy().astype(np.float32) for score in snip_scores]\n",
    "    mask_idx = 0\n",
    "\n",
    "    for i, var in enumerate(model.trainable_variables):\n",
    "        if 'kernel' in var.name:\n",
    "            var.assign(var * masks[mask_idx])\n",
    "            mask_idx += 1\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def run_snip_training(sparsity=0.5, batch_size=128, epochs=150):\n",
    "    # Load and preprocess data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "    # Build and initialize model\n",
    "    model = build_resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    # SNIP pruning on small batch\n",
    "    x_sample = x_train[:batch_size]\n",
    "    y_sample = y_train[:batch_size]\n",
    "    snip_prune(model, x_sample, y_sample, sparsity)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Prepare saving\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    checkpoint_path = f'models/pruned_resnet20_SNIP_sparsity_{sparsity}.h5'\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[checkpoint_callback],\n",
    "                        verbose=1)\n",
    "\n",
    "    print(\"Training complete. Best model saved.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7ab000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_18[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 15s 32ms/step - loss: 1.4990 - accuracy: 0.4910 - val_loss: 1.5870 - val_accuracy: 0.4804\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 1.1018 - accuracy: 0.6448 - val_loss: 1.4436 - val_accuracy: 0.5510\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.9295 - accuracy: 0.7107 - val_loss: 1.1498 - val_accuracy: 0.6468\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.8015 - accuracy: 0.7570 - val_loss: 1.2912 - val_accuracy: 0.6224\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7120 - accuracy: 0.7924 - val_loss: 1.0215 - val_accuracy: 0.6959\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6395 - accuracy: 0.8179 - val_loss: 1.0416 - val_accuracy: 0.6847\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5803 - accuracy: 0.8423 - val_loss: 0.9812 - val_accuracy: 0.7193\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5319 - accuracy: 0.8586 - val_loss: 1.6433 - val_accuracy: 0.6050\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4873 - accuracy: 0.8760 - val_loss: 0.9903 - val_accuracy: 0.7339\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4490 - accuracy: 0.8904 - val_loss: 1.0848 - val_accuracy: 0.7139\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.4111 - accuracy: 0.9062 - val_loss: 1.4024 - val_accuracy: 0.6779\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3902 - accuracy: 0.9141 - val_loss: 1.1527 - val_accuracy: 0.7241\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3672 - accuracy: 0.9238 - val_loss: 1.7088 - val_accuracy: 0.6419\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3398 - accuracy: 0.9331 - val_loss: 1.4553 - val_accuracy: 0.6802\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3271 - accuracy: 0.9394 - val_loss: 1.4718 - val_accuracy: 0.7068\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3170 - accuracy: 0.9447 - val_loss: 1.4589 - val_accuracy: 0.6990\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3009 - accuracy: 0.9502 - val_loss: 1.7102 - val_accuracy: 0.6792\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2989 - accuracy: 0.9523 - val_loss: 2.0089 - val_accuracy: 0.6389\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2974 - accuracy: 0.9525 - val_loss: 1.5227 - val_accuracy: 0.7059\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2912 - accuracy: 0.9571 - val_loss: 1.9356 - val_accuracy: 0.6658\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2838 - accuracy: 0.9592 - val_loss: 1.6194 - val_accuracy: 0.6921\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2810 - accuracy: 0.9615 - val_loss: 1.3914 - val_accuracy: 0.7245\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2821 - accuracy: 0.9613 - val_loss: 1.5335 - val_accuracy: 0.7199\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2792 - accuracy: 0.9620 - val_loss: 1.1669 - val_accuracy: 0.7570\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2748 - accuracy: 0.9645 - val_loss: 1.6771 - val_accuracy: 0.7061\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2741 - accuracy: 0.9653 - val_loss: 1.5295 - val_accuracy: 0.7188\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2831 - accuracy: 0.9623 - val_loss: 1.6541 - val_accuracy: 0.7238\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2727 - accuracy: 0.9674 - val_loss: 1.5406 - val_accuracy: 0.7329\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2774 - accuracy: 0.9655 - val_loss: 1.6557 - val_accuracy: 0.7041\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2762 - accuracy: 0.9666 - val_loss: 1.8545 - val_accuracy: 0.6853\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2615 - accuracy: 0.9719 - val_loss: 2.1098 - val_accuracy: 0.6618\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2819 - accuracy: 0.9646 - val_loss: 1.5650 - val_accuracy: 0.7088\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2674 - accuracy: 0.9705 - val_loss: 1.7404 - val_accuracy: 0.7093\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2681 - accuracy: 0.9706 - val_loss: 1.5457 - val_accuracy: 0.7228\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2775 - accuracy: 0.9673 - val_loss: 1.8546 - val_accuracy: 0.6870\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2649 - accuracy: 0.9715 - val_loss: 1.4943 - val_accuracy: 0.7369\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 10s 26ms/step - loss: 0.2729 - accuracy: 0.9695 - val_loss: 1.6273 - val_accuracy: 0.7102\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2682 - accuracy: 0.9710 - val_loss: 1.4534 - val_accuracy: 0.7518\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2630 - accuracy: 0.9740 - val_loss: 1.5343 - val_accuracy: 0.7377\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2716 - accuracy: 0.9707 - val_loss: 2.8290 - val_accuracy: 0.6353\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2648 - accuracy: 0.9727 - val_loss: 1.5472 - val_accuracy: 0.7401\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2610 - accuracy: 0.9744 - val_loss: 1.7149 - val_accuracy: 0.7134\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2695 - accuracy: 0.9717 - val_loss: 1.4548 - val_accuracy: 0.7476\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2804 - accuracy: 0.9683 - val_loss: 1.5058 - val_accuracy: 0.7457\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2572 - accuracy: 0.9760 - val_loss: 1.5070 - val_accuracy: 0.7392\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2607 - accuracy: 0.9748 - val_loss: 1.6959 - val_accuracy: 0.7216\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2686 - accuracy: 0.9721 - val_loss: 2.2852 - val_accuracy: 0.6821\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2617 - accuracy: 0.9755 - val_loss: 1.5261 - val_accuracy: 0.7338\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2705 - accuracy: 0.9712 - val_loss: 1.8228 - val_accuracy: 0.6900\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2638 - accuracy: 0.9738 - val_loss: 2.1854 - val_accuracy: 0.6610\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2600 - accuracy: 0.9754 - val_loss: 1.5310 - val_accuracy: 0.7507\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2652 - accuracy: 0.9731 - val_loss: 1.6686 - val_accuracy: 0.7334\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2723 - accuracy: 0.9718 - val_loss: 1.6318 - val_accuracy: 0.7207\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2581 - accuracy: 0.9770 - val_loss: 2.0622 - val_accuracy: 0.6696\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2554 - accuracy: 0.9776 - val_loss: 2.0008 - val_accuracy: 0.7069\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2683 - accuracy: 0.9728 - val_loss: 2.4199 - val_accuracy: 0.6349\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2710 - accuracy: 0.9727 - val_loss: 1.7115 - val_accuracy: 0.7276\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2539 - accuracy: 0.9788 - val_loss: 1.7323 - val_accuracy: 0.7183\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2569 - accuracy: 0.9782 - val_loss: 1.5633 - val_accuracy: 0.7371\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2680 - accuracy: 0.9730 - val_loss: 1.5692 - val_accuracy: 0.7335\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2542 - accuracy: 0.9790 - val_loss: 2.1683 - val_accuracy: 0.6797\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2579 - accuracy: 0.9770 - val_loss: 2.7047 - val_accuracy: 0.6094\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2578 - accuracy: 0.9764 - val_loss: 2.1633 - val_accuracy: 0.6775\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2740 - accuracy: 0.9716 - val_loss: 1.6091 - val_accuracy: 0.7353\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2528 - accuracy: 0.9793 - val_loss: 1.8302 - val_accuracy: 0.7110\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2547 - accuracy: 0.9775 - val_loss: 1.5989 - val_accuracy: 0.7362\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2635 - accuracy: 0.9749 - val_loss: 1.8837 - val_accuracy: 0.6987\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2623 - accuracy: 0.9754 - val_loss: 1.5238 - val_accuracy: 0.7363\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2605 - accuracy: 0.9760 - val_loss: 1.6688 - val_accuracy: 0.7382\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2475 - accuracy: 0.9802 - val_loss: 1.4153 - val_accuracy: 0.7614\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2684 - accuracy: 0.9730 - val_loss: 1.6322 - val_accuracy: 0.7385\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2546 - accuracy: 0.9779 - val_loss: 2.5939 - val_accuracy: 0.6671\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2523 - accuracy: 0.9786 - val_loss: 1.5227 - val_accuracy: 0.7562\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2598 - accuracy: 0.9751 - val_loss: 1.9011 - val_accuracy: 0.7233\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2533 - accuracy: 0.9781 - val_loss: 1.7548 - val_accuracy: 0.7413\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2567 - accuracy: 0.9770 - val_loss: 1.3347 - val_accuracy: 0.7695\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2527 - accuracy: 0.9780 - val_loss: 1.4545 - val_accuracy: 0.7674\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2587 - accuracy: 0.9762 - val_loss: 1.4367 - val_accuracy: 0.7653\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2578 - accuracy: 0.9765 - val_loss: 2.1147 - val_accuracy: 0.6724\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2531 - accuracy: 0.9785 - val_loss: 1.7338 - val_accuracy: 0.7234\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2594 - accuracy: 0.9762 - val_loss: 1.8665 - val_accuracy: 0.7131\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2462 - accuracy: 0.9807 - val_loss: 1.4704 - val_accuracy: 0.7600\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2510 - accuracy: 0.9790 - val_loss: 1.5122 - val_accuracy: 0.7557\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2582 - accuracy: 0.9765 - val_loss: 1.5405 - val_accuracy: 0.7542\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2484 - accuracy: 0.9801 - val_loss: 1.4583 - val_accuracy: 0.7535\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2489 - accuracy: 0.9800 - val_loss: 1.3998 - val_accuracy: 0.7709\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2639 - accuracy: 0.9736 - val_loss: 1.7090 - val_accuracy: 0.7349\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2462 - accuracy: 0.9805 - val_loss: 1.7307 - val_accuracy: 0.7383\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2473 - accuracy: 0.9798 - val_loss: 1.7321 - val_accuracy: 0.7280\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2483 - accuracy: 0.9790 - val_loss: 1.6703 - val_accuracy: 0.7231\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2592 - accuracy: 0.9757 - val_loss: 1.5975 - val_accuracy: 0.7613\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2495 - accuracy: 0.9790 - val_loss: 1.7512 - val_accuracy: 0.7410\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2505 - accuracy: 0.9787 - val_loss: 2.1428 - val_accuracy: 0.6968\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2516 - accuracy: 0.9790 - val_loss: 1.6331 - val_accuracy: 0.7380\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2477 - accuracy: 0.9793 - val_loss: 2.5685 - val_accuracy: 0.6644\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2608 - accuracy: 0.9753 - val_loss: 1.5198 - val_accuracy: 0.7510\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2503 - accuracy: 0.9792 - val_loss: 1.7260 - val_accuracy: 0.7386\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2386 - accuracy: 0.9824 - val_loss: 1.6525 - val_accuracy: 0.7377\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2559 - accuracy: 0.9767 - val_loss: 2.0363 - val_accuracy: 0.6987\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2521 - accuracy: 0.9778 - val_loss: 1.5360 - val_accuracy: 0.7470\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2453 - accuracy: 0.9801 - val_loss: 1.6271 - val_accuracy: 0.7378\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2453 - accuracy: 0.9792 - val_loss: 1.5315 - val_accuracy: 0.7520\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2640 - accuracy: 0.9740 - val_loss: 1.7213 - val_accuracy: 0.7304\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2472 - accuracy: 0.9806 - val_loss: 1.4866 - val_accuracy: 0.7539\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2410 - accuracy: 0.9822 - val_loss: 1.5411 - val_accuracy: 0.7539\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2449 - accuracy: 0.9803 - val_loss: 2.3407 - val_accuracy: 0.6910\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2465 - accuracy: 0.9794 - val_loss: 2.1220 - val_accuracy: 0.6988\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2522 - accuracy: 0.9771 - val_loss: 1.7473 - val_accuracy: 0.7228\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2532 - accuracy: 0.9771 - val_loss: 1.5488 - val_accuracy: 0.7282\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2388 - accuracy: 0.9822 - val_loss: 1.4800 - val_accuracy: 0.7583\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2490 - accuracy: 0.9782 - val_loss: 1.4101 - val_accuracy: 0.7567\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2466 - accuracy: 0.9800 - val_loss: 2.3674 - val_accuracy: 0.6806\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2402 - accuracy: 0.9812 - val_loss: 1.5487 - val_accuracy: 0.7431\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2443 - accuracy: 0.9799 - val_loss: 1.2901 - val_accuracy: 0.7807\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2507 - accuracy: 0.9778 - val_loss: 1.5531 - val_accuracy: 0.7490\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2471 - accuracy: 0.9793 - val_loss: 1.4744 - val_accuracy: 0.7522\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2381 - accuracy: 0.9820 - val_loss: 1.8536 - val_accuracy: 0.7111\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2477 - accuracy: 0.9789 - val_loss: 2.0373 - val_accuracy: 0.7072\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2438 - accuracy: 0.9794 - val_loss: 1.5758 - val_accuracy: 0.7437\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2386 - accuracy: 0.9820 - val_loss: 2.0996 - val_accuracy: 0.7104\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2514 - accuracy: 0.9770 - val_loss: 2.4058 - val_accuracy: 0.6664\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2455 - accuracy: 0.9788 - val_loss: 2.0120 - val_accuracy: 0.6910\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2350 - accuracy: 0.9828 - val_loss: 1.7217 - val_accuracy: 0.7181\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2480 - accuracy: 0.9781 - val_loss: 1.6460 - val_accuracy: 0.7433\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2446 - accuracy: 0.9791 - val_loss: 1.7290 - val_accuracy: 0.7401\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2373 - accuracy: 0.9818 - val_loss: 1.5401 - val_accuracy: 0.7614\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2448 - accuracy: 0.9791 - val_loss: 1.9689 - val_accuracy: 0.7035\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2407 - accuracy: 0.9806 - val_loss: 1.9173 - val_accuracy: 0.7138\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2516 - accuracy: 0.9769 - val_loss: 1.4325 - val_accuracy: 0.7586\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2406 - accuracy: 0.9810 - val_loss: 1.6482 - val_accuracy: 0.7450\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2362 - accuracy: 0.9812 - val_loss: 2.5866 - val_accuracy: 0.6703\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2413 - accuracy: 0.9801 - val_loss: 1.7187 - val_accuracy: 0.7360\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2487 - accuracy: 0.9784 - val_loss: 1.7460 - val_accuracy: 0.7305\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2367 - accuracy: 0.9818 - val_loss: 1.7197 - val_accuracy: 0.7433\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2358 - accuracy: 0.9818 - val_loss: 1.4873 - val_accuracy: 0.7664\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2433 - accuracy: 0.9796 - val_loss: 1.8049 - val_accuracy: 0.7268\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2433 - accuracy: 0.9794 - val_loss: 1.2784 - val_accuracy: 0.7803\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2390 - accuracy: 0.9805 - val_loss: 1.5668 - val_accuracy: 0.7688\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2417 - accuracy: 0.9797 - val_loss: 1.4794 - val_accuracy: 0.7645\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2414 - accuracy: 0.9794 - val_loss: 1.6613 - val_accuracy: 0.7565\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2330 - accuracy: 0.9826 - val_loss: 1.5156 - val_accuracy: 0.7658\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2362 - accuracy: 0.9812 - val_loss: 1.7963 - val_accuracy: 0.7314\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2460 - accuracy: 0.9783 - val_loss: 1.3883 - val_accuracy: 0.7739\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2407 - accuracy: 0.9803 - val_loss: 2.0949 - val_accuracy: 0.7066\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2344 - accuracy: 0.9829 - val_loss: 1.4857 - val_accuracy: 0.7594\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2385 - accuracy: 0.9809 - val_loss: 1.6326 - val_accuracy: 0.7618\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2395 - accuracy: 0.9797 - val_loss: 1.6701 - val_accuracy: 0.7320\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2439 - accuracy: 0.9786 - val_loss: 1.3804 - val_accuracy: 0.7762\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.2272 - accuracy: 0.9843 - val_loss: 1.7802 - val_accuracy: 0.7402\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2465 - accuracy: 0.9780 - val_loss: 1.3099 - val_accuracy: 0.7924\n",
      "Model training complete and saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e334b7fe20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.3, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e800df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 16)  64          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 16)  64          ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 32, 32, 16)  64          ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 32, 16)   0           ['activation_19[0][0]',          \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 16)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32, 32, 16)  64          ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 32, 32, 16)  64          ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 32, 32, 16)   0           ['activation_21[0][0]',          \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 16)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 32, 32, 16)   0           ['activation_23[0][0]',          \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 16)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   544         ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['conv2d_30[0][0]',              \n",
      "                                                                  'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_27[0][0]',          \n",
      "                                                                  'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_29[0][0]',          \n",
      "                                                                  'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 64)    256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['conv2d_37[0][0]',              \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 64)     0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['activation_33[0][0]',          \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 64)     0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['activation_35[0][0]',          \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['activation_37[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 14s 29ms/step - loss: 1.5312 - accuracy: 0.4918 - val_loss: 1.7869 - val_accuracy: 0.4817\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.1064 - accuracy: 0.6512 - val_loss: 1.3200 - val_accuracy: 0.6108\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.9197 - accuracy: 0.7197 - val_loss: 1.3135 - val_accuracy: 0.6031\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.8061 - accuracy: 0.7627 - val_loss: 1.1903 - val_accuracy: 0.6519\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7184 - accuracy: 0.7957 - val_loss: 1.1659 - val_accuracy: 0.6630\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6493 - accuracy: 0.8212 - val_loss: 1.3521 - val_accuracy: 0.6126\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5944 - accuracy: 0.8401 - val_loss: 1.4322 - val_accuracy: 0.6242\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5434 - accuracy: 0.8600 - val_loss: 0.9866 - val_accuracy: 0.7268\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4969 - accuracy: 0.8776 - val_loss: 1.3654 - val_accuracy: 0.6655\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4598 - accuracy: 0.8912 - val_loss: 1.1124 - val_accuracy: 0.7126\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4273 - accuracy: 0.9043 - val_loss: 1.1235 - val_accuracy: 0.7190\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.3944 - accuracy: 0.9167 - val_loss: 1.1200 - val_accuracy: 0.7279\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3721 - accuracy: 0.9255 - val_loss: 1.3821 - val_accuracy: 0.6815\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3510 - accuracy: 0.9327 - val_loss: 1.1114 - val_accuracy: 0.7436\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3368 - accuracy: 0.9391 - val_loss: 1.4246 - val_accuracy: 0.6884\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3215 - accuracy: 0.9451 - val_loss: 1.3069 - val_accuracy: 0.7264\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3146 - accuracy: 0.9494 - val_loss: 1.1437 - val_accuracy: 0.7364\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2986 - accuracy: 0.9561 - val_loss: 1.6720 - val_accuracy: 0.6815\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3041 - accuracy: 0.9524 - val_loss: 1.7849 - val_accuracy: 0.7041\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3012 - accuracy: 0.9548 - val_loss: 1.5824 - val_accuracy: 0.6989\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2782 - accuracy: 0.9642 - val_loss: 1.4002 - val_accuracy: 0.7211\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2932 - accuracy: 0.9587 - val_loss: 1.7395 - val_accuracy: 0.6899\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2877 - accuracy: 0.9611 - val_loss: 2.2392 - val_accuracy: 0.6612\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2863 - accuracy: 0.9630 - val_loss: 1.9631 - val_accuracy: 0.6679\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2824 - accuracy: 0.9640 - val_loss: 1.5775 - val_accuracy: 0.7073\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2835 - accuracy: 0.9645 - val_loss: 2.3196 - val_accuracy: 0.6452\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2842 - accuracy: 0.9636 - val_loss: 1.8712 - val_accuracy: 0.6521\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2802 - accuracy: 0.9659 - val_loss: 1.6679 - val_accuracy: 0.7112\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2795 - accuracy: 0.9671 - val_loss: 1.8734 - val_accuracy: 0.6743\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2801 - accuracy: 0.9671 - val_loss: 2.6161 - val_accuracy: 0.5862\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2799 - accuracy: 0.9678 - val_loss: 1.5505 - val_accuracy: 0.7276\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2781 - accuracy: 0.9678 - val_loss: 2.3534 - val_accuracy: 0.6257\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2813 - accuracy: 0.9677 - val_loss: 1.4144 - val_accuracy: 0.7449\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2692 - accuracy: 0.9723 - val_loss: 1.7410 - val_accuracy: 0.7185\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2769 - accuracy: 0.9700 - val_loss: 1.4339 - val_accuracy: 0.7413\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2838 - accuracy: 0.9673 - val_loss: 1.6392 - val_accuracy: 0.7259\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2706 - accuracy: 0.9730 - val_loss: 1.8980 - val_accuracy: 0.6860\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2696 - accuracy: 0.9730 - val_loss: 1.8444 - val_accuracy: 0.7142\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2777 - accuracy: 0.9699 - val_loss: 1.4281 - val_accuracy: 0.7521\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2743 - accuracy: 0.9711 - val_loss: 1.7082 - val_accuracy: 0.7192\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2734 - accuracy: 0.9715 - val_loss: 2.1834 - val_accuracy: 0.6726\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2707 - accuracy: 0.9732 - val_loss: 1.6090 - val_accuracy: 0.7358\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2726 - accuracy: 0.9725 - val_loss: 2.2157 - val_accuracy: 0.6696\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2731 - accuracy: 0.9729 - val_loss: 1.8315 - val_accuracy: 0.7104\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2751 - accuracy: 0.9716 - val_loss: 1.7991 - val_accuracy: 0.7280\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2717 - accuracy: 0.9737 - val_loss: 1.5795 - val_accuracy: 0.7345\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2719 - accuracy: 0.9732 - val_loss: 1.9933 - val_accuracy: 0.7154\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2743 - accuracy: 0.9727 - val_loss: 2.5804 - val_accuracy: 0.6479\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2706 - accuracy: 0.9747 - val_loss: 2.5648 - val_accuracy: 0.6337\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2781 - accuracy: 0.9715 - val_loss: 1.6118 - val_accuracy: 0.7505\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2726 - accuracy: 0.9734 - val_loss: 1.5245 - val_accuracy: 0.7240\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2602 - accuracy: 0.9782 - val_loss: 1.9708 - val_accuracy: 0.7071\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2708 - accuracy: 0.9744 - val_loss: 1.4660 - val_accuracy: 0.7572\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2711 - accuracy: 0.9735 - val_loss: 1.5346 - val_accuracy: 0.7510\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2686 - accuracy: 0.9747 - val_loss: 1.9044 - val_accuracy: 0.6996\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2678 - accuracy: 0.9750 - val_loss: 1.7435 - val_accuracy: 0.7163\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2678 - accuracy: 0.9755 - val_loss: 2.3488 - val_accuracy: 0.6552\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2729 - accuracy: 0.9725 - val_loss: 1.3876 - val_accuracy: 0.7520\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2712 - accuracy: 0.9739 - val_loss: 1.4585 - val_accuracy: 0.7498\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2673 - accuracy: 0.9750 - val_loss: 1.7548 - val_accuracy: 0.7117\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2673 - accuracy: 0.9758 - val_loss: 1.6075 - val_accuracy: 0.7292\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2610 - accuracy: 0.9782 - val_loss: 1.9229 - val_accuracy: 0.7077\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2740 - accuracy: 0.9729 - val_loss: 1.6708 - val_accuracy: 0.7222\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2648 - accuracy: 0.9765 - val_loss: 1.7621 - val_accuracy: 0.7102\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2647 - accuracy: 0.9765 - val_loss: 1.7875 - val_accuracy: 0.7229\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2746 - accuracy: 0.9726 - val_loss: 1.8476 - val_accuracy: 0.7177\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2648 - accuracy: 0.9770 - val_loss: 1.8192 - val_accuracy: 0.7161\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2578 - accuracy: 0.9791 - val_loss: 1.6574 - val_accuracy: 0.7324\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2682 - accuracy: 0.9751 - val_loss: 1.6263 - val_accuracy: 0.7352\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2642 - accuracy: 0.9770 - val_loss: 1.5887 - val_accuracy: 0.7353\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2676 - accuracy: 0.9751 - val_loss: 1.6199 - val_accuracy: 0.7328\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2622 - accuracy: 0.9772 - val_loss: 1.6716 - val_accuracy: 0.7352\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2667 - accuracy: 0.9757 - val_loss: 1.5148 - val_accuracy: 0.7412\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2644 - accuracy: 0.9768 - val_loss: 2.0380 - val_accuracy: 0.6939\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2631 - accuracy: 0.9771 - val_loss: 2.0690 - val_accuracy: 0.7103\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2648 - accuracy: 0.9762 - val_loss: 2.0496 - val_accuracy: 0.6971\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2654 - accuracy: 0.9764 - val_loss: 1.6166 - val_accuracy: 0.7343\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2582 - accuracy: 0.9790 - val_loss: 1.7400 - val_accuracy: 0.7295\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2624 - accuracy: 0.9775 - val_loss: 1.5833 - val_accuracy: 0.7412\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2694 - accuracy: 0.9744 - val_loss: 1.6436 - val_accuracy: 0.7254\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2548 - accuracy: 0.9805 - val_loss: 1.7519 - val_accuracy: 0.7264\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2528 - accuracy: 0.9799 - val_loss: 1.4968 - val_accuracy: 0.7506\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2674 - accuracy: 0.9744 - val_loss: 3.0066 - val_accuracy: 0.6127\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2624 - accuracy: 0.9774 - val_loss: 1.4351 - val_accuracy: 0.7415\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2615 - accuracy: 0.9772 - val_loss: 2.3115 - val_accuracy: 0.6813\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2587 - accuracy: 0.9784 - val_loss: 1.8188 - val_accuracy: 0.7182\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2612 - accuracy: 0.9775 - val_loss: 1.9163 - val_accuracy: 0.7109\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2676 - accuracy: 0.9747 - val_loss: 2.2990 - val_accuracy: 0.6645\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2534 - accuracy: 0.9797 - val_loss: 1.4081 - val_accuracy: 0.7604\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2528 - accuracy: 0.9799 - val_loss: 2.0596 - val_accuracy: 0.7176\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2557 - accuracy: 0.9791 - val_loss: 1.9547 - val_accuracy: 0.7103\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2629 - accuracy: 0.9762 - val_loss: 1.5743 - val_accuracy: 0.7440\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2609 - accuracy: 0.9770 - val_loss: 2.0178 - val_accuracy: 0.7166\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2638 - accuracy: 0.9759 - val_loss: 1.3512 - val_accuracy: 0.7740\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2447 - accuracy: 0.9826 - val_loss: 1.5008 - val_accuracy: 0.7571\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2717 - accuracy: 0.9737 - val_loss: 1.8639 - val_accuracy: 0.7055\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2548 - accuracy: 0.9802 - val_loss: 1.9043 - val_accuracy: 0.7043\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2494 - accuracy: 0.9805 - val_loss: 1.7265 - val_accuracy: 0.7226\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2580 - accuracy: 0.9771 - val_loss: 1.6795 - val_accuracy: 0.7362\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2621 - accuracy: 0.9763 - val_loss: 1.6325 - val_accuracy: 0.7582\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2501 - accuracy: 0.9799 - val_loss: 1.3903 - val_accuracy: 0.7612\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2501 - accuracy: 0.9803 - val_loss: 1.9018 - val_accuracy: 0.7267\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2574 - accuracy: 0.9775 - val_loss: 1.5684 - val_accuracy: 0.7530\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2631 - accuracy: 0.9753 - val_loss: 1.5930 - val_accuracy: 0.7317\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2542 - accuracy: 0.9790 - val_loss: 1.9171 - val_accuracy: 0.7051\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2446 - accuracy: 0.9823 - val_loss: 1.4918 - val_accuracy: 0.7516\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2535 - accuracy: 0.9790 - val_loss: 1.5623 - val_accuracy: 0.7566\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2602 - accuracy: 0.9772 - val_loss: 2.1427 - val_accuracy: 0.7041\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2585 - accuracy: 0.9775 - val_loss: 1.5008 - val_accuracy: 0.7517\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2478 - accuracy: 0.9818 - val_loss: 1.4987 - val_accuracy: 0.7627\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2481 - accuracy: 0.9808 - val_loss: 1.3633 - val_accuracy: 0.7545\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2549 - accuracy: 0.9782 - val_loss: 4.0793 - val_accuracy: 0.5493\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2573 - accuracy: 0.9775 - val_loss: 1.5702 - val_accuracy: 0.7440\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2538 - accuracy: 0.9790 - val_loss: 1.6519 - val_accuracy: 0.7407\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2433 - accuracy: 0.9824 - val_loss: 2.3284 - val_accuracy: 0.6947\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2622 - accuracy: 0.9754 - val_loss: 1.8031 - val_accuracy: 0.7238\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2533 - accuracy: 0.9783 - val_loss: 1.5098 - val_accuracy: 0.7526\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2529 - accuracy: 0.9789 - val_loss: 1.4176 - val_accuracy: 0.7711\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2448 - accuracy: 0.9815 - val_loss: 1.6501 - val_accuracy: 0.7524\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2468 - accuracy: 0.9798 - val_loss: 2.0388 - val_accuracy: 0.7181\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2540 - accuracy: 0.9779 - val_loss: 1.8380 - val_accuracy: 0.7173\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2548 - accuracy: 0.9772 - val_loss: 1.9166 - val_accuracy: 0.7267\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2452 - accuracy: 0.9816 - val_loss: 1.5086 - val_accuracy: 0.7561\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2480 - accuracy: 0.9794 - val_loss: 1.6242 - val_accuracy: 0.7448\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2516 - accuracy: 0.9791 - val_loss: 1.4117 - val_accuracy: 0.7703\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2444 - accuracy: 0.9815 - val_loss: 1.6483 - val_accuracy: 0.7440\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2519 - accuracy: 0.9782 - val_loss: 2.0169 - val_accuracy: 0.7165\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2489 - accuracy: 0.9792 - val_loss: 1.5674 - val_accuracy: 0.7518\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2515 - accuracy: 0.9797 - val_loss: 1.3360 - val_accuracy: 0.7650\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2393 - accuracy: 0.9831 - val_loss: 2.3707 - val_accuracy: 0.6639\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2510 - accuracy: 0.9776 - val_loss: 1.7236 - val_accuracy: 0.7291\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2483 - accuracy: 0.9793 - val_loss: 1.5319 - val_accuracy: 0.7624\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2479 - accuracy: 0.9797 - val_loss: 1.7624 - val_accuracy: 0.7265\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2477 - accuracy: 0.9797 - val_loss: 1.7132 - val_accuracy: 0.7299\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2453 - accuracy: 0.9797 - val_loss: 1.6414 - val_accuracy: 0.7270\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2434 - accuracy: 0.9807 - val_loss: 1.9119 - val_accuracy: 0.7102\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2446 - accuracy: 0.9797 - val_loss: 1.4575 - val_accuracy: 0.7611\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2472 - accuracy: 0.9795 - val_loss: 1.7134 - val_accuracy: 0.7429\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2445 - accuracy: 0.9803 - val_loss: 1.4883 - val_accuracy: 0.7552\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2484 - accuracy: 0.9791 - val_loss: 1.6120 - val_accuracy: 0.7398\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2456 - accuracy: 0.9801 - val_loss: 1.3861 - val_accuracy: 0.7716\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2424 - accuracy: 0.9810 - val_loss: 1.4149 - val_accuracy: 0.7682\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2359 - accuracy: 0.9835 - val_loss: 1.5478 - val_accuracy: 0.7514\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2485 - accuracy: 0.9784 - val_loss: 1.5678 - val_accuracy: 0.7369\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2422 - accuracy: 0.9815 - val_loss: 1.4655 - val_accuracy: 0.7630\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2457 - accuracy: 0.9801 - val_loss: 1.9289 - val_accuracy: 0.7055\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2421 - accuracy: 0.9803 - val_loss: 1.4170 - val_accuracy: 0.7746\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2452 - accuracy: 0.9787 - val_loss: 1.5581 - val_accuracy: 0.7532\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2412 - accuracy: 0.9819 - val_loss: 1.9440 - val_accuracy: 0.6960\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2452 - accuracy: 0.9800 - val_loss: 1.8595 - val_accuracy: 0.7231\n",
      "Model training complete and saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4fdcfdaf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.5, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fdf4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_18[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 32s 75ms/step - loss: 1.5879 - accuracy: 0.4797 - val_loss: 1.7085 - val_accuracy: 0.4496\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.1753 - accuracy: 0.6297 - val_loss: 1.3182 - val_accuracy: 0.5944\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.9933 - accuracy: 0.6991 - val_loss: 1.2292 - val_accuracy: 0.6304\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.8767 - accuracy: 0.7416 - val_loss: 1.1884 - val_accuracy: 0.6480\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.7857 - accuracy: 0.7748 - val_loss: 1.1293 - val_accuracy: 0.6766\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.7116 - accuracy: 0.8004 - val_loss: 1.0536 - val_accuracy: 0.6938\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6470 - accuracy: 0.8251 - val_loss: 2.0294 - val_accuracy: 0.5496\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5984 - accuracy: 0.8423 - val_loss: 1.1773 - val_accuracy: 0.6735\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5418 - accuracy: 0.8655 - val_loss: 2.1042 - val_accuracy: 0.4943\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5037 - accuracy: 0.8789 - val_loss: 1.3561 - val_accuracy: 0.6797\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4681 - accuracy: 0.8930 - val_loss: 1.4241 - val_accuracy: 0.6589\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4351 - accuracy: 0.9038 - val_loss: 1.6006 - val_accuracy: 0.6489\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4057 - accuracy: 0.9153 - val_loss: 1.5296 - val_accuracy: 0.6708\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3744 - accuracy: 0.9278 - val_loss: 1.2119 - val_accuracy: 0.7195\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3698 - accuracy: 0.9306 - val_loss: 1.4580 - val_accuracy: 0.6774\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3464 - accuracy: 0.9405 - val_loss: 1.8681 - val_accuracy: 0.6505\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3383 - accuracy: 0.9427 - val_loss: 1.6683 - val_accuracy: 0.6795\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3321 - accuracy: 0.9449 - val_loss: 2.2631 - val_accuracy: 0.6022\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3238 - accuracy: 0.9487 - val_loss: 1.3167 - val_accuracy: 0.7201\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3107 - accuracy: 0.9552 - val_loss: 1.5357 - val_accuracy: 0.6968\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3144 - accuracy: 0.9531 - val_loss: 1.6715 - val_accuracy: 0.6922\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3099 - accuracy: 0.9555 - val_loss: 1.5338 - val_accuracy: 0.7061\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3020 - accuracy: 0.9589 - val_loss: 1.5266 - val_accuracy: 0.7075\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3045 - accuracy: 0.9575 - val_loss: 1.7670 - val_accuracy: 0.6857\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2999 - accuracy: 0.9596 - val_loss: 1.5100 - val_accuracy: 0.7122\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2911 - accuracy: 0.9641 - val_loss: 1.8294 - val_accuracy: 0.6821\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3036 - accuracy: 0.9602 - val_loss: 2.0206 - val_accuracy: 0.6741\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2996 - accuracy: 0.9618 - val_loss: 1.5881 - val_accuracy: 0.7162\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2910 - accuracy: 0.9652 - val_loss: 1.7453 - val_accuracy: 0.7105\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2861 - accuracy: 0.9670 - val_loss: 1.4237 - val_accuracy: 0.7384\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2906 - accuracy: 0.9658 - val_loss: 2.4982 - val_accuracy: 0.6328\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2897 - accuracy: 0.9663 - val_loss: 2.1991 - val_accuracy: 0.6513\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2912 - accuracy: 0.9667 - val_loss: 2.1270 - val_accuracy: 0.6725\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2991 - accuracy: 0.9628 - val_loss: 2.0983 - val_accuracy: 0.6515\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2875 - accuracy: 0.9683 - val_loss: 2.1218 - val_accuracy: 0.6570\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2826 - accuracy: 0.9697 - val_loss: 1.7373 - val_accuracy: 0.7129\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2835 - accuracy: 0.9696 - val_loss: 1.8843 - val_accuracy: 0.6913\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2897 - accuracy: 0.9680 - val_loss: 1.6875 - val_accuracy: 0.7049\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2876 - accuracy: 0.9680 - val_loss: 1.5672 - val_accuracy: 0.7156\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2951 - accuracy: 0.9662 - val_loss: 1.7006 - val_accuracy: 0.7140\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2769 - accuracy: 0.9727 - val_loss: 1.5394 - val_accuracy: 0.7181\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2844 - accuracy: 0.9696 - val_loss: 2.7910 - val_accuracy: 0.6227\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2889 - accuracy: 0.9687 - val_loss: 1.7665 - val_accuracy: 0.7062\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2844 - accuracy: 0.9697 - val_loss: 1.6894 - val_accuracy: 0.7139\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2808 - accuracy: 0.9713 - val_loss: 2.1593 - val_accuracy: 0.6760\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2884 - accuracy: 0.9686 - val_loss: 1.8025 - val_accuracy: 0.7025\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2826 - accuracy: 0.9719 - val_loss: 1.5794 - val_accuracy: 0.7335\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2799 - accuracy: 0.9729 - val_loss: 1.7036 - val_accuracy: 0.7135\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2884 - accuracy: 0.9694 - val_loss: 2.0219 - val_accuracy: 0.6998\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2828 - accuracy: 0.9719 - val_loss: 1.5300 - val_accuracy: 0.7473\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2750 - accuracy: 0.9756 - val_loss: 1.7931 - val_accuracy: 0.7200\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2820 - accuracy: 0.9716 - val_loss: 2.5117 - val_accuracy: 0.6387\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2889 - accuracy: 0.9706 - val_loss: 1.5579 - val_accuracy: 0.7307\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2749 - accuracy: 0.9751 - val_loss: 2.2853 - val_accuracy: 0.6520\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2806 - accuracy: 0.9724 - val_loss: 1.5721 - val_accuracy: 0.7429\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2748 - accuracy: 0.9749 - val_loss: 1.7916 - val_accuracy: 0.7065\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2759 - accuracy: 0.9747 - val_loss: 1.7805 - val_accuracy: 0.7254\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2823 - accuracy: 0.9714 - val_loss: 1.8374 - val_accuracy: 0.7154\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2844 - accuracy: 0.9717 - val_loss: 1.8069 - val_accuracy: 0.7254\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2755 - accuracy: 0.9748 - val_loss: 1.8393 - val_accuracy: 0.7074\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2684 - accuracy: 0.9771 - val_loss: 1.8968 - val_accuracy: 0.7118\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2856 - accuracy: 0.9704 - val_loss: 1.9278 - val_accuracy: 0.7109\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2850 - accuracy: 0.9714 - val_loss: 1.8487 - val_accuracy: 0.6968\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2759 - accuracy: 0.9749 - val_loss: 2.4557 - val_accuracy: 0.6730\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2793 - accuracy: 0.9728 - val_loss: 2.5198 - val_accuracy: 0.6599\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2685 - accuracy: 0.9775 - val_loss: 2.0612 - val_accuracy: 0.7031\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2747 - accuracy: 0.9756 - val_loss: 1.8938 - val_accuracy: 0.7089\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2768 - accuracy: 0.9746 - val_loss: 1.7608 - val_accuracy: 0.7248\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2773 - accuracy: 0.9740 - val_loss: 1.7851 - val_accuracy: 0.7164\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2680 - accuracy: 0.9769 - val_loss: 1.6831 - val_accuracy: 0.7380\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2761 - accuracy: 0.9741 - val_loss: 1.7172 - val_accuracy: 0.7212\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2810 - accuracy: 0.9725 - val_loss: 2.1927 - val_accuracy: 0.6790\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2734 - accuracy: 0.9757 - val_loss: 1.7047 - val_accuracy: 0.7337\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2716 - accuracy: 0.9764 - val_loss: 1.7906 - val_accuracy: 0.7131\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2719 - accuracy: 0.9758 - val_loss: 2.4484 - val_accuracy: 0.6553\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2756 - accuracy: 0.9743 - val_loss: 1.9201 - val_accuracy: 0.6978\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2717 - accuracy: 0.9759 - val_loss: 2.1895 - val_accuracy: 0.6768\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2691 - accuracy: 0.9765 - val_loss: 2.0778 - val_accuracy: 0.6737\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2679 - accuracy: 0.9769 - val_loss: 1.7881 - val_accuracy: 0.7250\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2699 - accuracy: 0.9770 - val_loss: 1.5475 - val_accuracy: 0.7486\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2663 - accuracy: 0.9765 - val_loss: 1.6439 - val_accuracy: 0.7245\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2693 - accuracy: 0.9762 - val_loss: 1.8987 - val_accuracy: 0.7053\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2790 - accuracy: 0.9731 - val_loss: 1.5771 - val_accuracy: 0.7408\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2635 - accuracy: 0.9785 - val_loss: 2.4527 - val_accuracy: 0.6724\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2746 - accuracy: 0.9745 - val_loss: 2.0529 - val_accuracy: 0.6980\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2671 - accuracy: 0.9777 - val_loss: 2.2792 - val_accuracy: 0.6786\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2661 - accuracy: 0.9768 - val_loss: 1.6933 - val_accuracy: 0.7383\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2664 - accuracy: 0.9773 - val_loss: 1.5670 - val_accuracy: 0.7272\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2698 - accuracy: 0.9753 - val_loss: 1.7453 - val_accuracy: 0.7355\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2669 - accuracy: 0.9767 - val_loss: 1.9033 - val_accuracy: 0.7147\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2664 - accuracy: 0.9774 - val_loss: 1.5944 - val_accuracy: 0.7459\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2612 - accuracy: 0.9787 - val_loss: 1.8364 - val_accuracy: 0.7214\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2605 - accuracy: 0.9783 - val_loss: 1.9690 - val_accuracy: 0.6913\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2720 - accuracy: 0.9748 - val_loss: 2.3416 - val_accuracy: 0.6782\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2672 - accuracy: 0.9766 - val_loss: 2.0078 - val_accuracy: 0.7016\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2633 - accuracy: 0.9772 - val_loss: 1.5529 - val_accuracy: 0.7541\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2640 - accuracy: 0.9775 - val_loss: 1.5540 - val_accuracy: 0.7569\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2589 - accuracy: 0.9788 - val_loss: 1.5405 - val_accuracy: 0.7502\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2676 - accuracy: 0.9754 - val_loss: 1.5870 - val_accuracy: 0.7465\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2681 - accuracy: 0.9765 - val_loss: 1.6315 - val_accuracy: 0.7413\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2596 - accuracy: 0.9785 - val_loss: 1.6578 - val_accuracy: 0.7395\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2600 - accuracy: 0.9777 - val_loss: 1.8216 - val_accuracy: 0.7159\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2708 - accuracy: 0.9744 - val_loss: 2.0006 - val_accuracy: 0.6932\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2578 - accuracy: 0.9796 - val_loss: 1.5631 - val_accuracy: 0.7534\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2567 - accuracy: 0.9786 - val_loss: 1.7865 - val_accuracy: 0.7307\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2622 - accuracy: 0.9775 - val_loss: 1.6043 - val_accuracy: 0.7498\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2668 - accuracy: 0.9760 - val_loss: 1.5823 - val_accuracy: 0.7458\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2650 - accuracy: 0.9772 - val_loss: 1.5937 - val_accuracy: 0.7296\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2546 - accuracy: 0.9805 - val_loss: 1.6590 - val_accuracy: 0.7475\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2573 - accuracy: 0.9795 - val_loss: 2.0260 - val_accuracy: 0.7202\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2645 - accuracy: 0.9769 - val_loss: 1.7223 - val_accuracy: 0.7198\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2625 - accuracy: 0.9782 - val_loss: 1.7767 - val_accuracy: 0.7252\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2497 - accuracy: 0.9818 - val_loss: 1.8836 - val_accuracy: 0.7213\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2626 - accuracy: 0.9770 - val_loss: 1.6565 - val_accuracy: 0.7331\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2614 - accuracy: 0.9767 - val_loss: 1.7973 - val_accuracy: 0.7085\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2549 - accuracy: 0.9792 - val_loss: 1.8650 - val_accuracy: 0.7212\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2636 - accuracy: 0.9768 - val_loss: 1.8456 - val_accuracy: 0.7238\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2502 - accuracy: 0.9810 - val_loss: 1.6805 - val_accuracy: 0.7482\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2634 - accuracy: 0.9768 - val_loss: 1.4671 - val_accuracy: 0.7559\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2670 - accuracy: 0.9748 - val_loss: 1.6569 - val_accuracy: 0.7424\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2498 - accuracy: 0.9811 - val_loss: 1.9167 - val_accuracy: 0.7281\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2564 - accuracy: 0.9789 - val_loss: 1.7777 - val_accuracy: 0.7379\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2663 - accuracy: 0.9751 - val_loss: 1.7381 - val_accuracy: 0.7162\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2482 - accuracy: 0.9814 - val_loss: 2.1401 - val_accuracy: 0.6965\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2514 - accuracy: 0.9797 - val_loss: 1.7618 - val_accuracy: 0.7377\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2671 - accuracy: 0.9752 - val_loss: 1.3640 - val_accuracy: 0.7617\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2558 - accuracy: 0.9788 - val_loss: 1.3754 - val_accuracy: 0.7633\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2498 - accuracy: 0.9810 - val_loss: 1.7042 - val_accuracy: 0.7356\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2548 - accuracy: 0.9783 - val_loss: 1.4111 - val_accuracy: 0.7680\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2602 - accuracy: 0.9764 - val_loss: 1.4674 - val_accuracy: 0.7536\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2602 - accuracy: 0.9773 - val_loss: 2.2479 - val_accuracy: 0.6577\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2494 - accuracy: 0.9811 - val_loss: 1.9409 - val_accuracy: 0.7216\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2411 - accuracy: 0.9832 - val_loss: 1.6582 - val_accuracy: 0.7311\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2586 - accuracy: 0.9768 - val_loss: 1.7836 - val_accuracy: 0.7201\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2597 - accuracy: 0.9771 - val_loss: 2.1219 - val_accuracy: 0.7005\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2457 - accuracy: 0.9819 - val_loss: 2.0520 - val_accuracy: 0.6994\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2517 - accuracy: 0.9789 - val_loss: 1.5876 - val_accuracy: 0.7514\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2616 - accuracy: 0.9760 - val_loss: 2.5304 - val_accuracy: 0.6790\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2573 - accuracy: 0.9782 - val_loss: 1.4889 - val_accuracy: 0.7539\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2409 - accuracy: 0.9835 - val_loss: 1.9338 - val_accuracy: 0.7280\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2540 - accuracy: 0.9788 - val_loss: 1.5965 - val_accuracy: 0.7476\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2453 - accuracy: 0.9815 - val_loss: 1.6685 - val_accuracy: 0.7328\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2578 - accuracy: 0.9767 - val_loss: 1.9763 - val_accuracy: 0.7185\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2463 - accuracy: 0.9809 - val_loss: 1.6016 - val_accuracy: 0.7409\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2482 - accuracy: 0.9800 - val_loss: 1.5937 - val_accuracy: 0.7457\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9791 - val_loss: 1.8158 - val_accuracy: 0.7172\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2564 - accuracy: 0.9776 - val_loss: 1.6415 - val_accuracy: 0.7360\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2448 - accuracy: 0.9816 - val_loss: 1.6299 - val_accuracy: 0.7401\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2475 - accuracy: 0.9802 - val_loss: 1.5972 - val_accuracy: 0.7422\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2614 - accuracy: 0.9762 - val_loss: 1.8069 - val_accuracy: 0.7111\n",
      "Model training complete and saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21515d61b80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.7, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2584559",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3b883ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dcf9d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f596239",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918404b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   448         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   544         ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 32)  128         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 32)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 32)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 16, 16, 32)  128         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 8, 8, 64)     0           ['conv2d_79[0][0]',              \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 64)     0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 64)    256         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 8, 8, 64)     0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 64)     0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 64)    256         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['activation_73[0][0]',          \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 64)     0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 64)          0           ['activation_75[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 13s 29ms/step - loss: 1.5439 - accuracy: 0.4878 - val_loss: 1.6642 - val_accuracy: 0.4686\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 1.1471 - accuracy: 0.6424 - val_loss: 1.4389 - val_accuracy: 0.5393\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.9681 - accuracy: 0.7103 - val_loss: 1.4398 - val_accuracy: 0.5683\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.8481 - accuracy: 0.7538 - val_loss: 1.1772 - val_accuracy: 0.6468\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7613 - accuracy: 0.7854 - val_loss: 1.6040 - val_accuracy: 0.5633\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.6882 - accuracy: 0.8112 - val_loss: 1.1919 - val_accuracy: 0.6838\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.6298 - accuracy: 0.8342 - val_loss: 0.9465 - val_accuracy: 0.7353\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.5775 - accuracy: 0.8524 - val_loss: 1.4504 - val_accuracy: 0.6569\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.5322 - accuracy: 0.8689 - val_loss: 1.2473 - val_accuracy: 0.6909\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.4922 - accuracy: 0.8844 - val_loss: 2.1093 - val_accuracy: 0.5433\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.4538 - accuracy: 0.8976 - val_loss: 1.3727 - val_accuracy: 0.6717\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.4178 - accuracy: 0.9124 - val_loss: 1.7094 - val_accuracy: 0.6335\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3982 - accuracy: 0.9198 - val_loss: 1.4468 - val_accuracy: 0.6790\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3773 - accuracy: 0.9265 - val_loss: 1.6095 - val_accuracy: 0.6572\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3595 - accuracy: 0.9345 - val_loss: 1.2040 - val_accuracy: 0.7359\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3450 - accuracy: 0.9393 - val_loss: 2.1194 - val_accuracy: 0.6342\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3310 - accuracy: 0.9452 - val_loss: 1.9999 - val_accuracy: 0.6350\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3235 - accuracy: 0.9490 - val_loss: 2.3747 - val_accuracy: 0.6335\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 10s 27ms/step - loss: 0.3170 - accuracy: 0.9509 - val_loss: 1.2639 - val_accuracy: 0.7449\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3112 - accuracy: 0.9541 - val_loss: 1.7398 - val_accuracy: 0.6786\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.3078 - accuracy: 0.9556 - val_loss: 1.9678 - val_accuracy: 0.6601\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3069 - accuracy: 0.9568 - val_loss: 1.4052 - val_accuracy: 0.7098\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3013 - accuracy: 0.9582 - val_loss: 1.6088 - val_accuracy: 0.7041\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.3079 - accuracy: 0.9568 - val_loss: 1.8479 - val_accuracy: 0.6886\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2859 - accuracy: 0.9656 - val_loss: 2.0429 - val_accuracy: 0.6555\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2999 - accuracy: 0.9606 - val_loss: 1.4945 - val_accuracy: 0.7237\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2993 - accuracy: 0.9623 - val_loss: 1.9072 - val_accuracy: 0.6588\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2949 - accuracy: 0.9632 - val_loss: 2.5092 - val_accuracy: 0.6434\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2972 - accuracy: 0.9632 - val_loss: 1.6691 - val_accuracy: 0.7082\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2846 - accuracy: 0.9678 - val_loss: 1.8619 - val_accuracy: 0.6583\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2933 - accuracy: 0.9649 - val_loss: 1.8809 - val_accuracy: 0.6849\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2916 - accuracy: 0.9655 - val_loss: 2.1009 - val_accuracy: 0.6726\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2870 - accuracy: 0.9674 - val_loss: 2.5459 - val_accuracy: 0.6027\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2915 - accuracy: 0.9665 - val_loss: 1.9721 - val_accuracy: 0.6806\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2876 - accuracy: 0.9676 - val_loss: 1.4545 - val_accuracy: 0.7404\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2865 - accuracy: 0.9682 - val_loss: 1.8003 - val_accuracy: 0.7186\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2783 - accuracy: 0.9713 - val_loss: 1.8896 - val_accuracy: 0.7162\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2917 - accuracy: 0.9671 - val_loss: 1.8679 - val_accuracy: 0.6944\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2745 - accuracy: 0.9732 - val_loss: 1.5113 - val_accuracy: 0.7286\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2928 - accuracy: 0.9668 - val_loss: 2.2665 - val_accuracy: 0.6774\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2883 - accuracy: 0.9684 - val_loss: 1.4691 - val_accuracy: 0.7272\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2797 - accuracy: 0.9721 - val_loss: 1.4713 - val_accuracy: 0.7386\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2813 - accuracy: 0.9707 - val_loss: 1.8383 - val_accuracy: 0.7064\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2809 - accuracy: 0.9724 - val_loss: 1.8324 - val_accuracy: 0.6911\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2729 - accuracy: 0.9746 - val_loss: 1.4590 - val_accuracy: 0.7355\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2827 - accuracy: 0.9711 - val_loss: 1.6025 - val_accuracy: 0.7211\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2870 - accuracy: 0.9689 - val_loss: 1.6717 - val_accuracy: 0.7229\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2810 - accuracy: 0.9718 - val_loss: 1.6661 - val_accuracy: 0.7146\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2810 - accuracy: 0.9717 - val_loss: 1.4133 - val_accuracy: 0.7442\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2758 - accuracy: 0.9728 - val_loss: 1.8439 - val_accuracy: 0.7009\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2826 - accuracy: 0.9716 - val_loss: 1.6660 - val_accuracy: 0.7220\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2824 - accuracy: 0.9717 - val_loss: 1.7582 - val_accuracy: 0.7158\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2735 - accuracy: 0.9749 - val_loss: 2.0154 - val_accuracy: 0.7051\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2778 - accuracy: 0.9733 - val_loss: 1.7300 - val_accuracy: 0.7144\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2756 - accuracy: 0.9738 - val_loss: 2.0395 - val_accuracy: 0.6856\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2734 - accuracy: 0.9743 - val_loss: 1.6571 - val_accuracy: 0.7222\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2876 - accuracy: 0.9702 - val_loss: 2.8113 - val_accuracy: 0.6174\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2756 - accuracy: 0.9744 - val_loss: 2.4462 - val_accuracy: 0.6734\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2736 - accuracy: 0.9743 - val_loss: 2.0606 - val_accuracy: 0.6867\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2759 - accuracy: 0.9742 - val_loss: 1.8118 - val_accuracy: 0.7215\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2668 - accuracy: 0.9771 - val_loss: 1.9687 - val_accuracy: 0.7137\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2802 - accuracy: 0.9723 - val_loss: 1.5852 - val_accuracy: 0.7255\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2710 - accuracy: 0.9761 - val_loss: 1.4136 - val_accuracy: 0.7587\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2845 - accuracy: 0.9714 - val_loss: 1.7090 - val_accuracy: 0.7243\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2694 - accuracy: 0.9766 - val_loss: 1.8162 - val_accuracy: 0.7029\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2738 - accuracy: 0.9747 - val_loss: 2.0706 - val_accuracy: 0.6854\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2723 - accuracy: 0.9749 - val_loss: 1.5255 - val_accuracy: 0.7351\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2787 - accuracy: 0.9726 - val_loss: 1.6995 - val_accuracy: 0.7281\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2661 - accuracy: 0.9771 - val_loss: 1.9316 - val_accuracy: 0.6967\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2791 - accuracy: 0.9731 - val_loss: 1.8611 - val_accuracy: 0.7045\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2777 - accuracy: 0.9730 - val_loss: 1.6968 - val_accuracy: 0.7183\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2697 - accuracy: 0.9761 - val_loss: 1.7030 - val_accuracy: 0.7295\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2663 - accuracy: 0.9772 - val_loss: 1.7895 - val_accuracy: 0.7416\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2733 - accuracy: 0.9745 - val_loss: 1.9416 - val_accuracy: 0.7191\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2758 - accuracy: 0.9743 - val_loss: 1.6719 - val_accuracy: 0.7235\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2639 - accuracy: 0.9779 - val_loss: 2.0270 - val_accuracy: 0.6875\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2712 - accuracy: 0.9755 - val_loss: 1.7707 - val_accuracy: 0.7028\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2702 - accuracy: 0.9760 - val_loss: 1.6279 - val_accuracy: 0.7370\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2662 - accuracy: 0.9774 - val_loss: 2.4085 - val_accuracy: 0.6771\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2743 - accuracy: 0.9739 - val_loss: 2.1822 - val_accuracy: 0.6784\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2617 - accuracy: 0.9788 - val_loss: 1.4478 - val_accuracy: 0.7520\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2757 - accuracy: 0.9735 - val_loss: 1.6495 - val_accuracy: 0.7350\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2638 - accuracy: 0.9778 - val_loss: 1.8574 - val_accuracy: 0.7178\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2687 - accuracy: 0.9755 - val_loss: 2.5515 - val_accuracy: 0.6286\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2534 - accuracy: 0.9808 - val_loss: 1.5520 - val_accuracy: 0.7400\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2763 - accuracy: 0.9732 - val_loss: 1.7416 - val_accuracy: 0.7145\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2769 - accuracy: 0.9734 - val_loss: 1.8951 - val_accuracy: 0.7100\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2566 - accuracy: 0.9804 - val_loss: 1.8049 - val_accuracy: 0.6964\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2624 - accuracy: 0.9778 - val_loss: 1.7638 - val_accuracy: 0.7187\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2657 - accuracy: 0.9758 - val_loss: 1.5325 - val_accuracy: 0.7459\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2622 - accuracy: 0.9779 - val_loss: 1.4817 - val_accuracy: 0.7605\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2641 - accuracy: 0.9766 - val_loss: 1.9298 - val_accuracy: 0.6951\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2667 - accuracy: 0.9756 - val_loss: 1.7866 - val_accuracy: 0.7081\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2606 - accuracy: 0.9785 - val_loss: 1.8740 - val_accuracy: 0.7182\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2652 - accuracy: 0.9759 - val_loss: 1.7624 - val_accuracy: 0.7405\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2644 - accuracy: 0.9769 - val_loss: 1.4389 - val_accuracy: 0.7697\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2590 - accuracy: 0.9794 - val_loss: 2.0632 - val_accuracy: 0.6971\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2608 - accuracy: 0.9777 - val_loss: 2.2981 - val_accuracy: 0.6738\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2730 - accuracy: 0.9726 - val_loss: 1.4555 - val_accuracy: 0.7565\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2529 - accuracy: 0.9814 - val_loss: 1.4967 - val_accuracy: 0.7517\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2607 - accuracy: 0.9782 - val_loss: 1.6383 - val_accuracy: 0.7359\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2647 - accuracy: 0.9762 - val_loss: 1.8647 - val_accuracy: 0.7225\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2631 - accuracy: 0.9773 - val_loss: 1.8403 - val_accuracy: 0.7169\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.2551 - accuracy: 0.9794 - val_loss: 2.3692 - val_accuracy: 0.6850\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2640 - accuracy: 0.9763 - val_loss: 1.5598 - val_accuracy: 0.7388\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2636 - accuracy: 0.9765 - val_loss: 2.6915 - val_accuracy: 0.6535\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2570 - accuracy: 0.9787 - val_loss: 2.0703 - val_accuracy: 0.6839\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2591 - accuracy: 0.9784 - val_loss: 1.6347 - val_accuracy: 0.7397\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2553 - accuracy: 0.9792 - val_loss: 1.8717 - val_accuracy: 0.7180\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2633 - accuracy: 0.9765 - val_loss: 1.7782 - val_accuracy: 0.7332\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2595 - accuracy: 0.9777 - val_loss: 1.8273 - val_accuracy: 0.7288\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2599 - accuracy: 0.9779 - val_loss: 1.5756 - val_accuracy: 0.7485\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2630 - accuracy: 0.9767 - val_loss: 1.8390 - val_accuracy: 0.7081\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2567 - accuracy: 0.9781 - val_loss: 1.8040 - val_accuracy: 0.7199\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2549 - accuracy: 0.9790 - val_loss: 1.5886 - val_accuracy: 0.7402\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.2521 - accuracy: 0.9791 - val_loss: 1.7181 - val_accuracy: 0.7204\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2629 - accuracy: 0.9761 - val_loss: 1.7957 - val_accuracy: 0.7254\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2521 - accuracy: 0.9800 - val_loss: 1.6889 - val_accuracy: 0.7226\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2608 - accuracy: 0.9759 - val_loss: 1.6186 - val_accuracy: 0.7508\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2544 - accuracy: 0.9790 - val_loss: 1.7965 - val_accuracy: 0.7253\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2526 - accuracy: 0.9791 - val_loss: 2.4632 - val_accuracy: 0.7099\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2586 - accuracy: 0.9771 - val_loss: 2.1534 - val_accuracy: 0.6911\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2556 - accuracy: 0.9786 - val_loss: 1.5450 - val_accuracy: 0.7330\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2610 - accuracy: 0.9765 - val_loss: 1.5037 - val_accuracy: 0.7701\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2477 - accuracy: 0.9815 - val_loss: 1.5983 - val_accuracy: 0.7457\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2446 - accuracy: 0.9823 - val_loss: 1.6823 - val_accuracy: 0.7446\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2552 - accuracy: 0.9782 - val_loss: 1.7169 - val_accuracy: 0.7409\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2604 - accuracy: 0.9764 - val_loss: 1.5719 - val_accuracy: 0.7501\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2485 - accuracy: 0.9802 - val_loss: 2.4271 - val_accuracy: 0.6550\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2618 - accuracy: 0.9759 - val_loss: 2.4762 - val_accuracy: 0.6733\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2471 - accuracy: 0.9817 - val_loss: 2.0462 - val_accuracy: 0.6910\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2516 - accuracy: 0.9792 - val_loss: 1.5046 - val_accuracy: 0.7590\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2576 - accuracy: 0.9774 - val_loss: 1.9578 - val_accuracy: 0.7082\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2504 - accuracy: 0.9793 - val_loss: 1.5505 - val_accuracy: 0.7526\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2575 - accuracy: 0.9775 - val_loss: 1.9727 - val_accuracy: 0.7123\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2473 - accuracy: 0.9812 - val_loss: 1.7019 - val_accuracy: 0.7273\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2472 - accuracy: 0.9809 - val_loss: 1.7769 - val_accuracy: 0.7441\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2565 - accuracy: 0.9769 - val_loss: 1.9469 - val_accuracy: 0.6966\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2454 - accuracy: 0.9814 - val_loss: 1.9050 - val_accuracy: 0.7328\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2545 - accuracy: 0.9780 - val_loss: 1.9341 - val_accuracy: 0.7143\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2450 - accuracy: 0.9809 - val_loss: 1.9140 - val_accuracy: 0.7163\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2539 - accuracy: 0.9776 - val_loss: 1.5477 - val_accuracy: 0.7467\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2533 - accuracy: 0.9788 - val_loss: 2.2695 - val_accuracy: 0.6933\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2429 - accuracy: 0.9819 - val_loss: 1.9367 - val_accuracy: 0.7209\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2431 - accuracy: 0.9818 - val_loss: 1.9706 - val_accuracy: 0.7129\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2567 - accuracy: 0.9765 - val_loss: 1.8722 - val_accuracy: 0.7227\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2447 - accuracy: 0.9816 - val_loss: 1.5171 - val_accuracy: 0.7475\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2467 - accuracy: 0.9806 - val_loss: 1.5236 - val_accuracy: 0.7389\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2581 - accuracy: 0.9766 - val_loss: 1.3933 - val_accuracy: 0.7661\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.2364 - accuracy: 0.9842 - val_loss: 1.8594 - val_accuracy: 0.7182\n",
      "Model training complete and saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e344b4f9d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.9, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e6e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
