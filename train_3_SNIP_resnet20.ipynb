{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaa0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from models.resnet20 import build_resnet20\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# -----------------------------\n",
    "# SNIP Pruning Function\n",
    "# -----------------------------\n",
    "def snip_prune(model, x_batch, y_batch, sparsity):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    snip_scores = [tf.abs(w * g) for w, g in zip(model.trainable_variables, grads) if g is not None and 'kernel' in w.name]\n",
    "    all_scores = tf.concat([tf.reshape(score, [-1]) for score in snip_scores], axis=0)\n",
    "\n",
    "    k = int((1 - sparsity) * tf.size(all_scores).numpy())\n",
    "    threshold = tf.sort(all_scores)[k].numpy()\n",
    "\n",
    "    masks = [(tf.abs(score) > threshold).numpy().astype(np.float32) for score in snip_scores]\n",
    "    mask_idx = 0\n",
    "\n",
    "    for i, var in enumerate(model.trainable_variables):\n",
    "        if 'kernel' in var.name:\n",
    "            var.assign(var * masks[mask_idx])\n",
    "            mask_idx += 1\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def run_snip_training(sparsity=0.5, batch_size=128, epochs=150):\n",
    "    # Load and preprocess data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "    # Build and initialize model\n",
    "    model = build_resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    # SNIP pruning on small batch\n",
    "    x_sample = x_train[:batch_size]\n",
    "    y_sample = y_train[:batch_size]\n",
    "    snip_prune(model, x_sample, y_sample, sparsity)\n",
    "\n",
    "    # Compile and train\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(x_test, y_test),\n",
    "              verbose=1)\n",
    "\n",
    "    print(\"✅ SNIP pruning and training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f7ab000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 16)  64          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 16)  64          ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 32, 32, 16)  64          ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 32, 16)   0           ['activation_19[0][0]',          \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 16)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32, 32, 16)  64          ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 32, 32, 16)  64          ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 32, 32, 16)   0           ['activation_21[0][0]',          \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 16)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 32, 32, 16)   0           ['activation_23[0][0]',          \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 16)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   544         ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['conv2d_30[0][0]',              \n",
      "                                                                  'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_27[0][0]',          \n",
      "                                                                  'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_29[0][0]',          \n",
      "                                                                  'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 64)    256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['conv2d_37[0][0]',              \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 64)     0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['activation_33[0][0]',          \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 64)     0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['activation_35[0][0]',          \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['activation_37[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 36s 83ms/step - loss: 1.5184 - accuracy: 0.4857 - val_loss: 1.9150 - val_accuracy: 0.3916\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 1.1181 - accuracy: 0.6355 - val_loss: 1.2876 - val_accuracy: 0.5954\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.9342 - accuracy: 0.7068 - val_loss: 1.2620 - val_accuracy: 0.6167\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.8144 - accuracy: 0.7507 - val_loss: 1.0010 - val_accuracy: 0.6882\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.7205 - accuracy: 0.7879 - val_loss: 1.1632 - val_accuracy: 0.6509\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6493 - accuracy: 0.8139 - val_loss: 1.1758 - val_accuracy: 0.6780\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5923 - accuracy: 0.8365 - val_loss: 1.0101 - val_accuracy: 0.7076\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5476 - accuracy: 0.8533 - val_loss: 1.2502 - val_accuracy: 0.6669\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.5017 - accuracy: 0.8704 - val_loss: 0.9986 - val_accuracy: 0.7239\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.4640 - accuracy: 0.8865 - val_loss: 1.2558 - val_accuracy: 0.6627\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.4341 - accuracy: 0.8977 - val_loss: 1.1354 - val_accuracy: 0.7245\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3878 - accuracy: 0.9151 - val_loss: 1.4589 - val_accuracy: 0.6637\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3808 - accuracy: 0.9179 - val_loss: 1.3728 - val_accuracy: 0.6865\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3521 - accuracy: 0.9300 - val_loss: 1.3057 - val_accuracy: 0.7102\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3367 - accuracy: 0.9360 - val_loss: 1.6838 - val_accuracy: 0.6689\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3272 - accuracy: 0.9414 - val_loss: 1.9722 - val_accuracy: 0.6424\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3235 - accuracy: 0.9429 - val_loss: 1.3647 - val_accuracy: 0.7302\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3055 - accuracy: 0.9502 - val_loss: 1.5577 - val_accuracy: 0.7129\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3021 - accuracy: 0.9516 - val_loss: 1.3682 - val_accuracy: 0.7192\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3004 - accuracy: 0.9531 - val_loss: 1.5726 - val_accuracy: 0.7029\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2856 - accuracy: 0.9597 - val_loss: 1.8674 - val_accuracy: 0.6850\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2967 - accuracy: 0.9560 - val_loss: 1.4329 - val_accuracy: 0.7295\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2883 - accuracy: 0.9591 - val_loss: 1.3365 - val_accuracy: 0.7344\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2839 - accuracy: 0.9611 - val_loss: 2.1342 - val_accuracy: 0.6470\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2780 - accuracy: 0.9641 - val_loss: 2.2969 - val_accuracy: 0.6436\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2845 - accuracy: 0.9614 - val_loss: 1.4288 - val_accuracy: 0.7450\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2816 - accuracy: 0.9640 - val_loss: 1.5690 - val_accuracy: 0.7124\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2790 - accuracy: 0.9647 - val_loss: 1.4496 - val_accuracy: 0.7108\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2770 - accuracy: 0.9664 - val_loss: 1.9184 - val_accuracy: 0.6810\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2797 - accuracy: 0.9655 - val_loss: 2.3331 - val_accuracy: 0.6406\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2794 - accuracy: 0.9661 - val_loss: 1.4253 - val_accuracy: 0.7335\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2779 - accuracy: 0.9679 - val_loss: 1.6349 - val_accuracy: 0.7207\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2701 - accuracy: 0.9697 - val_loss: 1.7233 - val_accuracy: 0.7101\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2756 - accuracy: 0.9681 - val_loss: 1.9370 - val_accuracy: 0.7032\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2791 - accuracy: 0.9670 - val_loss: 1.5829 - val_accuracy: 0.7264\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2708 - accuracy: 0.9706 - val_loss: 1.8766 - val_accuracy: 0.6970\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2747 - accuracy: 0.9700 - val_loss: 1.7291 - val_accuracy: 0.6998\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2739 - accuracy: 0.9701 - val_loss: 1.7918 - val_accuracy: 0.6997\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2798 - accuracy: 0.9680 - val_loss: 2.0817 - val_accuracy: 0.6607\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2686 - accuracy: 0.9717 - val_loss: 1.5789 - val_accuracy: 0.7349\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2697 - accuracy: 0.9729 - val_loss: 1.6011 - val_accuracy: 0.7220\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2687 - accuracy: 0.9719 - val_loss: 1.4721 - val_accuracy: 0.7537\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2805 - accuracy: 0.9691 - val_loss: 1.9787 - val_accuracy: 0.6973\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2709 - accuracy: 0.9726 - val_loss: 2.0856 - val_accuracy: 0.6732\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2659 - accuracy: 0.9740 - val_loss: 1.7805 - val_accuracy: 0.7073\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2713 - accuracy: 0.9716 - val_loss: 1.8411 - val_accuracy: 0.7163\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2722 - accuracy: 0.9713 - val_loss: 1.7837 - val_accuracy: 0.7210\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2650 - accuracy: 0.9752 - val_loss: 1.5436 - val_accuracy: 0.7395\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2730 - accuracy: 0.9718 - val_loss: 1.8872 - val_accuracy: 0.7119\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2679 - accuracy: 0.9748 - val_loss: 1.5730 - val_accuracy: 0.7347\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2708 - accuracy: 0.9730 - val_loss: 1.5913 - val_accuracy: 0.7451\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2721 - accuracy: 0.9726 - val_loss: 1.4493 - val_accuracy: 0.7535\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2693 - accuracy: 0.9733 - val_loss: 1.4246 - val_accuracy: 0.7510\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2698 - accuracy: 0.9735 - val_loss: 1.6231 - val_accuracy: 0.7201\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2745 - accuracy: 0.9724 - val_loss: 1.8019 - val_accuracy: 0.7129\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2594 - accuracy: 0.9773 - val_loss: 1.7744 - val_accuracy: 0.7325\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2687 - accuracy: 0.9745 - val_loss: 1.4501 - val_accuracy: 0.7640\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2667 - accuracy: 0.9752 - val_loss: 1.5257 - val_accuracy: 0.7492\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2612 - accuracy: 0.9765 - val_loss: 1.7622 - val_accuracy: 0.7241\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2774 - accuracy: 0.9715 - val_loss: 1.4819 - val_accuracy: 0.7408\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2577 - accuracy: 0.9785 - val_loss: 1.6472 - val_accuracy: 0.7295\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2652 - accuracy: 0.9750 - val_loss: 1.5862 - val_accuracy: 0.7300\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2665 - accuracy: 0.9750 - val_loss: 1.6551 - val_accuracy: 0.7285\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2651 - accuracy: 0.9756 - val_loss: 1.8330 - val_accuracy: 0.7271\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2600 - accuracy: 0.9771 - val_loss: 1.5640 - val_accuracy: 0.7303\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2639 - accuracy: 0.9762 - val_loss: 1.7214 - val_accuracy: 0.7186\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2711 - accuracy: 0.9733 - val_loss: 1.7761 - val_accuracy: 0.7004\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2576 - accuracy: 0.9782 - val_loss: 1.6678 - val_accuracy: 0.7234\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2636 - accuracy: 0.9759 - val_loss: 2.1204 - val_accuracy: 0.7049\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2711 - accuracy: 0.9735 - val_loss: 1.4458 - val_accuracy: 0.7579\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2592 - accuracy: 0.9782 - val_loss: 1.4143 - val_accuracy: 0.7612\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2601 - accuracy: 0.9776 - val_loss: 1.7153 - val_accuracy: 0.7271\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2575 - accuracy: 0.9774 - val_loss: 1.8720 - val_accuracy: 0.7128\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2618 - accuracy: 0.9759 - val_loss: 1.4416 - val_accuracy: 0.7646\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2686 - accuracy: 0.9747 - val_loss: 1.7914 - val_accuracy: 0.7214\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2568 - accuracy: 0.9786 - val_loss: 1.9531 - val_accuracy: 0.7006\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2635 - accuracy: 0.9752 - val_loss: 1.3678 - val_accuracy: 0.7619\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2622 - accuracy: 0.9764 - val_loss: 1.5783 - val_accuracy: 0.7473\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2559 - accuracy: 0.9787 - val_loss: 1.6314 - val_accuracy: 0.7465\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2585 - accuracy: 0.9772 - val_loss: 1.6342 - val_accuracy: 0.7289\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2651 - accuracy: 0.9740 - val_loss: 3.1247 - val_accuracy: 0.6177\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2563 - accuracy: 0.9781 - val_loss: 2.0722 - val_accuracy: 0.7062\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2541 - accuracy: 0.9789 - val_loss: 1.6327 - val_accuracy: 0.7310\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2654 - accuracy: 0.9752 - val_loss: 2.0512 - val_accuracy: 0.6979\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2509 - accuracy: 0.9799 - val_loss: 1.9680 - val_accuracy: 0.7101\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2547 - accuracy: 0.9774 - val_loss: 1.6839 - val_accuracy: 0.7296\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2621 - accuracy: 0.9756 - val_loss: 1.4106 - val_accuracy: 0.7611\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2654 - accuracy: 0.9747 - val_loss: 1.7931 - val_accuracy: 0.7081\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2542 - accuracy: 0.9789 - val_loss: 1.5508 - val_accuracy: 0.7493\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2557 - accuracy: 0.9779 - val_loss: 1.4248 - val_accuracy: 0.7643\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2510 - accuracy: 0.9798 - val_loss: 1.8469 - val_accuracy: 0.7247\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2610 - accuracy: 0.9760 - val_loss: 1.5022 - val_accuracy: 0.7623\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2577 - accuracy: 0.9768 - val_loss: 1.4532 - val_accuracy: 0.7575\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2503 - accuracy: 0.9795 - val_loss: 2.0070 - val_accuracy: 0.7230\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2598 - accuracy: 0.9765 - val_loss: 1.6539 - val_accuracy: 0.7282\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2576 - accuracy: 0.9768 - val_loss: 1.9649 - val_accuracy: 0.7118\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2585 - accuracy: 0.9764 - val_loss: 1.7185 - val_accuracy: 0.7347\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2504 - accuracy: 0.9798 - val_loss: 1.6288 - val_accuracy: 0.7339\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2522 - accuracy: 0.9789 - val_loss: 1.4365 - val_accuracy: 0.7638\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2574 - accuracy: 0.9774 - val_loss: 1.5714 - val_accuracy: 0.7460\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2486 - accuracy: 0.9806 - val_loss: 1.6595 - val_accuracy: 0.7485\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2534 - accuracy: 0.9780 - val_loss: 1.6523 - val_accuracy: 0.7365\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2627 - accuracy: 0.9745 - val_loss: 2.0336 - val_accuracy: 0.6776\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2537 - accuracy: 0.9783 - val_loss: 1.5553 - val_accuracy: 0.7503\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2472 - accuracy: 0.9802 - val_loss: 1.6730 - val_accuracy: 0.7327\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2567 - accuracy: 0.9770 - val_loss: 2.0733 - val_accuracy: 0.6967\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2484 - accuracy: 0.9800 - val_loss: 1.5974 - val_accuracy: 0.7514\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2507 - accuracy: 0.9796 - val_loss: 1.6778 - val_accuracy: 0.7233\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2411 - accuracy: 0.9823 - val_loss: 1.6020 - val_accuracy: 0.7435\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2607 - accuracy: 0.9753 - val_loss: 1.7255 - val_accuracy: 0.7451\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2498 - accuracy: 0.9798 - val_loss: 1.8173 - val_accuracy: 0.7257\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2449 - accuracy: 0.9804 - val_loss: 1.8473 - val_accuracy: 0.7427\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2619 - accuracy: 0.9745 - val_loss: 1.8068 - val_accuracy: 0.7421\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2526 - accuracy: 0.9785 - val_loss: 1.4647 - val_accuracy: 0.7628\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2360 - accuracy: 0.9839 - val_loss: 1.3511 - val_accuracy: 0.7668\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2574 - accuracy: 0.9768 - val_loss: 1.9240 - val_accuracy: 0.7001\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2481 - accuracy: 0.9799 - val_loss: 1.7031 - val_accuracy: 0.7265\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2485 - accuracy: 0.9788 - val_loss: 1.6396 - val_accuracy: 0.7219\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2519 - accuracy: 0.9782 - val_loss: 1.9705 - val_accuracy: 0.7280\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2437 - accuracy: 0.9815 - val_loss: 1.7856 - val_accuracy: 0.7365\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2523 - accuracy: 0.9777 - val_loss: 1.3859 - val_accuracy: 0.7711\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2509 - accuracy: 0.9782 - val_loss: 1.6321 - val_accuracy: 0.7349\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2486 - accuracy: 0.9793 - val_loss: 1.7003 - val_accuracy: 0.7475\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2403 - accuracy: 0.9811 - val_loss: 1.5019 - val_accuracy: 0.7711\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2547 - accuracy: 0.9771 - val_loss: 1.6578 - val_accuracy: 0.7369\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9778 - val_loss: 1.3744 - val_accuracy: 0.7767\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2454 - accuracy: 0.9803 - val_loss: 1.3206 - val_accuracy: 0.7751\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2448 - accuracy: 0.9804 - val_loss: 1.8594 - val_accuracy: 0.7253\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2459 - accuracy: 0.9803 - val_loss: 1.5590 - val_accuracy: 0.7552\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2483 - accuracy: 0.9784 - val_loss: 1.5289 - val_accuracy: 0.7531\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2465 - accuracy: 0.9793 - val_loss: 1.7853 - val_accuracy: 0.7226\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2468 - accuracy: 0.9790 - val_loss: 1.5593 - val_accuracy: 0.7373\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2473 - accuracy: 0.9791 - val_loss: 1.5784 - val_accuracy: 0.7576\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2466 - accuracy: 0.9794 - val_loss: 1.5778 - val_accuracy: 0.7625\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2383 - accuracy: 0.9821 - val_loss: 1.4959 - val_accuracy: 0.7512\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.2449 - accuracy: 0.9794 - val_loss: 1.8058 - val_accuracy: 0.7290\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2542 - accuracy: 0.9771 - val_loss: 1.3914 - val_accuracy: 0.7632\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2355 - accuracy: 0.9832 - val_loss: 1.8039 - val_accuracy: 0.7190\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2405 - accuracy: 0.9812 - val_loss: 1.6195 - val_accuracy: 0.7560\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2491 - accuracy: 0.9788 - val_loss: 1.4988 - val_accuracy: 0.7518\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2444 - accuracy: 0.9796 - val_loss: 1.4654 - val_accuracy: 0.7584\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2430 - accuracy: 0.9799 - val_loss: 1.5834 - val_accuracy: 0.7405\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2452 - accuracy: 0.9791 - val_loss: 1.6245 - val_accuracy: 0.7484\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2451 - accuracy: 0.9796 - val_loss: 1.5440 - val_accuracy: 0.7424\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2414 - accuracy: 0.9805 - val_loss: 1.7718 - val_accuracy: 0.7292\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2378 - accuracy: 0.9815 - val_loss: 1.8498 - val_accuracy: 0.7356\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2453 - accuracy: 0.9790 - val_loss: 2.2276 - val_accuracy: 0.6637\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2510 - accuracy: 0.9780 - val_loss: 1.5403 - val_accuracy: 0.7705\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2358 - accuracy: 0.9827 - val_loss: 1.9812 - val_accuracy: 0.7207\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2480 - accuracy: 0.9781 - val_loss: 1.5161 - val_accuracy: 0.7759\n",
      "✅ SNIP pruning and training complete.\n"
     ]
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.3, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e800df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 16)  64          ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 16)  64          ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 32, 32, 16)   0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 32, 32, 16)   0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32, 32, 16)  64          ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 32, 32, 16)   0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 32, 32, 16)   0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, 32, 16)  64          ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 32, 32, 16)   0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 16)   0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 16, 16, 32)   544         ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 16, 16, 32)   0           ['conv2d_51[0][0]',              \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 32)   0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 32)  128         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 16, 16, 32)   0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 32)   0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 32)  128         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 16, 16, 32)  128         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 16, 16, 32)   0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 16, 16, 32)   0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['conv2d_58[0][0]',              \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_54[0][0]',          \n",
      "                                                                  'batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 64)          0           ['activation_56[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           650         ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 29s 69ms/step - loss: 1.5535 - accuracy: 0.4823 - val_loss: 2.0043 - val_accuracy: 0.4451\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.1431 - accuracy: 0.6327 - val_loss: 1.4079 - val_accuracy: 0.5591\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.9691 - accuracy: 0.7033 - val_loss: 1.6694 - val_accuracy: 0.5344\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.8474 - accuracy: 0.7475 - val_loss: 1.5622 - val_accuracy: 0.5418\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.7558 - accuracy: 0.7804 - val_loss: 1.1413 - val_accuracy: 0.6632\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.6829 - accuracy: 0.8075 - val_loss: 1.0305 - val_accuracy: 0.7062\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6220 - accuracy: 0.8319 - val_loss: 1.0421 - val_accuracy: 0.7125\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5647 - accuracy: 0.8510 - val_loss: 1.3146 - val_accuracy: 0.6533\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5222 - accuracy: 0.8681 - val_loss: 1.1453 - val_accuracy: 0.6910\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4784 - accuracy: 0.8832 - val_loss: 1.2071 - val_accuracy: 0.6835\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4452 - accuracy: 0.8961 - val_loss: 1.0340 - val_accuracy: 0.7339\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.4111 - accuracy: 0.9097 - val_loss: 1.3098 - val_accuracy: 0.6994\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3857 - accuracy: 0.9195 - val_loss: 1.5212 - val_accuracy: 0.6719\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3629 - accuracy: 0.9284 - val_loss: 1.0966 - val_accuracy: 0.7426\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3493 - accuracy: 0.9335 - val_loss: 1.3773 - val_accuracy: 0.7088\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3338 - accuracy: 0.9402 - val_loss: 1.5412 - val_accuracy: 0.6779\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3271 - accuracy: 0.9454 - val_loss: 1.5295 - val_accuracy: 0.6930\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3109 - accuracy: 0.9502 - val_loss: 1.9723 - val_accuracy: 0.6505\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3047 - accuracy: 0.9521 - val_loss: 1.4007 - val_accuracy: 0.7115\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3048 - accuracy: 0.9538 - val_loss: 2.3161 - val_accuracy: 0.5997\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2986 - accuracy: 0.9561 - val_loss: 1.5127 - val_accuracy: 0.6955\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2968 - accuracy: 0.9571 - val_loss: 1.7010 - val_accuracy: 0.6714\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2862 - accuracy: 0.9610 - val_loss: 1.8671 - val_accuracy: 0.6797\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2983 - accuracy: 0.9577 - val_loss: 1.6023 - val_accuracy: 0.7221\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2862 - accuracy: 0.9632 - val_loss: 2.0666 - val_accuracy: 0.6532\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2912 - accuracy: 0.9613 - val_loss: 1.6377 - val_accuracy: 0.6988\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2875 - accuracy: 0.9638 - val_loss: 1.6059 - val_accuracy: 0.7091\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2793 - accuracy: 0.9668 - val_loss: 2.0346 - val_accuracy: 0.6656\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2829 - accuracy: 0.9652 - val_loss: 1.4480 - val_accuracy: 0.7336\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2806 - accuracy: 0.9668 - val_loss: 1.6202 - val_accuracy: 0.7202\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2871 - accuracy: 0.9656 - val_loss: 1.5042 - val_accuracy: 0.7183\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2833 - accuracy: 0.9664 - val_loss: 1.6852 - val_accuracy: 0.7080\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2823 - accuracy: 0.9677 - val_loss: 1.6496 - val_accuracy: 0.7109\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2726 - accuracy: 0.9711 - val_loss: 1.5429 - val_accuracy: 0.7309\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2833 - accuracy: 0.9665 - val_loss: 1.5385 - val_accuracy: 0.7212\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2825 - accuracy: 0.9678 - val_loss: 1.7072 - val_accuracy: 0.7109\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2755 - accuracy: 0.9709 - val_loss: 1.8169 - val_accuracy: 0.6957\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2882 - accuracy: 0.9660 - val_loss: 1.4080 - val_accuracy: 0.7371\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2736 - accuracy: 0.9724 - val_loss: 2.2652 - val_accuracy: 0.6537\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2760 - accuracy: 0.9706 - val_loss: 2.0106 - val_accuracy: 0.6889\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2728 - accuracy: 0.9717 - val_loss: 1.7516 - val_accuracy: 0.7221\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2726 - accuracy: 0.9714 - val_loss: 1.4548 - val_accuracy: 0.7425\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2778 - accuracy: 0.9704 - val_loss: 1.7446 - val_accuracy: 0.7148\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2864 - accuracy: 0.9682 - val_loss: 1.8490 - val_accuracy: 0.6988\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2644 - accuracy: 0.9759 - val_loss: 1.6649 - val_accuracy: 0.7310\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2721 - accuracy: 0.9731 - val_loss: 1.7451 - val_accuracy: 0.7205\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2821 - accuracy: 0.9697 - val_loss: 1.4498 - val_accuracy: 0.7482\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2756 - accuracy: 0.9711 - val_loss: 1.6283 - val_accuracy: 0.7216\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2768 - accuracy: 0.9723 - val_loss: 1.8847 - val_accuracy: 0.7217\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2704 - accuracy: 0.9742 - val_loss: 1.7825 - val_accuracy: 0.7007\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2679 - accuracy: 0.9753 - val_loss: 1.8084 - val_accuracy: 0.7068\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2782 - accuracy: 0.9714 - val_loss: 1.5103 - val_accuracy: 0.7376\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2751 - accuracy: 0.9723 - val_loss: 1.7563 - val_accuracy: 0.7138\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2716 - accuracy: 0.9748 - val_loss: 2.3233 - val_accuracy: 0.6642\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2676 - accuracy: 0.9749 - val_loss: 1.7657 - val_accuracy: 0.7192\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2782 - accuracy: 0.9721 - val_loss: 1.8719 - val_accuracy: 0.7061\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2654 - accuracy: 0.9767 - val_loss: 1.7386 - val_accuracy: 0.7327\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2757 - accuracy: 0.9728 - val_loss: 2.0600 - val_accuracy: 0.6775\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2677 - accuracy: 0.9757 - val_loss: 1.5998 - val_accuracy: 0.7288\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2733 - accuracy: 0.9732 - val_loss: 2.2864 - val_accuracy: 0.6784\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2706 - accuracy: 0.9744 - val_loss: 1.7224 - val_accuracy: 0.7139\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2750 - accuracy: 0.9731 - val_loss: 1.9551 - val_accuracy: 0.6972\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2766 - accuracy: 0.9728 - val_loss: 1.6428 - val_accuracy: 0.7264\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2618 - accuracy: 0.9783 - val_loss: 1.7293 - val_accuracy: 0.7231\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2662 - accuracy: 0.9758 - val_loss: 1.9806 - val_accuracy: 0.7034\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2677 - accuracy: 0.9757 - val_loss: 1.9291 - val_accuracy: 0.7139\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2720 - accuracy: 0.9737 - val_loss: 2.1482 - val_accuracy: 0.6720\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2690 - accuracy: 0.9751 - val_loss: 1.6019 - val_accuracy: 0.7269\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2723 - accuracy: 0.9748 - val_loss: 1.5963 - val_accuracy: 0.7401\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2598 - accuracy: 0.9783 - val_loss: 1.9408 - val_accuracy: 0.6875\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2683 - accuracy: 0.9749 - val_loss: 1.6789 - val_accuracy: 0.7254\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2663 - accuracy: 0.9757 - val_loss: 1.5286 - val_accuracy: 0.7405\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2615 - accuracy: 0.9776 - val_loss: 1.6919 - val_accuracy: 0.7403\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2585 - accuracy: 0.9778 - val_loss: 1.9369 - val_accuracy: 0.7133\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2716 - accuracy: 0.9739 - val_loss: 1.4524 - val_accuracy: 0.7522\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2709 - accuracy: 0.9743 - val_loss: 1.9923 - val_accuracy: 0.6996\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2651 - accuracy: 0.9760 - val_loss: 1.5540 - val_accuracy: 0.7405\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2613 - accuracy: 0.9773 - val_loss: 1.4962 - val_accuracy: 0.7555\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2597 - accuracy: 0.9780 - val_loss: 1.5973 - val_accuracy: 0.7478\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2681 - accuracy: 0.9739 - val_loss: 1.7981 - val_accuracy: 0.7284\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2640 - accuracy: 0.9762 - val_loss: 1.9769 - val_accuracy: 0.7049\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2599 - accuracy: 0.9779 - val_loss: 2.8792 - val_accuracy: 0.6291\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2571 - accuracy: 0.9786 - val_loss: 1.8461 - val_accuracy: 0.7188\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2704 - accuracy: 0.9746 - val_loss: 1.6809 - val_accuracy: 0.7316\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2607 - accuracy: 0.9778 - val_loss: 1.5645 - val_accuracy: 0.7350\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2422 - accuracy: 0.9840 - val_loss: 1.8255 - val_accuracy: 0.7112\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2733 - accuracy: 0.9713 - val_loss: 1.4561 - val_accuracy: 0.7722\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2704 - accuracy: 0.9740 - val_loss: 1.5086 - val_accuracy: 0.7658\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2601 - accuracy: 0.9778 - val_loss: 1.8768 - val_accuracy: 0.7020\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2493 - accuracy: 0.9815 - val_loss: 1.6196 - val_accuracy: 0.7426\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2681 - accuracy: 0.9746 - val_loss: 1.6356 - val_accuracy: 0.7388\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2596 - accuracy: 0.9779 - val_loss: 2.1817 - val_accuracy: 0.7021\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2587 - accuracy: 0.9778 - val_loss: 2.1056 - val_accuracy: 0.6871\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2572 - accuracy: 0.9783 - val_loss: 2.0183 - val_accuracy: 0.6920\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2615 - accuracy: 0.9770 - val_loss: 2.0591 - val_accuracy: 0.7158\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2609 - accuracy: 0.9772 - val_loss: 1.5738 - val_accuracy: 0.7499\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2527 - accuracy: 0.9801 - val_loss: 1.3534 - val_accuracy: 0.7664\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2623 - accuracy: 0.9768 - val_loss: 1.6386 - val_accuracy: 0.7499\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2626 - accuracy: 0.9767 - val_loss: 2.2173 - val_accuracy: 0.6766\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2480 - accuracy: 0.9814 - val_loss: 1.9590 - val_accuracy: 0.7069\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2595 - accuracy: 0.9769 - val_loss: 1.7146 - val_accuracy: 0.7362\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2579 - accuracy: 0.9780 - val_loss: 1.9677 - val_accuracy: 0.7148\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2499 - accuracy: 0.9800 - val_loss: 1.5916 - val_accuracy: 0.7561\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2604 - accuracy: 0.9765 - val_loss: 1.9370 - val_accuracy: 0.7230\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2494 - accuracy: 0.9802 - val_loss: 1.6845 - val_accuracy: 0.7603\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2634 - accuracy: 0.9758 - val_loss: 1.7520 - val_accuracy: 0.7264\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2477 - accuracy: 0.9805 - val_loss: 1.5733 - val_accuracy: 0.7546\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2580 - accuracy: 0.9764 - val_loss: 1.6676 - val_accuracy: 0.7471\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2560 - accuracy: 0.9774 - val_loss: 1.5965 - val_accuracy: 0.7331\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2587 - accuracy: 0.9772 - val_loss: 1.6644 - val_accuracy: 0.7268\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2515 - accuracy: 0.9798 - val_loss: 1.8610 - val_accuracy: 0.7271\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2555 - accuracy: 0.9775 - val_loss: 1.6406 - val_accuracy: 0.7498\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2578 - accuracy: 0.9771 - val_loss: 1.7616 - val_accuracy: 0.7240\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2447 - accuracy: 0.9813 - val_loss: 1.8658 - val_accuracy: 0.7337\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2506 - accuracy: 0.9793 - val_loss: 1.8900 - val_accuracy: 0.7145\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2511 - accuracy: 0.9787 - val_loss: 1.4777 - val_accuracy: 0.7500\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2520 - accuracy: 0.9785 - val_loss: 1.3645 - val_accuracy: 0.7710\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2536 - accuracy: 0.9782 - val_loss: 1.7395 - val_accuracy: 0.7489\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2493 - accuracy: 0.9795 - val_loss: 1.5343 - val_accuracy: 0.7581\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2454 - accuracy: 0.9804 - val_loss: 1.9049 - val_accuracy: 0.7101\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2588 - accuracy: 0.9769 - val_loss: 1.9007 - val_accuracy: 0.7118\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2509 - accuracy: 0.9791 - val_loss: 1.7947 - val_accuracy: 0.7191\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2417 - accuracy: 0.9819 - val_loss: 2.0760 - val_accuracy: 0.7042\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2543 - accuracy: 0.9774 - val_loss: 1.4824 - val_accuracy: 0.7663\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2465 - accuracy: 0.9800 - val_loss: 1.7381 - val_accuracy: 0.7349\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2569 - accuracy: 0.9769 - val_loss: 1.9305 - val_accuracy: 0.6950\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2479 - accuracy: 0.9795 - val_loss: 2.0560 - val_accuracy: 0.6887\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2494 - accuracy: 0.9802 - val_loss: 1.5508 - val_accuracy: 0.7538\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2539 - accuracy: 0.9769 - val_loss: 1.8503 - val_accuracy: 0.7295\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2406 - accuracy: 0.9824 - val_loss: 1.4904 - val_accuracy: 0.7562\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2415 - accuracy: 0.9805 - val_loss: 1.4008 - val_accuracy: 0.7635\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2562 - accuracy: 0.9769 - val_loss: 1.9992 - val_accuracy: 0.7227\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2512 - accuracy: 0.9784 - val_loss: 1.6155 - val_accuracy: 0.7331\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2468 - accuracy: 0.9800 - val_loss: 1.4448 - val_accuracy: 0.7625\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2412 - accuracy: 0.9823 - val_loss: 1.5192 - val_accuracy: 0.7595\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2494 - accuracy: 0.9780 - val_loss: 1.9842 - val_accuracy: 0.7236\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2539 - accuracy: 0.9776 - val_loss: 1.5614 - val_accuracy: 0.7472\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2398 - accuracy: 0.9816 - val_loss: 1.7223 - val_accuracy: 0.7389\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2477 - accuracy: 0.9786 - val_loss: 1.8581 - val_accuracy: 0.7081\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2550 - accuracy: 0.9769 - val_loss: 1.5901 - val_accuracy: 0.7417\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2413 - accuracy: 0.9813 - val_loss: 1.4472 - val_accuracy: 0.7665\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2446 - accuracy: 0.9804 - val_loss: 1.8254 - val_accuracy: 0.7265\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2482 - accuracy: 0.9793 - val_loss: 1.5391 - val_accuracy: 0.7499\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2388 - accuracy: 0.9821 - val_loss: 1.6735 - val_accuracy: 0.7331\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2475 - accuracy: 0.9790 - val_loss: 1.9759 - val_accuracy: 0.7182\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2458 - accuracy: 0.9791 - val_loss: 1.4604 - val_accuracy: 0.7693\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2423 - accuracy: 0.9813 - val_loss: 1.6202 - val_accuracy: 0.7475\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2440 - accuracy: 0.9800 - val_loss: 1.5601 - val_accuracy: 0.7526\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2460 - accuracy: 0.9792 - val_loss: 1.5696 - val_accuracy: 0.7547\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2462 - accuracy: 0.9790 - val_loss: 1.4711 - val_accuracy: 0.7663\n",
      "✅ SNIP pruning and training complete.\n"
     ]
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.5, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fdf4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   448         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   544         ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 32)  128         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 32)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 32)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 16, 16, 32)  128         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 8, 8, 64)     0           ['conv2d_79[0][0]',              \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 64)     0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 64)    256         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 8, 8, 64)     0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 64)     0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 64)    256         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['activation_73[0][0]',          \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 64)     0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 64)          0           ['activation_75[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 31s 73ms/step - loss: 1.6084 - accuracy: 0.4729 - val_loss: 1.8658 - val_accuracy: 0.4191\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.1981 - accuracy: 0.6237 - val_loss: 1.3263 - val_accuracy: 0.5915\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.0082 - accuracy: 0.6931 - val_loss: 1.1165 - val_accuracy: 0.6599\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.8700 - accuracy: 0.7461 - val_loss: 1.7993 - val_accuracy: 0.5310\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.7740 - accuracy: 0.7797 - val_loss: 1.3685 - val_accuracy: 0.6118\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6974 - accuracy: 0.8088 - val_loss: 1.3225 - val_accuracy: 0.6125\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6337 - accuracy: 0.8324 - val_loss: 1.2100 - val_accuracy: 0.6672\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5814 - accuracy: 0.8503 - val_loss: 1.0515 - val_accuracy: 0.7071\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5337 - accuracy: 0.8671 - val_loss: 1.4079 - val_accuracy: 0.6472\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4840 - accuracy: 0.8862 - val_loss: 1.2230 - val_accuracy: 0.6967\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4588 - accuracy: 0.8962 - val_loss: 1.1931 - val_accuracy: 0.7107\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4167 - accuracy: 0.9116 - val_loss: 1.1483 - val_accuracy: 0.7280\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3897 - accuracy: 0.9209 - val_loss: 1.8006 - val_accuracy: 0.6365\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3765 - accuracy: 0.9278 - val_loss: 1.5660 - val_accuracy: 0.6607\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3614 - accuracy: 0.9310 - val_loss: 1.9071 - val_accuracy: 0.6189\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3389 - accuracy: 0.9416 - val_loss: 1.4132 - val_accuracy: 0.7033\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3343 - accuracy: 0.9432 - val_loss: 1.2350 - val_accuracy: 0.7396\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3231 - accuracy: 0.9488 - val_loss: 1.7100 - val_accuracy: 0.6779\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3159 - accuracy: 0.9519 - val_loss: 2.0858 - val_accuracy: 0.6337\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3113 - accuracy: 0.9539 - val_loss: 1.4562 - val_accuracy: 0.7191\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3103 - accuracy: 0.9539 - val_loss: 1.6452 - val_accuracy: 0.6826\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3006 - accuracy: 0.9590 - val_loss: 1.4325 - val_accuracy: 0.7176\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3014 - accuracy: 0.9587 - val_loss: 2.2045 - val_accuracy: 0.6257\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2961 - accuracy: 0.9611 - val_loss: 1.4301 - val_accuracy: 0.7286\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2917 - accuracy: 0.9628 - val_loss: 1.8675 - val_accuracy: 0.6831\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2953 - accuracy: 0.9616 - val_loss: 1.8114 - val_accuracy: 0.6895\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2938 - accuracy: 0.9626 - val_loss: 1.4953 - val_accuracy: 0.7181\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2823 - accuracy: 0.9671 - val_loss: 1.8446 - val_accuracy: 0.6780\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2905 - accuracy: 0.9646 - val_loss: 1.8890 - val_accuracy: 0.6882\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2899 - accuracy: 0.9654 - val_loss: 1.9736 - val_accuracy: 0.6859\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2916 - accuracy: 0.9644 - val_loss: 1.5037 - val_accuracy: 0.7271\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2878 - accuracy: 0.9662 - val_loss: 1.4558 - val_accuracy: 0.7302\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2770 - accuracy: 0.9705 - val_loss: 1.8828 - val_accuracy: 0.6945\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2830 - accuracy: 0.9689 - val_loss: 1.6616 - val_accuracy: 0.7034\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2773 - accuracy: 0.9709 - val_loss: 1.7733 - val_accuracy: 0.7068\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2988 - accuracy: 0.9627 - val_loss: 2.9346 - val_accuracy: 0.6186\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2795 - accuracy: 0.9695 - val_loss: 1.9932 - val_accuracy: 0.6862\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2815 - accuracy: 0.9697 - val_loss: 1.8393 - val_accuracy: 0.7058\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2802 - accuracy: 0.9705 - val_loss: 1.6881 - val_accuracy: 0.7182\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2893 - accuracy: 0.9671 - val_loss: 1.8085 - val_accuracy: 0.7108\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2773 - accuracy: 0.9728 - val_loss: 1.6246 - val_accuracy: 0.7129\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2784 - accuracy: 0.9716 - val_loss: 1.5532 - val_accuracy: 0.7380\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2781 - accuracy: 0.9712 - val_loss: 2.1057 - val_accuracy: 0.6695\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2872 - accuracy: 0.9695 - val_loss: 1.4197 - val_accuracy: 0.7510\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2801 - accuracy: 0.9715 - val_loss: 1.5069 - val_accuracy: 0.7459\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2725 - accuracy: 0.9752 - val_loss: 1.7211 - val_accuracy: 0.7172\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2805 - accuracy: 0.9719 - val_loss: 1.7892 - val_accuracy: 0.6899\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2735 - accuracy: 0.9737 - val_loss: 1.6531 - val_accuracy: 0.7291\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2746 - accuracy: 0.9735 - val_loss: 1.7114 - val_accuracy: 0.7155\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2840 - accuracy: 0.9703 - val_loss: 2.1257 - val_accuracy: 0.6963\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2771 - accuracy: 0.9728 - val_loss: 2.7073 - val_accuracy: 0.6389\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2716 - accuracy: 0.9751 - val_loss: 1.7013 - val_accuracy: 0.7334\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2810 - accuracy: 0.9715 - val_loss: 1.8501 - val_accuracy: 0.7162\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2697 - accuracy: 0.9757 - val_loss: 1.6834 - val_accuracy: 0.7283\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2775 - accuracy: 0.9726 - val_loss: 1.9225 - val_accuracy: 0.7070\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2734 - accuracy: 0.9740 - val_loss: 1.8189 - val_accuracy: 0.7099\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2659 - accuracy: 0.9772 - val_loss: 1.3628 - val_accuracy: 0.7658\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2709 - accuracy: 0.9744 - val_loss: 2.0613 - val_accuracy: 0.6693\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2798 - accuracy: 0.9714 - val_loss: 2.1448 - val_accuracy: 0.6671\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2704 - accuracy: 0.9751 - val_loss: 1.9949 - val_accuracy: 0.7017\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2760 - accuracy: 0.9732 - val_loss: 1.5775 - val_accuracy: 0.7447\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2802 - accuracy: 0.9719 - val_loss: 1.3394 - val_accuracy: 0.7574\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2618 - accuracy: 0.9783 - val_loss: 1.7368 - val_accuracy: 0.7391\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2811 - accuracy: 0.9714 - val_loss: 1.7053 - val_accuracy: 0.7240\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2630 - accuracy: 0.9766 - val_loss: 1.5047 - val_accuracy: 0.7442\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2765 - accuracy: 0.9726 - val_loss: 2.1629 - val_accuracy: 0.7090\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2673 - accuracy: 0.9764 - val_loss: 1.5092 - val_accuracy: 0.7403\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2591 - accuracy: 0.9789 - val_loss: 1.4795 - val_accuracy: 0.7561\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2838 - accuracy: 0.9704 - val_loss: 2.0609 - val_accuracy: 0.6874\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2644 - accuracy: 0.9767 - val_loss: 1.6476 - val_accuracy: 0.7218\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2722 - accuracy: 0.9746 - val_loss: 1.4672 - val_accuracy: 0.7577\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2749 - accuracy: 0.9731 - val_loss: 1.5777 - val_accuracy: 0.7477\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2551 - accuracy: 0.9804 - val_loss: 1.5071 - val_accuracy: 0.7417\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2720 - accuracy: 0.9742 - val_loss: 1.8517 - val_accuracy: 0.7188\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2704 - accuracy: 0.9750 - val_loss: 1.8292 - val_accuracy: 0.7216\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2598 - accuracy: 0.9783 - val_loss: 1.6584 - val_accuracy: 0.7512\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2629 - accuracy: 0.9774 - val_loss: 1.5369 - val_accuracy: 0.7520\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2735 - accuracy: 0.9735 - val_loss: 1.9077 - val_accuracy: 0.7094\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2703 - accuracy: 0.9750 - val_loss: 1.4782 - val_accuracy: 0.7542\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2622 - accuracy: 0.9778 - val_loss: 1.4527 - val_accuracy: 0.7687\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2574 - accuracy: 0.9791 - val_loss: 1.3604 - val_accuracy: 0.7583\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2689 - accuracy: 0.9744 - val_loss: 1.6352 - val_accuracy: 0.7281\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2704 - accuracy: 0.9735 - val_loss: 1.4517 - val_accuracy: 0.7558\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2585 - accuracy: 0.9787 - val_loss: 1.5379 - val_accuracy: 0.7551\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2582 - accuracy: 0.9784 - val_loss: 1.5990 - val_accuracy: 0.7486\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2645 - accuracy: 0.9760 - val_loss: 1.9307 - val_accuracy: 0.6906\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2705 - accuracy: 0.9740 - val_loss: 2.0805 - val_accuracy: 0.7112\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2533 - accuracy: 0.9810 - val_loss: 1.6177 - val_accuracy: 0.7527\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2571 - accuracy: 0.9783 - val_loss: 1.5727 - val_accuracy: 0.7502\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2618 - accuracy: 0.9767 - val_loss: 1.6526 - val_accuracy: 0.7443\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2671 - accuracy: 0.9751 - val_loss: 1.9198 - val_accuracy: 0.6992\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2615 - accuracy: 0.9768 - val_loss: 1.6352 - val_accuracy: 0.7454\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2614 - accuracy: 0.9777 - val_loss: 1.8207 - val_accuracy: 0.7193\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2573 - accuracy: 0.9783 - val_loss: 1.5898 - val_accuracy: 0.7539\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2529 - accuracy: 0.9796 - val_loss: 2.0995 - val_accuracy: 0.7126\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2508 - accuracy: 0.9809 - val_loss: 1.5528 - val_accuracy: 0.7494\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2680 - accuracy: 0.9740 - val_loss: 1.9920 - val_accuracy: 0.6949\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2576 - accuracy: 0.9790 - val_loss: 1.4804 - val_accuracy: 0.7554\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2525 - accuracy: 0.9801 - val_loss: 1.7655 - val_accuracy: 0.7323\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2550 - accuracy: 0.9786 - val_loss: 1.7538 - val_accuracy: 0.7194\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2565 - accuracy: 0.9787 - val_loss: 2.4632 - val_accuracy: 0.6390\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2513 - accuracy: 0.9792 - val_loss: 1.6230 - val_accuracy: 0.7344\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2673 - accuracy: 0.9737 - val_loss: 2.6364 - val_accuracy: 0.6675\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2573 - accuracy: 0.9781 - val_loss: 1.3387 - val_accuracy: 0.7749\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2408 - accuracy: 0.9829 - val_loss: 1.9294 - val_accuracy: 0.7163\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2630 - accuracy: 0.9759 - val_loss: 1.3240 - val_accuracy: 0.7814\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2541 - accuracy: 0.9781 - val_loss: 1.4958 - val_accuracy: 0.7636\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2485 - accuracy: 0.9811 - val_loss: 1.7020 - val_accuracy: 0.7197\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2541 - accuracy: 0.9786 - val_loss: 1.4084 - val_accuracy: 0.7783\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2531 - accuracy: 0.9790 - val_loss: 1.6438 - val_accuracy: 0.7447\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2561 - accuracy: 0.9776 - val_loss: 1.3843 - val_accuracy: 0.7518\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2483 - accuracy: 0.9805 - val_loss: 1.9275 - val_accuracy: 0.7198\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2524 - accuracy: 0.9790 - val_loss: 1.4698 - val_accuracy: 0.7570\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2567 - accuracy: 0.9773 - val_loss: 1.5965 - val_accuracy: 0.7337\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2518 - accuracy: 0.9787 - val_loss: 1.6749 - val_accuracy: 0.7561\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2510 - accuracy: 0.9787 - val_loss: 1.5364 - val_accuracy: 0.7524\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2431 - accuracy: 0.9817 - val_loss: 2.2146 - val_accuracy: 0.6781\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2546 - accuracy: 0.9776 - val_loss: 2.1297 - val_accuracy: 0.6841\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2517 - accuracy: 0.9793 - val_loss: 1.6534 - val_accuracy: 0.7320\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2518 - accuracy: 0.9784 - val_loss: 1.5143 - val_accuracy: 0.7469\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2449 - accuracy: 0.9809 - val_loss: 1.6140 - val_accuracy: 0.7471\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2491 - accuracy: 0.9797 - val_loss: 1.4129 - val_accuracy: 0.7658\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2584 - accuracy: 0.9760 - val_loss: 1.7187 - val_accuracy: 0.7298\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2489 - accuracy: 0.9792 - val_loss: 1.5472 - val_accuracy: 0.7596\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2450 - accuracy: 0.9813 - val_loss: 1.5483 - val_accuracy: 0.7679\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2506 - accuracy: 0.9789 - val_loss: 1.6971 - val_accuracy: 0.7307\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2506 - accuracy: 0.9789 - val_loss: 1.8071 - val_accuracy: 0.7316\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2505 - accuracy: 0.9791 - val_loss: 1.8071 - val_accuracy: 0.7355\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2494 - accuracy: 0.9794 - val_loss: 1.5507 - val_accuracy: 0.7484\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2487 - accuracy: 0.9794 - val_loss: 1.6964 - val_accuracy: 0.7354\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2389 - accuracy: 0.9830 - val_loss: 2.2517 - val_accuracy: 0.6975\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2529 - accuracy: 0.9781 - val_loss: 1.4238 - val_accuracy: 0.7664\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2447 - accuracy: 0.9811 - val_loss: 1.8780 - val_accuracy: 0.7210\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2484 - accuracy: 0.9793 - val_loss: 2.0633 - val_accuracy: 0.7123\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9780 - val_loss: 1.8537 - val_accuracy: 0.7347\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2395 - accuracy: 0.9819 - val_loss: 1.4236 - val_accuracy: 0.7778\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2540 - accuracy: 0.9775 - val_loss: 1.4578 - val_accuracy: 0.7685\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2438 - accuracy: 0.9806 - val_loss: 1.6670 - val_accuracy: 0.7501\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2388 - accuracy: 0.9822 - val_loss: 1.6404 - val_accuracy: 0.7435\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2442 - accuracy: 0.9807 - val_loss: 1.4736 - val_accuracy: 0.7602\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2499 - accuracy: 0.9778 - val_loss: 1.6633 - val_accuracy: 0.7542\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2332 - accuracy: 0.9844 - val_loss: 1.7010 - val_accuracy: 0.7367\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2517 - accuracy: 0.9767 - val_loss: 1.5203 - val_accuracy: 0.7627\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2491 - accuracy: 0.9776 - val_loss: 1.8197 - val_accuracy: 0.7139\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2415 - accuracy: 0.9816 - val_loss: 1.7275 - val_accuracy: 0.7560\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2367 - accuracy: 0.9820 - val_loss: 1.7950 - val_accuracy: 0.7335\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2423 - accuracy: 0.9802 - val_loss: 1.4762 - val_accuracy: 0.7552\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2475 - accuracy: 0.9785 - val_loss: 1.6145 - val_accuracy: 0.7494\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2464 - accuracy: 0.9785 - val_loss: 1.5350 - val_accuracy: 0.7639\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2382 - accuracy: 0.9819 - val_loss: 1.8325 - val_accuracy: 0.7287\n",
      "✅ SNIP pruning and training complete.\n"
     ]
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.7, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918404b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 32, 32, 16)   448         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32, 32, 16)  64          ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 32, 32, 16)  64          ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 32, 32, 16)  64          ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 32, 32, 16)   0           ['activation_76[0][0]',          \n",
      "                                                                  'batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 32, 32, 16)   0           ['add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 32, 32, 16)  64          ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 32, 32, 16)  64          ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 32, 32, 16)   0           ['activation_78[0][0]',          \n",
      "                                                                  'batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 32, 32, 16)   0           ['add_37[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 32, 32, 16)  64          ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 32, 32, 16)  64          ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 32, 32, 16)   0           ['activation_80[0][0]',          \n",
      "                                                                  'batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 32, 32, 16)   0           ['add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 16, 16, 32)  128         ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 16, 16, 32)   544         ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 16, 16, 32)  128         ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 16, 16, 32)   0           ['conv2d_93[0][0]',              \n",
      "                                                                  'batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 16, 16, 32)   0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 16, 16, 32)  128         ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_85[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16, 16, 32)  128         ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 16, 16, 32)   0           ['activation_84[0][0]',          \n",
      "                                                                  'batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 16, 16, 32)   0           ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16, 16, 32)  128         ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_87[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 16, 16, 32)  128         ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 16, 16, 32)   0           ['activation_86[0][0]',          \n",
      "                                                                  'batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 16, 16, 32)   0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 8, 8, 64)    256         ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 8, 8, 64)     2112        ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 8, 8, 64)    256         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 8, 8, 64)     0           ['conv2d_100[0][0]',             \n",
      "                                                                  'batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 8, 8, 64)     0           ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 8, 8, 64)    256         ['conv2d_101[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 8, 8, 64)    256         ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 8, 8, 64)     0           ['activation_90[0][0]',          \n",
      "                                                                  'batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 8, 8, 64)     0           ['add_43[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 8, 8, 64)    256         ['conv2d_103[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 8, 8, 64)    256         ['conv2d_104[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 8, 8, 64)     0           ['activation_92[0][0]',          \n",
      "                                                                  'batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 8, 8, 64)     0           ['add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 64)          0           ['activation_94[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 10)           650         ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 31s 73ms/step - loss: 1.5605 - accuracy: 0.4905 - val_loss: 1.6107 - val_accuracy: 0.4906\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.1700 - accuracy: 0.6353 - val_loss: 1.4470 - val_accuracy: 0.5373\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.9898 - accuracy: 0.7005 - val_loss: 1.3086 - val_accuracy: 0.6209\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.8632 - accuracy: 0.7467 - val_loss: 1.0832 - val_accuracy: 0.6860\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.7682 - accuracy: 0.7845 - val_loss: 1.0809 - val_accuracy: 0.6895\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6923 - accuracy: 0.8086 - val_loss: 1.3204 - val_accuracy: 0.6302\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6310 - accuracy: 0.8316 - val_loss: 0.9602 - val_accuracy: 0.7359\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.5796 - accuracy: 0.8499 - val_loss: 1.2663 - val_accuracy: 0.6597\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5319 - accuracy: 0.8677 - val_loss: 1.3020 - val_accuracy: 0.6575\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4954 - accuracy: 0.8838 - val_loss: 1.5202 - val_accuracy: 0.6433\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4581 - accuracy: 0.8951 - val_loss: 1.2256 - val_accuracy: 0.7101\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4267 - accuracy: 0.9066 - val_loss: 1.5841 - val_accuracy: 0.6338\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3984 - accuracy: 0.9166 - val_loss: 1.6245 - val_accuracy: 0.6514\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3794 - accuracy: 0.9260 - val_loss: 1.5267 - val_accuracy: 0.6809\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3638 - accuracy: 0.9319 - val_loss: 1.5907 - val_accuracy: 0.6709\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3481 - accuracy: 0.9377 - val_loss: 1.3743 - val_accuracy: 0.7101\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3334 - accuracy: 0.9445 - val_loss: 1.7950 - val_accuracy: 0.6410\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3206 - accuracy: 0.9489 - val_loss: 1.6723 - val_accuracy: 0.6570\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3265 - accuracy: 0.9471 - val_loss: 1.3026 - val_accuracy: 0.7320\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3099 - accuracy: 0.9544 - val_loss: 1.4538 - val_accuracy: 0.7091\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2989 - accuracy: 0.9588 - val_loss: 1.5228 - val_accuracy: 0.7058\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3093 - accuracy: 0.9543 - val_loss: 1.5285 - val_accuracy: 0.7111\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2978 - accuracy: 0.9602 - val_loss: 1.5018 - val_accuracy: 0.7078\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2983 - accuracy: 0.9601 - val_loss: 2.0794 - val_accuracy: 0.6351\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2967 - accuracy: 0.9606 - val_loss: 2.0301 - val_accuracy: 0.6608\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2938 - accuracy: 0.9622 - val_loss: 1.9671 - val_accuracy: 0.6936\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2927 - accuracy: 0.9631 - val_loss: 2.1076 - val_accuracy: 0.6625\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2941 - accuracy: 0.9618 - val_loss: 1.4987 - val_accuracy: 0.7217\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2882 - accuracy: 0.9652 - val_loss: 1.9209 - val_accuracy: 0.6875\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2811 - accuracy: 0.9683 - val_loss: 1.4573 - val_accuracy: 0.7311\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2886 - accuracy: 0.9664 - val_loss: 1.5677 - val_accuracy: 0.7243\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2829 - accuracy: 0.9671 - val_loss: 1.5606 - val_accuracy: 0.7182\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2888 - accuracy: 0.9654 - val_loss: 1.7566 - val_accuracy: 0.7035\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2862 - accuracy: 0.9668 - val_loss: 1.4800 - val_accuracy: 0.7313\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2797 - accuracy: 0.9701 - val_loss: 1.5671 - val_accuracy: 0.7164\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2848 - accuracy: 0.9675 - val_loss: 2.5928 - val_accuracy: 0.6199\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2828 - accuracy: 0.9684 - val_loss: 1.8824 - val_accuracy: 0.6854\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2846 - accuracy: 0.9683 - val_loss: 1.6957 - val_accuracy: 0.7165\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2832 - accuracy: 0.9690 - val_loss: 1.9976 - val_accuracy: 0.6737\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2841 - accuracy: 0.9688 - val_loss: 1.6448 - val_accuracy: 0.7072\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2862 - accuracy: 0.9684 - val_loss: 1.4022 - val_accuracy: 0.7601\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2700 - accuracy: 0.9743 - val_loss: 2.0471 - val_accuracy: 0.6579\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2808 - accuracy: 0.9705 - val_loss: 1.3840 - val_accuracy: 0.7534\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2850 - accuracy: 0.9683 - val_loss: 1.7038 - val_accuracy: 0.7168\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2774 - accuracy: 0.9718 - val_loss: 1.6923 - val_accuracy: 0.7152\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2817 - accuracy: 0.9710 - val_loss: 2.1566 - val_accuracy: 0.6684\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2790 - accuracy: 0.9714 - val_loss: 1.9534 - val_accuracy: 0.6960\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2697 - accuracy: 0.9748 - val_loss: 1.4854 - val_accuracy: 0.7380\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2723 - accuracy: 0.9744 - val_loss: 1.6963 - val_accuracy: 0.7190\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2776 - accuracy: 0.9719 - val_loss: 1.8840 - val_accuracy: 0.7059\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2824 - accuracy: 0.9707 - val_loss: 1.5577 - val_accuracy: 0.7374\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2670 - accuracy: 0.9761 - val_loss: 1.5760 - val_accuracy: 0.7453\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2730 - accuracy: 0.9733 - val_loss: 2.0600 - val_accuracy: 0.6786\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2814 - accuracy: 0.9715 - val_loss: 2.2000 - val_accuracy: 0.6868\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2725 - accuracy: 0.9733 - val_loss: 1.4999 - val_accuracy: 0.7385\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2712 - accuracy: 0.9741 - val_loss: 1.5874 - val_accuracy: 0.7408\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2867 - accuracy: 0.9697 - val_loss: 1.8574 - val_accuracy: 0.7028\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2684 - accuracy: 0.9760 - val_loss: 1.7600 - val_accuracy: 0.7159\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2683 - accuracy: 0.9759 - val_loss: 1.4532 - val_accuracy: 0.7547\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2783 - accuracy: 0.9726 - val_loss: 2.0234 - val_accuracy: 0.7036\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2777 - accuracy: 0.9723 - val_loss: 2.0813 - val_accuracy: 0.6996\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2678 - accuracy: 0.9759 - val_loss: 1.6890 - val_accuracy: 0.7352\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2707 - accuracy: 0.9752 - val_loss: 1.6580 - val_accuracy: 0.7392\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2710 - accuracy: 0.9746 - val_loss: 1.4763 - val_accuracy: 0.7517\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2657 - accuracy: 0.9765 - val_loss: 1.5729 - val_accuracy: 0.7452\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2715 - accuracy: 0.9748 - val_loss: 2.9643 - val_accuracy: 0.6138\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2608 - accuracy: 0.9778 - val_loss: 2.1200 - val_accuracy: 0.6811\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2670 - accuracy: 0.9756 - val_loss: 1.7909 - val_accuracy: 0.7261\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2706 - accuracy: 0.9743 - val_loss: 2.0193 - val_accuracy: 0.6768\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2745 - accuracy: 0.9729 - val_loss: 1.5810 - val_accuracy: 0.7459\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2664 - accuracy: 0.9768 - val_loss: 1.4261 - val_accuracy: 0.7588\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2579 - accuracy: 0.9789 - val_loss: 2.0743 - val_accuracy: 0.6903\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2767 - accuracy: 0.9722 - val_loss: 1.3806 - val_accuracy: 0.7597\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2651 - accuracy: 0.9767 - val_loss: 2.0969 - val_accuracy: 0.6915\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2603 - accuracy: 0.9779 - val_loss: 2.1286 - val_accuracy: 0.7132\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2692 - accuracy: 0.9749 - val_loss: 1.8402 - val_accuracy: 0.7168\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2669 - accuracy: 0.9759 - val_loss: 3.0918 - val_accuracy: 0.5921\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2570 - accuracy: 0.9796 - val_loss: 1.7319 - val_accuracy: 0.7242\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2625 - accuracy: 0.9769 - val_loss: 1.5951 - val_accuracy: 0.7525\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2761 - accuracy: 0.9725 - val_loss: 1.7250 - val_accuracy: 0.7267\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2655 - accuracy: 0.9764 - val_loss: 1.6205 - val_accuracy: 0.7238\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2573 - accuracy: 0.9793 - val_loss: 1.7262 - val_accuracy: 0.7334\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2636 - accuracy: 0.9769 - val_loss: 1.7840 - val_accuracy: 0.7145\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2685 - accuracy: 0.9752 - val_loss: 1.4281 - val_accuracy: 0.7630\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2565 - accuracy: 0.9792 - val_loss: 1.7890 - val_accuracy: 0.7248\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2648 - accuracy: 0.9753 - val_loss: 1.7192 - val_accuracy: 0.7270\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2676 - accuracy: 0.9748 - val_loss: 1.6785 - val_accuracy: 0.7420\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2653 - accuracy: 0.9765 - val_loss: 1.5633 - val_accuracy: 0.7417\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2534 - accuracy: 0.9804 - val_loss: 1.3628 - val_accuracy: 0.7768\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2607 - accuracy: 0.9773 - val_loss: 2.1167 - val_accuracy: 0.6857\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2612 - accuracy: 0.9768 - val_loss: 1.5170 - val_accuracy: 0.7696\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2654 - accuracy: 0.9752 - val_loss: 2.3349 - val_accuracy: 0.6837\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2650 - accuracy: 0.9758 - val_loss: 1.7998 - val_accuracy: 0.7062\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2587 - accuracy: 0.9768 - val_loss: 1.7379 - val_accuracy: 0.7407\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2520 - accuracy: 0.9803 - val_loss: 1.4852 - val_accuracy: 0.7431\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2588 - accuracy: 0.9775 - val_loss: 1.3910 - val_accuracy: 0.7595\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2653 - accuracy: 0.9756 - val_loss: 1.8885 - val_accuracy: 0.7049\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2574 - accuracy: 0.9781 - val_loss: 2.0595 - val_accuracy: 0.6930\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2687 - accuracy: 0.9739 - val_loss: 1.6590 - val_accuracy: 0.7290\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2461 - accuracy: 0.9824 - val_loss: 1.3504 - val_accuracy: 0.7791\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2541 - accuracy: 0.9786 - val_loss: 1.5643 - val_accuracy: 0.7466\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2667 - accuracy: 0.9748 - val_loss: 1.7433 - val_accuracy: 0.7304\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2562 - accuracy: 0.9782 - val_loss: 1.5336 - val_accuracy: 0.7368\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2547 - accuracy: 0.9784 - val_loss: 2.2017 - val_accuracy: 0.6913\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2501 - accuracy: 0.9806 - val_loss: 1.4773 - val_accuracy: 0.7564\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2611 - accuracy: 0.9760 - val_loss: 1.7268 - val_accuracy: 0.7213\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2545 - accuracy: 0.9794 - val_loss: 1.3424 - val_accuracy: 0.7576\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2498 - accuracy: 0.9799 - val_loss: 2.2036 - val_accuracy: 0.6898\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2614 - accuracy: 0.9766 - val_loss: 1.8630 - val_accuracy: 0.6914\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2539 - accuracy: 0.9793 - val_loss: 1.5640 - val_accuracy: 0.7579\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2521 - accuracy: 0.9791 - val_loss: 1.6515 - val_accuracy: 0.7503\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2544 - accuracy: 0.9784 - val_loss: 1.4127 - val_accuracy: 0.7655\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2610 - accuracy: 0.9763 - val_loss: 1.5711 - val_accuracy: 0.7496\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2554 - accuracy: 0.9781 - val_loss: 1.7639 - val_accuracy: 0.7239\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2514 - accuracy: 0.9799 - val_loss: 2.1443 - val_accuracy: 0.6764\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2516 - accuracy: 0.9793 - val_loss: 1.6245 - val_accuracy: 0.7346\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2567 - accuracy: 0.9776 - val_loss: 1.6428 - val_accuracy: 0.7367\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2482 - accuracy: 0.9801 - val_loss: 1.6432 - val_accuracy: 0.7440\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2535 - accuracy: 0.9779 - val_loss: 1.6068 - val_accuracy: 0.7516\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2544 - accuracy: 0.9780 - val_loss: 2.2054 - val_accuracy: 0.6818\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2455 - accuracy: 0.9806 - val_loss: 1.7298 - val_accuracy: 0.7262\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2482 - accuracy: 0.9802 - val_loss: 1.8642 - val_accuracy: 0.7043\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2604 - accuracy: 0.9756 - val_loss: 1.4179 - val_accuracy: 0.7629\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2445 - accuracy: 0.9818 - val_loss: 1.5037 - val_accuracy: 0.7507\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2490 - accuracy: 0.9795 - val_loss: 1.7778 - val_accuracy: 0.7307\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2512 - accuracy: 0.9788 - val_loss: 1.8424 - val_accuracy: 0.7311\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2551 - accuracy: 0.9771 - val_loss: 1.5738 - val_accuracy: 0.7436\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2492 - accuracy: 0.9798 - val_loss: 1.4730 - val_accuracy: 0.7602\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2451 - accuracy: 0.9813 - val_loss: 1.7487 - val_accuracy: 0.7189\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2555 - accuracy: 0.9773 - val_loss: 1.8120 - val_accuracy: 0.7191\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2526 - accuracy: 0.9787 - val_loss: 1.8103 - val_accuracy: 0.7174\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2415 - accuracy: 0.9820 - val_loss: 1.4230 - val_accuracy: 0.7601\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2417 - accuracy: 0.9816 - val_loss: 1.9122 - val_accuracy: 0.7356\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2541 - accuracy: 0.9770 - val_loss: 1.9208 - val_accuracy: 0.7209\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2533 - accuracy: 0.9775 - val_loss: 1.4195 - val_accuracy: 0.7609\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2513 - accuracy: 0.9782 - val_loss: 1.9192 - val_accuracy: 0.7269\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2373 - accuracy: 0.9827 - val_loss: 1.5998 - val_accuracy: 0.7546\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2418 - accuracy: 0.9811 - val_loss: 1.8167 - val_accuracy: 0.7327\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2448 - accuracy: 0.9801 - val_loss: 1.5185 - val_accuracy: 0.7559\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2602 - accuracy: 0.9749 - val_loss: 1.4637 - val_accuracy: 0.7520\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2352 - accuracy: 0.9842 - val_loss: 2.1549 - val_accuracy: 0.6913\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2511 - accuracy: 0.9774 - val_loss: 1.4632 - val_accuracy: 0.7652\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2410 - accuracy: 0.9812 - val_loss: 1.7818 - val_accuracy: 0.7372\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2457 - accuracy: 0.9796 - val_loss: 2.0884 - val_accuracy: 0.6890\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2467 - accuracy: 0.9798 - val_loss: 1.5073 - val_accuracy: 0.7587\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2452 - accuracy: 0.9795 - val_loss: 1.6484 - val_accuracy: 0.7573\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2457 - accuracy: 0.9788 - val_loss: 1.4922 - val_accuracy: 0.7425\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2425 - accuracy: 0.9809 - val_loss: 1.7626 - val_accuracy: 0.7389\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2451 - accuracy: 0.9799 - val_loss: 1.9078 - val_accuracy: 0.7197\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2432 - accuracy: 0.9803 - val_loss: 1.6368 - val_accuracy: 0.7448\n",
      "✅ SNIP pruning and training complete.\n"
     ]
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.9, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e6e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
