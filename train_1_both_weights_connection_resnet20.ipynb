{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c91c35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet20\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resnet20\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from models.resnet20 import resnet20\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "# --------------------------\n",
    "# Global Magnitude Pruning\n",
    "# --------------------------\n",
    "def global_magnitude_pruning(model, sparsity):\n",
    "    print(f\"[INFO] Applying global magnitude pruning with sparsity: {sparsity}\")\n",
    "    all_weights = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            weights = layer.get_weights()\n",
    "            if weights:\n",
    "                all_weights.append(weights[0].flatten())\n",
    "    \n",
    "    all_weights = np.concatenate(all_weights)\n",
    "    threshold = np.percentile(np.abs(all_weights), sparsity * 100)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            weights = layer.get_weights()\n",
    "            if weights:\n",
    "                w, b = weights\n",
    "                mask = np.abs(w) >= threshold\n",
    "                layer.set_weights([w * mask, b])\n",
    "\n",
    "# --------------------------\n",
    "# Load CIFAR-10 Dataset\n",
    "# --------------------------\n",
    "def load_cifar10():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# --------------------------\n",
    "# Training Function\n",
    "# --------------------------\n",
    "def train(model, x_train, y_train, x_test, y_test, epochs, batch_size, checkpoint_path):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           save_best_only=True,\n",
    "                                           verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    return history\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop for LWC (Han et al.)\n",
    "# --------------------------\n",
    "def run_lwc_training(sparsity=0.5, epochs=100, batch_size=128, iterations=5):\n",
    "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "    model = resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    initial_weights = model.get_weights()\n",
    "    per_iter_sparsity = sparsity / iterations\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"\\nüîÅ Iteration {i+1}/{iterations} | Sparsity this step: {per_iter_sparsity:.2f}\")\n",
    "\n",
    "        model.set_weights(initial_weights)\n",
    "        global_magnitude_pruning(model, per_iter_sparsity * (i + 1))\n",
    "\n",
    "        checkpoint_path = f\"results/lwc_resnet20_sparsity_{sparsity}_iter_{i+1}.h5\"\n",
    "        train(model, x_train, y_train, x_test, y_test, epochs, batch_size, checkpoint_path)\n",
    "\n",
    "    print(\"‚úÖ Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lwc_training(sparsity=0.6, epochs=150, batch_size=128, iterations=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
