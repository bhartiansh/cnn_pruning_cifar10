{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0080a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from models.resnet20 import build_resnet20  # Ensure this is in your Python path\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------\n",
    "# Dual Gradient-Based Iterative Pruning\n",
    "# -------------------------\n",
    "def dual_gradient_iterative_pruning(model, x_train, y_train, sparsity, iterations=10):\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_train[:256], training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y_train[:256], logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "        scores = [tf.abs(g * w) for g, w in zip(grads, model.trainable_weights) if g is not None]\n",
    "        all_scores = tf.concat([tf.reshape(score, [-1]) for score in scores], axis=0)\n",
    "        k = int((1 - sparsity) * tf.size(all_scores).numpy())\n",
    "        threshold = tf.sort(all_scores)[k].numpy()\n",
    "\n",
    "        masks = [(tf.abs(g * w) > threshold) if g is not None else tf.ones_like(w) for g, w in zip(grads, model.trainable_weights)]\n",
    "\n",
    "        # Apply masks to weights\n",
    "        for var, mask in zip(model.trainable_weights, masks):\n",
    "            var.assign(var * tf.cast(mask, tf.float32))\n",
    "\n",
    "    print(f\"✅ Applied Dual Gradient Iterative Pruning at sparsity {sparsity}\")\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Training Function\n",
    "# -------------------------\n",
    "def train_model(sparsity=0.5, batch_size=128, epochs=120):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "    model = build_resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    # Prune the model\n",
    "    model = dual_gradient_iterative_pruning(model, x_train, y_train, sparsity)\n",
    "\n",
    "    # Compile and train\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    # Save model\n",
    "    model.save(f'models/pruned_resnet20_dual_iterative_sparsity_{sparsity}.h5')\n",
    "    print(\"✅ Model training complete and saved.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9078158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_18[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "✅ Applied Dual Gradient Iterative Pruning at sparsity 0.3\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 16s 35ms/step - loss: 1.5724 - accuracy: 0.4774 - val_loss: 1.5605 - val_accuracy: 0.4902\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 1.1128 - accuracy: 0.6384 - val_loss: 1.1944 - val_accuracy: 0.6044\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.9508 - accuracy: 0.7001 - val_loss: 1.2472 - val_accuracy: 0.6188\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.8357 - accuracy: 0.7457 - val_loss: 1.3187 - val_accuracy: 0.6041\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 31s 81ms/step - loss: 0.7544 - accuracy: 0.7772 - val_loss: 0.9505 - val_accuracy: 0.7077\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 89s 229ms/step - loss: 0.6852 - accuracy: 0.8017 - val_loss: 1.0031 - val_accuracy: 0.6992\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.6223 - accuracy: 0.8253 - val_loss: 1.2963 - val_accuracy: 0.6547\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5740 - accuracy: 0.8447 - val_loss: 1.4031 - val_accuracy: 0.6204\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.5275 - accuracy: 0.8616 - val_loss: 1.0888 - val_accuracy: 0.7142\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.4952 - accuracy: 0.8730 - val_loss: 1.1358 - val_accuracy: 0.6986\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.4522 - accuracy: 0.8906 - val_loss: 1.1367 - val_accuracy: 0.7063\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.4219 - accuracy: 0.9011 - val_loss: 1.3278 - val_accuracy: 0.6930\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.3955 - accuracy: 0.9123 - val_loss: 1.3150 - val_accuracy: 0.7090\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 0.3732 - accuracy: 0.9206 - val_loss: 1.2879 - val_accuracy: 0.7172\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.3554 - accuracy: 0.9282 - val_loss: 1.6651 - val_accuracy: 0.6464\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.3310 - accuracy: 0.9385 - val_loss: 1.1163 - val_accuracy: 0.7426\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.3287 - accuracy: 0.9394 - val_loss: 1.2776 - val_accuracy: 0.7154\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.3109 - accuracy: 0.9465 - val_loss: 1.2229 - val_accuracy: 0.7333\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.3107 - accuracy: 0.9465 - val_loss: 1.4391 - val_accuracy: 0.7092\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.3057 - accuracy: 0.9489 - val_loss: 2.1076 - val_accuracy: 0.6441\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2957 - accuracy: 0.9542 - val_loss: 1.2219 - val_accuracy: 0.7456\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.2898 - accuracy: 0.9578 - val_loss: 1.3595 - val_accuracy: 0.7214\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.2990 - accuracy: 0.9546 - val_loss: 1.8328 - val_accuracy: 0.6752\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.2902 - accuracy: 0.9588 - val_loss: 1.8755 - val_accuracy: 0.6648\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2788 - accuracy: 0.9621 - val_loss: 1.3869 - val_accuracy: 0.7305\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2784 - accuracy: 0.9622 - val_loss: 1.4726 - val_accuracy: 0.7264\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.2903 - accuracy: 0.9591 - val_loss: 1.5552 - val_accuracy: 0.7110\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.2844 - accuracy: 0.9622 - val_loss: 1.9192 - val_accuracy: 0.6784\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 0.2693 - accuracy: 0.9679 - val_loss: 2.2171 - val_accuracy: 0.6511\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.2765 - accuracy: 0.9653 - val_loss: 1.7532 - val_accuracy: 0.6846\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2809 - accuracy: 0.9637 - val_loss: 1.4475 - val_accuracy: 0.7340\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 0.2718 - accuracy: 0.9676 - val_loss: 1.9495 - val_accuracy: 0.6567\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.2757 - accuracy: 0.9663 - val_loss: 2.0529 - val_accuracy: 0.6770\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.2736 - accuracy: 0.9672 - val_loss: 2.1724 - val_accuracy: 0.6509\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2738 - accuracy: 0.9682 - val_loss: 1.8437 - val_accuracy: 0.6822\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.2755 - accuracy: 0.9676 - val_loss: 1.6781 - val_accuracy: 0.7069\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 0.2773 - accuracy: 0.9664 - val_loss: 1.4960 - val_accuracy: 0.7360\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2670 - accuracy: 0.9712 - val_loss: 1.6331 - val_accuracy: 0.7248\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 0.2759 - accuracy: 0.9675 - val_loss: 1.7131 - val_accuracy: 0.7175\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2703 - accuracy: 0.9701 - val_loss: 2.0285 - val_accuracy: 0.6910\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 36s 93ms/step - loss: 0.2696 - accuracy: 0.9710 - val_loss: 1.7071 - val_accuracy: 0.7243\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.2730 - accuracy: 0.9695 - val_loss: 1.8503 - val_accuracy: 0.6930\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 37s 94ms/step - loss: 0.2730 - accuracy: 0.9698 - val_loss: 1.8460 - val_accuracy: 0.7072\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.2633 - accuracy: 0.9725 - val_loss: 2.0220 - val_accuracy: 0.6752\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2676 - accuracy: 0.9721 - val_loss: 1.7132 - val_accuracy: 0.7186\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2703 - accuracy: 0.9711 - val_loss: 1.4983 - val_accuracy: 0.7371\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 0.2697 - accuracy: 0.9712 - val_loss: 1.4578 - val_accuracy: 0.7510\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2693 - accuracy: 0.9716 - val_loss: 1.5627 - val_accuracy: 0.7289\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2694 - accuracy: 0.9713 - val_loss: 1.5773 - val_accuracy: 0.7445\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2647 - accuracy: 0.9737 - val_loss: 1.6942 - val_accuracy: 0.7395\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2678 - accuracy: 0.9732 - val_loss: 2.0011 - val_accuracy: 0.6843\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2651 - accuracy: 0.9738 - val_loss: 1.4596 - val_accuracy: 0.7462\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 0.2671 - accuracy: 0.9727 - val_loss: 1.3899 - val_accuracy: 0.7497\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 0.2672 - accuracy: 0.9717 - val_loss: 1.4860 - val_accuracy: 0.7441\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2728 - accuracy: 0.9716 - val_loss: 1.8776 - val_accuracy: 0.7182\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.2641 - accuracy: 0.9736 - val_loss: 1.5862 - val_accuracy: 0.7308\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 42s 109ms/step - loss: 0.2640 - accuracy: 0.9739 - val_loss: 1.5752 - val_accuracy: 0.7306\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2623 - accuracy: 0.9741 - val_loss: 1.9203 - val_accuracy: 0.6897\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2697 - accuracy: 0.9727 - val_loss: 1.6402 - val_accuracy: 0.7246\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 39s 100ms/step - loss: 0.2617 - accuracy: 0.9756 - val_loss: 1.5060 - val_accuracy: 0.7449\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 39s 101ms/step - loss: 0.2642 - accuracy: 0.9741 - val_loss: 1.8420 - val_accuracy: 0.7065\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2695 - accuracy: 0.9718 - val_loss: 1.9137 - val_accuracy: 0.7139\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 0.2696 - accuracy: 0.9729 - val_loss: 1.7996 - val_accuracy: 0.7231\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2608 - accuracy: 0.9762 - val_loss: 1.4123 - val_accuracy: 0.7565\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.2591 - accuracy: 0.9766 - val_loss: 1.6581 - val_accuracy: 0.7325\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2616 - accuracy: 0.9757 - val_loss: 1.4769 - val_accuracy: 0.7437\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2640 - accuracy: 0.9740 - val_loss: 1.3993 - val_accuracy: 0.7669\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.2577 - accuracy: 0.9766 - val_loss: 1.4577 - val_accuracy: 0.7484\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2651 - accuracy: 0.9743 - val_loss: 1.5397 - val_accuracy: 0.7378\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.2476 - accuracy: 0.9801 - val_loss: 1.7741 - val_accuracy: 0.7185\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2601 - accuracy: 0.9752 - val_loss: 1.9741 - val_accuracy: 0.6973\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.2659 - accuracy: 0.9735 - val_loss: 1.8199 - val_accuracy: 0.7103\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2601 - accuracy: 0.9758 - val_loss: 1.6396 - val_accuracy: 0.7331\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2544 - accuracy: 0.9779 - val_loss: 1.6031 - val_accuracy: 0.7452\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 43s 111ms/step - loss: 0.2621 - accuracy: 0.9750 - val_loss: 1.6829 - val_accuracy: 0.7449\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2624 - accuracy: 0.9748 - val_loss: 1.7368 - val_accuracy: 0.7208\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2509 - accuracy: 0.9795 - val_loss: 1.5128 - val_accuracy: 0.7429\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 40s 104ms/step - loss: 0.2710 - accuracy: 0.9721 - val_loss: 1.6884 - val_accuracy: 0.7200\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2609 - accuracy: 0.9757 - val_loss: 1.4394 - val_accuracy: 0.7579\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 42s 109ms/step - loss: 0.2528 - accuracy: 0.9781 - val_loss: 1.5111 - val_accuracy: 0.7412\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 0.2576 - accuracy: 0.9757 - val_loss: 1.5440 - val_accuracy: 0.7576\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2495 - accuracy: 0.9795 - val_loss: 1.5543 - val_accuracy: 0.7448\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2676 - accuracy: 0.9731 - val_loss: 1.9194 - val_accuracy: 0.7198\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2595 - accuracy: 0.9762 - val_loss: 1.8937 - val_accuracy: 0.7211\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2447 - accuracy: 0.9810 - val_loss: 1.5192 - val_accuracy: 0.7389\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2560 - accuracy: 0.9769 - val_loss: 2.3992 - val_accuracy: 0.6495\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.2607 - accuracy: 0.9745 - val_loss: 1.9709 - val_accuracy: 0.6793\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2527 - accuracy: 0.9777 - val_loss: 1.6557 - val_accuracy: 0.7261\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 42s 106ms/step - loss: 0.2529 - accuracy: 0.9781 - val_loss: 1.4728 - val_accuracy: 0.7487\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.2598 - accuracy: 0.9753 - val_loss: 1.8084 - val_accuracy: 0.7358\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2566 - accuracy: 0.9768 - val_loss: 1.6662 - val_accuracy: 0.7451\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2471 - accuracy: 0.9802 - val_loss: 1.4190 - val_accuracy: 0.7660\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 42s 107ms/step - loss: 0.2549 - accuracy: 0.9774 - val_loss: 1.7225 - val_accuracy: 0.7312\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.2522 - accuracy: 0.9779 - val_loss: 1.5581 - val_accuracy: 0.7438\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 41s 106ms/step - loss: 0.2506 - accuracy: 0.9784 - val_loss: 1.5617 - val_accuracy: 0.7313\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 43s 109ms/step - loss: 0.2551 - accuracy: 0.9774 - val_loss: 1.4296 - val_accuracy: 0.7660\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.2582 - accuracy: 0.9754 - val_loss: 1.4750 - val_accuracy: 0.7573\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 42s 108ms/step - loss: 0.2502 - accuracy: 0.9783 - val_loss: 1.4535 - val_accuracy: 0.7643\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 43s 110ms/step - loss: 0.2460 - accuracy: 0.9793 - val_loss: 1.6582 - val_accuracy: 0.7378\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2591 - accuracy: 0.9749 - val_loss: 1.7244 - val_accuracy: 0.7293\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2434 - accuracy: 0.9807 - val_loss: 1.6990 - val_accuracy: 0.7305\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2488 - accuracy: 0.9783 - val_loss: 1.8598 - val_accuracy: 0.7152\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2608 - accuracy: 0.9747 - val_loss: 1.6298 - val_accuracy: 0.7413\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2430 - accuracy: 0.9807 - val_loss: 1.6742 - val_accuracy: 0.7474\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2565 - accuracy: 0.9760 - val_loss: 1.4460 - val_accuracy: 0.7697\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2469 - accuracy: 0.9793 - val_loss: 1.8313 - val_accuracy: 0.7248\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2470 - accuracy: 0.9791 - val_loss: 1.9112 - val_accuracy: 0.7028\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2548 - accuracy: 0.9758 - val_loss: 1.5074 - val_accuracy: 0.7600\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2518 - accuracy: 0.9770 - val_loss: 1.7185 - val_accuracy: 0.7223\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2402 - accuracy: 0.9820 - val_loss: 1.6373 - val_accuracy: 0.7291\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2423 - accuracy: 0.9801 - val_loss: 2.3435 - val_accuracy: 0.6779\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2480 - accuracy: 0.9782 - val_loss: 1.5900 - val_accuracy: 0.7437\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2519 - accuracy: 0.9768 - val_loss: 1.4611 - val_accuracy: 0.7544\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2450 - accuracy: 0.9793 - val_loss: 2.0707 - val_accuracy: 0.7044\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2431 - accuracy: 0.9803 - val_loss: 1.4349 - val_accuracy: 0.7636\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2442 - accuracy: 0.9787 - val_loss: 1.5834 - val_accuracy: 0.7437\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2615 - accuracy: 0.9740 - val_loss: 1.7691 - val_accuracy: 0.7328\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2433 - accuracy: 0.9801 - val_loss: 1.5308 - val_accuracy: 0.7606\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2330 - accuracy: 0.9832 - val_loss: 1.5855 - val_accuracy: 0.7603\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2486 - accuracy: 0.9776 - val_loss: 1.4675 - val_accuracy: 0.7570\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2571 - accuracy: 0.9750 - val_loss: 1.4359 - val_accuracy: 0.7646\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2396 - accuracy: 0.9818 - val_loss: 1.9129 - val_accuracy: 0.7262\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2505 - accuracy: 0.9778 - val_loss: 1.9026 - val_accuracy: 0.7206\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2421 - accuracy: 0.9804 - val_loss: 1.4271 - val_accuracy: 0.7587\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2378 - accuracy: 0.9813 - val_loss: 1.5132 - val_accuracy: 0.7592\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2458 - accuracy: 0.9781 - val_loss: 2.3280 - val_accuracy: 0.6849\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2479 - accuracy: 0.9780 - val_loss: 1.8291 - val_accuracy: 0.7186\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2417 - accuracy: 0.9794 - val_loss: 1.5956 - val_accuracy: 0.7449\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2427 - accuracy: 0.9797 - val_loss: 1.7136 - val_accuracy: 0.7387\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2415 - accuracy: 0.9799 - val_loss: 1.6525 - val_accuracy: 0.7425\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2424 - accuracy: 0.9793 - val_loss: 2.2152 - val_accuracy: 0.6986\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2527 - accuracy: 0.9759 - val_loss: 1.6219 - val_accuracy: 0.7213\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2368 - accuracy: 0.9812 - val_loss: 1.9987 - val_accuracy: 0.7150\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2349 - accuracy: 0.9813 - val_loss: 2.3320 - val_accuracy: 0.6931\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2510 - accuracy: 0.9767 - val_loss: 1.8592 - val_accuracy: 0.7278\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2370 - accuracy: 0.9819 - val_loss: 1.7319 - val_accuracy: 0.7222\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2518 - accuracy: 0.9751 - val_loss: 2.1907 - val_accuracy: 0.6848\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2357 - accuracy: 0.9818 - val_loss: 1.5434 - val_accuracy: 0.7561\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2378 - accuracy: 0.9803 - val_loss: 1.8993 - val_accuracy: 0.7248\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2479 - accuracy: 0.9759 - val_loss: 1.4235 - val_accuracy: 0.7736\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2414 - accuracy: 0.9800 - val_loss: 1.8398 - val_accuracy: 0.7106\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2335 - accuracy: 0.9833 - val_loss: 1.7028 - val_accuracy: 0.7349\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2407 - accuracy: 0.9795 - val_loss: 2.0856 - val_accuracy: 0.7118\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2424 - accuracy: 0.9785 - val_loss: 1.4492 - val_accuracy: 0.7619\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2399 - accuracy: 0.9797 - val_loss: 1.5060 - val_accuracy: 0.7557\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2375 - accuracy: 0.9802 - val_loss: 1.8147 - val_accuracy: 0.7360\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2462 - accuracy: 0.9772 - val_loss: 1.5710 - val_accuracy: 0.7568\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2334 - accuracy: 0.9822 - val_loss: 1.4500 - val_accuracy: 0.7675\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2364 - accuracy: 0.9803 - val_loss: 1.4094 - val_accuracy: 0.7725\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2483 - accuracy: 0.9759 - val_loss: 1.5797 - val_accuracy: 0.7508\n",
      "✅ Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "history = train_model(sparsity=0.3, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89998a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 16)  64          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 16)  64          ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 32, 32, 16)  64          ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 32, 16)   0           ['activation_19[0][0]',          \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 16)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32, 32, 16)  64          ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 32, 32, 16)  64          ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 32, 32, 16)   0           ['activation_21[0][0]',          \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 16)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 32, 32, 16)   0           ['activation_23[0][0]',          \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 16)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   544         ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['conv2d_30[0][0]',              \n",
      "                                                                  'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_27[0][0]',          \n",
      "                                                                  'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_29[0][0]',          \n",
      "                                                                  'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 64)    256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['conv2d_37[0][0]',              \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 64)     0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['activation_33[0][0]',          \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 64)     0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['activation_35[0][0]',          \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['activation_37[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "✅ Applied Dual Gradient Iterative Pruning at sparsity 0.5\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 36s 88ms/step - loss: 1.5770 - accuracy: 0.4757 - val_loss: 1.4897 - val_accuracy: 0.5171\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 1.1696 - accuracy: 0.6290 - val_loss: 1.5925 - val_accuracy: 0.5206\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.9815 - accuracy: 0.6962 - val_loss: 1.1407 - val_accuracy: 0.6415\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.8534 - accuracy: 0.7451 - val_loss: 1.3927 - val_accuracy: 0.5920\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.7598 - accuracy: 0.7785 - val_loss: 0.9400 - val_accuracy: 0.7239\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.6876 - accuracy: 0.8054 - val_loss: 1.0071 - val_accuracy: 0.7128\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.6193 - accuracy: 0.8312 - val_loss: 1.0061 - val_accuracy: 0.7155\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5687 - accuracy: 0.8501 - val_loss: 1.5160 - val_accuracy: 0.6275\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.5198 - accuracy: 0.8679 - val_loss: 1.2892 - val_accuracy: 0.6564\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.4856 - accuracy: 0.8808 - val_loss: 1.4535 - val_accuracy: 0.6549\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.4418 - accuracy: 0.8977 - val_loss: 1.1747 - val_accuracy: 0.7049\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 36s 91ms/step - loss: 0.4151 - accuracy: 0.9073 - val_loss: 1.4483 - val_accuracy: 0.6692\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3920 - accuracy: 0.9165 - val_loss: 1.3179 - val_accuracy: 0.7002\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.3682 - accuracy: 0.9266 - val_loss: 1.5154 - val_accuracy: 0.6985\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3510 - accuracy: 0.9330 - val_loss: 1.2788 - val_accuracy: 0.7159\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 38s 98ms/step - loss: 0.3312 - accuracy: 0.9415 - val_loss: 1.2687 - val_accuracy: 0.7293\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.3253 - accuracy: 0.9428 - val_loss: 1.3254 - val_accuracy: 0.7142\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 36s 92ms/step - loss: 0.3096 - accuracy: 0.9497 - val_loss: 1.4656 - val_accuracy: 0.7231\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.3063 - accuracy: 0.9501 - val_loss: 1.4094 - val_accuracy: 0.7211\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2990 - accuracy: 0.9555 - val_loss: 1.4759 - val_accuracy: 0.7079\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2975 - accuracy: 0.9557 - val_loss: 1.3960 - val_accuracy: 0.7232\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2978 - accuracy: 0.9557 - val_loss: 1.4334 - val_accuracy: 0.7152\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2900 - accuracy: 0.9600 - val_loss: 1.8953 - val_accuracy: 0.6955\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2888 - accuracy: 0.9607 - val_loss: 1.3923 - val_accuracy: 0.7296\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2901 - accuracy: 0.9598 - val_loss: 1.3121 - val_accuracy: 0.7288\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2868 - accuracy: 0.9619 - val_loss: 2.0351 - val_accuracy: 0.6523\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2880 - accuracy: 0.9621 - val_loss: 2.3378 - val_accuracy: 0.6314\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2792 - accuracy: 0.9657 - val_loss: 1.6472 - val_accuracy: 0.7067\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2772 - accuracy: 0.9669 - val_loss: 1.4765 - val_accuracy: 0.7374\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2785 - accuracy: 0.9664 - val_loss: 1.3604 - val_accuracy: 0.7492\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2830 - accuracy: 0.9654 - val_loss: 1.4657 - val_accuracy: 0.7296\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2767 - accuracy: 0.9684 - val_loss: 1.9920 - val_accuracy: 0.6935\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2734 - accuracy: 0.9694 - val_loss: 1.6091 - val_accuracy: 0.7146\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2815 - accuracy: 0.9671 - val_loss: 1.4992 - val_accuracy: 0.7423\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2820 - accuracy: 0.9666 - val_loss: 1.7535 - val_accuracy: 0.6877\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2648 - accuracy: 0.9736 - val_loss: 1.7375 - val_accuracy: 0.7272\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2771 - accuracy: 0.9687 - val_loss: 1.7507 - val_accuracy: 0.7094\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2862 - accuracy: 0.9653 - val_loss: 1.9146 - val_accuracy: 0.7059\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2711 - accuracy: 0.9722 - val_loss: 1.6580 - val_accuracy: 0.7285\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2631 - accuracy: 0.9746 - val_loss: 1.6213 - val_accuracy: 0.7253\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2842 - accuracy: 0.9673 - val_loss: 1.4085 - val_accuracy: 0.7381\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2765 - accuracy: 0.9703 - val_loss: 1.7354 - val_accuracy: 0.7095\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2644 - accuracy: 0.9744 - val_loss: 1.8557 - val_accuracy: 0.7190\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2745 - accuracy: 0.9720 - val_loss: 3.1720 - val_accuracy: 0.5818\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2740 - accuracy: 0.9713 - val_loss: 1.5871 - val_accuracy: 0.7347\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2732 - accuracy: 0.9725 - val_loss: 1.6756 - val_accuracy: 0.7336\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2715 - accuracy: 0.9723 - val_loss: 2.6052 - val_accuracy: 0.6529\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2717 - accuracy: 0.9724 - val_loss: 1.6272 - val_accuracy: 0.7382\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2775 - accuracy: 0.9711 - val_loss: 1.7575 - val_accuracy: 0.7280\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2657 - accuracy: 0.9750 - val_loss: 1.7422 - val_accuracy: 0.7207\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2674 - accuracy: 0.9739 - val_loss: 1.9890 - val_accuracy: 0.6973\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2670 - accuracy: 0.9744 - val_loss: 1.6540 - val_accuracy: 0.7227\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2687 - accuracy: 0.9733 - val_loss: 1.9514 - val_accuracy: 0.7106\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2652 - accuracy: 0.9747 - val_loss: 1.7118 - val_accuracy: 0.7165\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2789 - accuracy: 0.9706 - val_loss: 1.7263 - val_accuracy: 0.7315\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2704 - accuracy: 0.9732 - val_loss: 1.6998 - val_accuracy: 0.7176\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2734 - accuracy: 0.9722 - val_loss: 1.8607 - val_accuracy: 0.7110\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2626 - accuracy: 0.9766 - val_loss: 1.6030 - val_accuracy: 0.7319\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2671 - accuracy: 0.9751 - val_loss: 1.9234 - val_accuracy: 0.6817\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2669 - accuracy: 0.9747 - val_loss: 1.7391 - val_accuracy: 0.7071\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2675 - accuracy: 0.9744 - val_loss: 2.0371 - val_accuracy: 0.6704\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2653 - accuracy: 0.9755 - val_loss: 1.9696 - val_accuracy: 0.7094\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2679 - accuracy: 0.9747 - val_loss: 1.9237 - val_accuracy: 0.7136\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2628 - accuracy: 0.9762 - val_loss: 1.4281 - val_accuracy: 0.7494\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2718 - accuracy: 0.9730 - val_loss: 2.4987 - val_accuracy: 0.6686\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2570 - accuracy: 0.9783 - val_loss: 1.8391 - val_accuracy: 0.7130\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2688 - accuracy: 0.9739 - val_loss: 1.9803 - val_accuracy: 0.7070\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2624 - accuracy: 0.9770 - val_loss: 2.1078 - val_accuracy: 0.6975\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2653 - accuracy: 0.9758 - val_loss: 1.9096 - val_accuracy: 0.7234\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2731 - accuracy: 0.9727 - val_loss: 1.6659 - val_accuracy: 0.7243\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2589 - accuracy: 0.9779 - val_loss: 1.5681 - val_accuracy: 0.7455\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2672 - accuracy: 0.9741 - val_loss: 1.6197 - val_accuracy: 0.7336\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2543 - accuracy: 0.9794 - val_loss: 1.5540 - val_accuracy: 0.7511\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2639 - accuracy: 0.9757 - val_loss: 2.1098 - val_accuracy: 0.6779\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2597 - accuracy: 0.9773 - val_loss: 1.6589 - val_accuracy: 0.7289\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2588 - accuracy: 0.9776 - val_loss: 1.8444 - val_accuracy: 0.7255\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2628 - accuracy: 0.9766 - val_loss: 1.6814 - val_accuracy: 0.7289\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2642 - accuracy: 0.9760 - val_loss: 1.7356 - val_accuracy: 0.7157\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2679 - accuracy: 0.9743 - val_loss: 1.8291 - val_accuracy: 0.7314\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2557 - accuracy: 0.9784 - val_loss: 1.8774 - val_accuracy: 0.7141\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2564 - accuracy: 0.9777 - val_loss: 1.7701 - val_accuracy: 0.7282\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2656 - accuracy: 0.9750 - val_loss: 1.6198 - val_accuracy: 0.7264\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2597 - accuracy: 0.9766 - val_loss: 2.6180 - val_accuracy: 0.6522\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2569 - accuracy: 0.9780 - val_loss: 1.6157 - val_accuracy: 0.7359\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2575 - accuracy: 0.9782 - val_loss: 1.6401 - val_accuracy: 0.7204\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2605 - accuracy: 0.9763 - val_loss: 2.0423 - val_accuracy: 0.7091\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2568 - accuracy: 0.9777 - val_loss: 1.9356 - val_accuracy: 0.7144\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2571 - accuracy: 0.9783 - val_loss: 1.7441 - val_accuracy: 0.7339\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2604 - accuracy: 0.9762 - val_loss: 1.5103 - val_accuracy: 0.7500\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2634 - accuracy: 0.9750 - val_loss: 1.5391 - val_accuracy: 0.7439\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2540 - accuracy: 0.9789 - val_loss: 1.6857 - val_accuracy: 0.7390\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2639 - accuracy: 0.9757 - val_loss: 1.7519 - val_accuracy: 0.7208\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2482 - accuracy: 0.9808 - val_loss: 1.5880 - val_accuracy: 0.7535\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2583 - accuracy: 0.9765 - val_loss: 1.7203 - val_accuracy: 0.7393\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2672 - accuracy: 0.9735 - val_loss: 1.5218 - val_accuracy: 0.7482\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2499 - accuracy: 0.9804 - val_loss: 1.8846 - val_accuracy: 0.7182\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2472 - accuracy: 0.9812 - val_loss: 1.7312 - val_accuracy: 0.7405\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2668 - accuracy: 0.9741 - val_loss: 1.3839 - val_accuracy: 0.7557\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2518 - accuracy: 0.9794 - val_loss: 1.6300 - val_accuracy: 0.7588\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2595 - accuracy: 0.9770 - val_loss: 1.7851 - val_accuracy: 0.7352\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2540 - accuracy: 0.9787 - val_loss: 1.4741 - val_accuracy: 0.7544\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2567 - accuracy: 0.9773 - val_loss: 1.7301 - val_accuracy: 0.7208\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2553 - accuracy: 0.9775 - val_loss: 1.6691 - val_accuracy: 0.7448\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2458 - accuracy: 0.9812 - val_loss: 1.5227 - val_accuracy: 0.7468\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2608 - accuracy: 0.9755 - val_loss: 1.4962 - val_accuracy: 0.7526\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2534 - accuracy: 0.9781 - val_loss: 1.7866 - val_accuracy: 0.7207\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2486 - accuracy: 0.9800 - val_loss: 1.9966 - val_accuracy: 0.7147\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2551 - accuracy: 0.9773 - val_loss: 1.4737 - val_accuracy: 0.7494\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2509 - accuracy: 0.9794 - val_loss: 1.4361 - val_accuracy: 0.7610\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2479 - accuracy: 0.9799 - val_loss: 1.7442 - val_accuracy: 0.7364\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2508 - accuracy: 0.9777 - val_loss: 1.6429 - val_accuracy: 0.7490\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2625 - accuracy: 0.9749 - val_loss: 1.5120 - val_accuracy: 0.7486\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2470 - accuracy: 0.9806 - val_loss: 2.1998 - val_accuracy: 0.7110\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2489 - accuracy: 0.9799 - val_loss: 1.5076 - val_accuracy: 0.7589\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2504 - accuracy: 0.9787 - val_loss: 1.7581 - val_accuracy: 0.7162\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2467 - accuracy: 0.9799 - val_loss: 1.5834 - val_accuracy: 0.7415\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2573 - accuracy: 0.9765 - val_loss: 1.6764 - val_accuracy: 0.7336\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2450 - accuracy: 0.9809 - val_loss: 1.8632 - val_accuracy: 0.7196\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2443 - accuracy: 0.9811 - val_loss: 1.7193 - val_accuracy: 0.7358\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2516 - accuracy: 0.9784 - val_loss: 2.2866 - val_accuracy: 0.6981\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2516 - accuracy: 0.9783 - val_loss: 1.9408 - val_accuracy: 0.7128\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2524 - accuracy: 0.9774 - val_loss: 1.4436 - val_accuracy: 0.7636\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2455 - accuracy: 0.9804 - val_loss: 1.5910 - val_accuracy: 0.7482\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2492 - accuracy: 0.9792 - val_loss: 1.9742 - val_accuracy: 0.6921\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2438 - accuracy: 0.9814 - val_loss: 1.5509 - val_accuracy: 0.7488\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2537 - accuracy: 0.9763 - val_loss: 1.3911 - val_accuracy: 0.7681\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2465 - accuracy: 0.9803 - val_loss: 1.9798 - val_accuracy: 0.7197\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2391 - accuracy: 0.9823 - val_loss: 1.8376 - val_accuracy: 0.7077\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2517 - accuracy: 0.9772 - val_loss: 2.0753 - val_accuracy: 0.7044\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2513 - accuracy: 0.9775 - val_loss: 1.9131 - val_accuracy: 0.7181\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2398 - accuracy: 0.9820 - val_loss: 1.7557 - val_accuracy: 0.7436\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2468 - accuracy: 0.9789 - val_loss: 1.6884 - val_accuracy: 0.7480\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2467 - accuracy: 0.9787 - val_loss: 1.6699 - val_accuracy: 0.7529\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2514 - accuracy: 0.9781 - val_loss: 1.9184 - val_accuracy: 0.7130\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2434 - accuracy: 0.9800 - val_loss: 1.7316 - val_accuracy: 0.7368\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2413 - accuracy: 0.9808 - val_loss: 1.4491 - val_accuracy: 0.7744\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2501 - accuracy: 0.9779 - val_loss: 1.4572 - val_accuracy: 0.7670\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2402 - accuracy: 0.9817 - val_loss: 1.5230 - val_accuracy: 0.7538\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2314 - accuracy: 0.9850 - val_loss: 1.7215 - val_accuracy: 0.7372\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2506 - accuracy: 0.9762 - val_loss: 1.8775 - val_accuracy: 0.7290\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2477 - accuracy: 0.9777 - val_loss: 1.8569 - val_accuracy: 0.7296\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2427 - accuracy: 0.9806 - val_loss: 1.4375 - val_accuracy: 0.7737\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2388 - accuracy: 0.9818 - val_loss: 2.0144 - val_accuracy: 0.7198\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2486 - accuracy: 0.9779 - val_loss: 1.7118 - val_accuracy: 0.7427\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2386 - accuracy: 0.9813 - val_loss: 1.7151 - val_accuracy: 0.7471\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2370 - accuracy: 0.9820 - val_loss: 1.6645 - val_accuracy: 0.7326\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2486 - accuracy: 0.9781 - val_loss: 1.4755 - val_accuracy: 0.7639\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2361 - accuracy: 0.9818 - val_loss: 1.5988 - val_accuracy: 0.7534\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 33s 83ms/step - loss: 0.2436 - accuracy: 0.9790 - val_loss: 1.6638 - val_accuracy: 0.7495\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 32s 83ms/step - loss: 0.2427 - accuracy: 0.9793 - val_loss: 1.6407 - val_accuracy: 0.7513\n",
      "✅ Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "history = train_model(sparsity=0.5, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e65386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 16)  64          ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 16)  64          ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 32, 32, 16)   0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 32, 32, 16)   0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32, 32, 16)  64          ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 32, 32, 16)   0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 32, 32, 16)   0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, 32, 16)  64          ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 32, 32, 16)   0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 16)   0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 16, 16, 32)   544         ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 16, 16, 32)   0           ['conv2d_51[0][0]',              \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 32)   0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 32)  128         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 16, 16, 32)   0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 32)   0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 32)  128         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 16, 16, 32)  128         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 16, 16, 32)   0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 16, 16, 32)   0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['conv2d_58[0][0]',              \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_54[0][0]',          \n",
      "                                                                  'batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 64)          0           ['activation_56[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           650         ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "✅ Applied Dual Gradient Iterative Pruning at sparsity 0.7\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 36s 85ms/step - loss: 1.6143 - accuracy: 0.4715 - val_loss: 1.6567 - val_accuracy: 0.4627\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.1888 - accuracy: 0.6243 - val_loss: 1.4578 - val_accuracy: 0.5222\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.9964 - accuracy: 0.6982 - val_loss: 1.1107 - val_accuracy: 0.6650\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.8600 - accuracy: 0.7468 - val_loss: 1.1440 - val_accuracy: 0.6442\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.7654 - accuracy: 0.7815 - val_loss: 1.3664 - val_accuracy: 0.6165\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.6926 - accuracy: 0.8066 - val_loss: 1.4945 - val_accuracy: 0.6032\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.6266 - accuracy: 0.8324 - val_loss: 1.4363 - val_accuracy: 0.6138\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.5830 - accuracy: 0.8475 - val_loss: 1.2447 - val_accuracy: 0.6797\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.5278 - accuracy: 0.8680 - val_loss: 1.3048 - val_accuracy: 0.6474\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.4922 - accuracy: 0.8811 - val_loss: 1.1714 - val_accuracy: 0.7115\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.4431 - accuracy: 0.9006 - val_loss: 1.4564 - val_accuracy: 0.6635\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.4152 - accuracy: 0.9116 - val_loss: 1.5020 - val_accuracy: 0.6716\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3898 - accuracy: 0.9205 - val_loss: 1.6655 - val_accuracy: 0.6747\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.3678 - accuracy: 0.9299 - val_loss: 1.1194 - val_accuracy: 0.7303\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.3486 - accuracy: 0.9372 - val_loss: 1.5846 - val_accuracy: 0.6820\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3373 - accuracy: 0.9408 - val_loss: 1.3360 - val_accuracy: 0.7400\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.3198 - accuracy: 0.9478 - val_loss: 1.4145 - val_accuracy: 0.7116\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3207 - accuracy: 0.9487 - val_loss: 1.6810 - val_accuracy: 0.6722\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.3126 - accuracy: 0.9516 - val_loss: 1.3545 - val_accuracy: 0.7258\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3006 - accuracy: 0.9562 - val_loss: 1.6687 - val_accuracy: 0.6951\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3000 - accuracy: 0.9568 - val_loss: 1.7687 - val_accuracy: 0.7000\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2938 - accuracy: 0.9604 - val_loss: 2.1872 - val_accuracy: 0.6298\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.3042 - accuracy: 0.9572 - val_loss: 1.6132 - val_accuracy: 0.7115\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2901 - accuracy: 0.9620 - val_loss: 2.6522 - val_accuracy: 0.6076\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2937 - accuracy: 0.9611 - val_loss: 1.8768 - val_accuracy: 0.6767\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2920 - accuracy: 0.9631 - val_loss: 2.0877 - val_accuracy: 0.6607\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2924 - accuracy: 0.9634 - val_loss: 2.1472 - val_accuracy: 0.6414\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2889 - accuracy: 0.9646 - val_loss: 1.8503 - val_accuracy: 0.6838\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2830 - accuracy: 0.9675 - val_loss: 1.8590 - val_accuracy: 0.6865\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2794 - accuracy: 0.9690 - val_loss: 2.5032 - val_accuracy: 0.6303\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2895 - accuracy: 0.9652 - val_loss: 1.9165 - val_accuracy: 0.6773\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2824 - accuracy: 0.9681 - val_loss: 1.9524 - val_accuracy: 0.6983\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2855 - accuracy: 0.9664 - val_loss: 1.7778 - val_accuracy: 0.7051\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2803 - accuracy: 0.9696 - val_loss: 2.0072 - val_accuracy: 0.6763\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2848 - accuracy: 0.9675 - val_loss: 1.8505 - val_accuracy: 0.6874\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2804 - accuracy: 0.9695 - val_loss: 2.3497 - val_accuracy: 0.6583\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2807 - accuracy: 0.9699 - val_loss: 1.9943 - val_accuracy: 0.7120\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2899 - accuracy: 0.9669 - val_loss: 1.6330 - val_accuracy: 0.7157\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2769 - accuracy: 0.9715 - val_loss: 1.8093 - val_accuracy: 0.6972\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2671 - accuracy: 0.9746 - val_loss: 1.7976 - val_accuracy: 0.7168\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2771 - accuracy: 0.9714 - val_loss: 1.9029 - val_accuracy: 0.6880\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2859 - accuracy: 0.9685 - val_loss: 1.7026 - val_accuracy: 0.7114\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2747 - accuracy: 0.9723 - val_loss: 1.5418 - val_accuracy: 0.7389\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2720 - accuracy: 0.9735 - val_loss: 1.6070 - val_accuracy: 0.7345\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2855 - accuracy: 0.9684 - val_loss: 1.5116 - val_accuracy: 0.7394\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2755 - accuracy: 0.9729 - val_loss: 2.2114 - val_accuracy: 0.6945\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2838 - accuracy: 0.9703 - val_loss: 1.5948 - val_accuracy: 0.7318\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2746 - accuracy: 0.9733 - val_loss: 1.5727 - val_accuracy: 0.7455\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2758 - accuracy: 0.9725 - val_loss: 1.3995 - val_accuracy: 0.7699\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2675 - accuracy: 0.9754 - val_loss: 1.5744 - val_accuracy: 0.7252\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2831 - accuracy: 0.9698 - val_loss: 1.6286 - val_accuracy: 0.7251\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2746 - accuracy: 0.9736 - val_loss: 1.5557 - val_accuracy: 0.7375\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2772 - accuracy: 0.9734 - val_loss: 2.7146 - val_accuracy: 0.6303\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2748 - accuracy: 0.9728 - val_loss: 1.6840 - val_accuracy: 0.7369\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 33s 86ms/step - loss: 0.2662 - accuracy: 0.9767 - val_loss: 1.6683 - val_accuracy: 0.7292\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2809 - accuracy: 0.9712 - val_loss: 2.0524 - val_accuracy: 0.7069\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2688 - accuracy: 0.9755 - val_loss: 1.5274 - val_accuracy: 0.7446\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2794 - accuracy: 0.9721 - val_loss: 1.9454 - val_accuracy: 0.7158\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2745 - accuracy: 0.9737 - val_loss: 1.4991 - val_accuracy: 0.7504\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2728 - accuracy: 0.9736 - val_loss: 1.4192 - val_accuracy: 0.7608\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2661 - accuracy: 0.9759 - val_loss: 1.8430 - val_accuracy: 0.6958\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2666 - accuracy: 0.9766 - val_loss: 1.4652 - val_accuracy: 0.7640\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2776 - accuracy: 0.9718 - val_loss: 1.5061 - val_accuracy: 0.7435\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2648 - accuracy: 0.9772 - val_loss: 2.4268 - val_accuracy: 0.6616\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2719 - accuracy: 0.9748 - val_loss: 1.5399 - val_accuracy: 0.7405\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2636 - accuracy: 0.9774 - val_loss: 2.1931 - val_accuracy: 0.6819\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2722 - accuracy: 0.9741 - val_loss: 1.4762 - val_accuracy: 0.7429\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2704 - accuracy: 0.9747 - val_loss: 1.4609 - val_accuracy: 0.7517\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2709 - accuracy: 0.9753 - val_loss: 1.8212 - val_accuracy: 0.6993\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2622 - accuracy: 0.9776 - val_loss: 2.1760 - val_accuracy: 0.6804\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2683 - accuracy: 0.9751 - val_loss: 1.7829 - val_accuracy: 0.7117\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2705 - accuracy: 0.9750 - val_loss: 1.5028 - val_accuracy: 0.7484\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2701 - accuracy: 0.9749 - val_loss: 1.8540 - val_accuracy: 0.7078\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2619 - accuracy: 0.9781 - val_loss: 1.9940 - val_accuracy: 0.7011\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2583 - accuracy: 0.9786 - val_loss: 1.5060 - val_accuracy: 0.7459\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2785 - accuracy: 0.9709 - val_loss: 1.7164 - val_accuracy: 0.7353\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2699 - accuracy: 0.9748 - val_loss: 1.6236 - val_accuracy: 0.7430\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2614 - accuracy: 0.9775 - val_loss: 1.5751 - val_accuracy: 0.7381\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2680 - accuracy: 0.9752 - val_loss: 1.6952 - val_accuracy: 0.7376\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2585 - accuracy: 0.9790 - val_loss: 1.7812 - val_accuracy: 0.7239\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2618 - accuracy: 0.9777 - val_loss: 1.6892 - val_accuracy: 0.7289\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2672 - accuracy: 0.9750 - val_loss: 1.6110 - val_accuracy: 0.7310\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2618 - accuracy: 0.9780 - val_loss: 1.8899 - val_accuracy: 0.7048\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2671 - accuracy: 0.9757 - val_loss: 1.9021 - val_accuracy: 0.7121\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2614 - accuracy: 0.9774 - val_loss: 1.6519 - val_accuracy: 0.7319\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2640 - accuracy: 0.9761 - val_loss: 1.5997 - val_accuracy: 0.7470\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2578 - accuracy: 0.9786 - val_loss: 2.0952 - val_accuracy: 0.7103\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2685 - accuracy: 0.9741 - val_loss: 1.5758 - val_accuracy: 0.7267\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2595 - accuracy: 0.9780 - val_loss: 1.7664 - val_accuracy: 0.7166\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2608 - accuracy: 0.9775 - val_loss: 1.6434 - val_accuracy: 0.7222\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2557 - accuracy: 0.9790 - val_loss: 1.8182 - val_accuracy: 0.7109\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2643 - accuracy: 0.9754 - val_loss: 1.6582 - val_accuracy: 0.7397\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2602 - accuracy: 0.9778 - val_loss: 1.5261 - val_accuracy: 0.7350\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2536 - accuracy: 0.9797 - val_loss: 1.5490 - val_accuracy: 0.7582\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2645 - accuracy: 0.9758 - val_loss: 1.4732 - val_accuracy: 0.7663\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2643 - accuracy: 0.9765 - val_loss: 1.5567 - val_accuracy: 0.7434\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2524 - accuracy: 0.9806 - val_loss: 1.4699 - val_accuracy: 0.7623\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2507 - accuracy: 0.9803 - val_loss: 1.7506 - val_accuracy: 0.7282\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2592 - accuracy: 0.9772 - val_loss: 2.0440 - val_accuracy: 0.6937\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2647 - accuracy: 0.9752 - val_loss: 1.6227 - val_accuracy: 0.7469\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2569 - accuracy: 0.9787 - val_loss: 1.7903 - val_accuracy: 0.7115\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2566 - accuracy: 0.9782 - val_loss: 1.5685 - val_accuracy: 0.7502\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2495 - accuracy: 0.9805 - val_loss: 1.4879 - val_accuracy: 0.7458\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2580 - accuracy: 0.9784 - val_loss: 1.5095 - val_accuracy: 0.7619\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2592 - accuracy: 0.9777 - val_loss: 2.1661 - val_accuracy: 0.6865\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2608 - accuracy: 0.9759 - val_loss: 1.8529 - val_accuracy: 0.7046\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2498 - accuracy: 0.9810 - val_loss: 1.6948 - val_accuracy: 0.7155\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2579 - accuracy: 0.9775 - val_loss: 1.7857 - val_accuracy: 0.7327\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2614 - accuracy: 0.9765 - val_loss: 1.5167 - val_accuracy: 0.7421\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2481 - accuracy: 0.9813 - val_loss: 1.5410 - val_accuracy: 0.7523\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2515 - accuracy: 0.9795 - val_loss: 2.1003 - val_accuracy: 0.7156\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2553 - accuracy: 0.9773 - val_loss: 1.9896 - val_accuracy: 0.6940\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2640 - accuracy: 0.9760 - val_loss: 1.7998 - val_accuracy: 0.7191\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2535 - accuracy: 0.9786 - val_loss: 1.7426 - val_accuracy: 0.7285\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2456 - accuracy: 0.9816 - val_loss: 1.6960 - val_accuracy: 0.7305\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2437 - accuracy: 0.9823 - val_loss: 1.5487 - val_accuracy: 0.7610\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2625 - accuracy: 0.9744 - val_loss: 1.9226 - val_accuracy: 0.7169\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2514 - accuracy: 0.9786 - val_loss: 1.7078 - val_accuracy: 0.7503\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2468 - accuracy: 0.9811 - val_loss: 2.0001 - val_accuracy: 0.7206\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2534 - accuracy: 0.9783 - val_loss: 1.7781 - val_accuracy: 0.7492\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2526 - accuracy: 0.9787 - val_loss: 1.4133 - val_accuracy: 0.7569\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2585 - accuracy: 0.9763 - val_loss: 1.6137 - val_accuracy: 0.7568\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2490 - accuracy: 0.9801 - val_loss: 1.5411 - val_accuracy: 0.7473\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2521 - accuracy: 0.9793 - val_loss: 2.4096 - val_accuracy: 0.6681\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2422 - accuracy: 0.9820 - val_loss: 1.6109 - val_accuracy: 0.7617\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2534 - accuracy: 0.9784 - val_loss: 1.5130 - val_accuracy: 0.7574\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2511 - accuracy: 0.9784 - val_loss: 1.4641 - val_accuracy: 0.7583\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2437 - accuracy: 0.9814 - val_loss: 1.3283 - val_accuracy: 0.7689\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2429 - accuracy: 0.9812 - val_loss: 1.7014 - val_accuracy: 0.7400\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2485 - accuracy: 0.9789 - val_loss: 1.5273 - val_accuracy: 0.7423\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2503 - accuracy: 0.9788 - val_loss: 1.7259 - val_accuracy: 0.7195\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2522 - accuracy: 0.9781 - val_loss: 1.6526 - val_accuracy: 0.7295\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2542 - accuracy: 0.9777 - val_loss: 1.5634 - val_accuracy: 0.7526\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2398 - accuracy: 0.9826 - val_loss: 1.6041 - val_accuracy: 0.7617\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2515 - accuracy: 0.9781 - val_loss: 1.6821 - val_accuracy: 0.7340\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2441 - accuracy: 0.9808 - val_loss: 1.7237 - val_accuracy: 0.7251\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2505 - accuracy: 0.9781 - val_loss: 1.3570 - val_accuracy: 0.7688\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2455 - accuracy: 0.9801 - val_loss: 1.7757 - val_accuracy: 0.7391\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2400 - accuracy: 0.9820 - val_loss: 2.1021 - val_accuracy: 0.7261\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2502 - accuracy: 0.9787 - val_loss: 1.6216 - val_accuracy: 0.7525\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2457 - accuracy: 0.9799 - val_loss: 1.6709 - val_accuracy: 0.7309\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2462 - accuracy: 0.9791 - val_loss: 1.7827 - val_accuracy: 0.7552\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2439 - accuracy: 0.9809 - val_loss: 1.6171 - val_accuracy: 0.7653\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2460 - accuracy: 0.9797 - val_loss: 1.8631 - val_accuracy: 0.7264\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2497 - accuracy: 0.9785 - val_loss: 1.4596 - val_accuracy: 0.7527\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2448 - accuracy: 0.9795 - val_loss: 1.5916 - val_accuracy: 0.7413\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2411 - accuracy: 0.9813 - val_loss: 1.8843 - val_accuracy: 0.7198\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2398 - accuracy: 0.9811 - val_loss: 1.6908 - val_accuracy: 0.7297\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2491 - accuracy: 0.9784 - val_loss: 1.8408 - val_accuracy: 0.7294\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2426 - accuracy: 0.9805 - val_loss: 1.4478 - val_accuracy: 0.7600\n",
      "✅ Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "history = train_model(sparsity=0.7, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d962e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   448         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   544         ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 32)  128         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 32)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 32)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 16, 16, 32)  128         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 8, 8, 64)     0           ['conv2d_79[0][0]',              \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 64)     0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 64)    256         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 8, 8, 64)     0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 64)     0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 64)    256         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['activation_73[0][0]',          \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 64)     0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 64)          0           ['activation_75[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "✅ Applied Dual Gradient Iterative Pruning at sparsity 0.9\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - 36s 86ms/step - loss: 1.6772 - accuracy: 0.4630 - val_loss: 1.5557 - val_accuracy: 0.4993\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 1.1594 - accuracy: 0.6376 - val_loss: 2.2099 - val_accuracy: 0.3662\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.9582 - accuracy: 0.7149 - val_loss: 1.1897 - val_accuracy: 0.6459\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.8394 - accuracy: 0.7565 - val_loss: 1.3766 - val_accuracy: 0.5901\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.7553 - accuracy: 0.7862 - val_loss: 1.3628 - val_accuracy: 0.6116\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.6848 - accuracy: 0.8120 - val_loss: 1.2672 - val_accuracy: 0.6536\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.6214 - accuracy: 0.8355 - val_loss: 1.1331 - val_accuracy: 0.6894\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.5703 - accuracy: 0.8519 - val_loss: 1.1652 - val_accuracy: 0.6747\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.5228 - accuracy: 0.8715 - val_loss: 1.2882 - val_accuracy: 0.6768\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.4877 - accuracy: 0.8846 - val_loss: 1.0566 - val_accuracy: 0.7262\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.4437 - accuracy: 0.8997 - val_loss: 1.0179 - val_accuracy: 0.7422\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.4178 - accuracy: 0.9096 - val_loss: 1.6908 - val_accuracy: 0.6458\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3905 - accuracy: 0.9206 - val_loss: 1.7668 - val_accuracy: 0.6462\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3778 - accuracy: 0.9247 - val_loss: 1.3294 - val_accuracy: 0.7180\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3495 - accuracy: 0.9360 - val_loss: 1.1354 - val_accuracy: 0.7404\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3395 - accuracy: 0.9403 - val_loss: 1.3277 - val_accuracy: 0.7308\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3276 - accuracy: 0.9446 - val_loss: 1.7554 - val_accuracy: 0.6631\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3232 - accuracy: 0.9472 - val_loss: 1.6796 - val_accuracy: 0.7153\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.3160 - accuracy: 0.9504 - val_loss: 1.8317 - val_accuracy: 0.6785\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.3044 - accuracy: 0.9551 - val_loss: 1.4891 - val_accuracy: 0.7072\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.3122 - accuracy: 0.9525 - val_loss: 1.7539 - val_accuracy: 0.6900\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2904 - accuracy: 0.9601 - val_loss: 1.4804 - val_accuracy: 0.7225\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2926 - accuracy: 0.9611 - val_loss: 1.5399 - val_accuracy: 0.7181\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.3018 - accuracy: 0.9590 - val_loss: 2.5927 - val_accuracy: 0.5972\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2891 - accuracy: 0.9616 - val_loss: 1.4419 - val_accuracy: 0.7238\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2823 - accuracy: 0.9656 - val_loss: 1.5674 - val_accuracy: 0.7288\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2971 - accuracy: 0.9607 - val_loss: 1.5033 - val_accuracy: 0.7429\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2800 - accuracy: 0.9667 - val_loss: 1.5702 - val_accuracy: 0.7213\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2793 - accuracy: 0.9668 - val_loss: 1.8935 - val_accuracy: 0.6892\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2912 - accuracy: 0.9635 - val_loss: 2.4462 - val_accuracy: 0.6299\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2831 - accuracy: 0.9672 - val_loss: 1.4693 - val_accuracy: 0.7371\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2837 - accuracy: 0.9660 - val_loss: 2.0346 - val_accuracy: 0.6720\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2786 - accuracy: 0.9681 - val_loss: 2.0304 - val_accuracy: 0.6833\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2887 - accuracy: 0.9649 - val_loss: 1.8068 - val_accuracy: 0.7026\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2755 - accuracy: 0.9705 - val_loss: 1.4558 - val_accuracy: 0.7431\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2739 - accuracy: 0.9705 - val_loss: 1.4892 - val_accuracy: 0.7538\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2713 - accuracy: 0.9715 - val_loss: 1.6286 - val_accuracy: 0.7206\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2799 - accuracy: 0.9685 - val_loss: 1.8598 - val_accuracy: 0.6958\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2783 - accuracy: 0.9702 - val_loss: 2.2303 - val_accuracy: 0.6666\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2803 - accuracy: 0.9687 - val_loss: 2.1309 - val_accuracy: 0.6841\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2639 - accuracy: 0.9753 - val_loss: 1.6987 - val_accuracy: 0.7154\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2798 - accuracy: 0.9689 - val_loss: 1.6154 - val_accuracy: 0.7334\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2791 - accuracy: 0.9693 - val_loss: 1.5443 - val_accuracy: 0.7278\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2648 - accuracy: 0.9757 - val_loss: 1.6407 - val_accuracy: 0.7137\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2672 - accuracy: 0.9738 - val_loss: 1.9543 - val_accuracy: 0.6859\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2802 - accuracy: 0.9698 - val_loss: 1.4519 - val_accuracy: 0.7407\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2770 - accuracy: 0.9709 - val_loss: 1.8446 - val_accuracy: 0.6843\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2714 - accuracy: 0.9730 - val_loss: 1.6425 - val_accuracy: 0.7301\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2750 - accuracy: 0.9718 - val_loss: 1.8002 - val_accuracy: 0.7199\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2677 - accuracy: 0.9741 - val_loss: 1.4778 - val_accuracy: 0.7468\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2709 - accuracy: 0.9736 - val_loss: 2.0332 - val_accuracy: 0.6769\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2751 - accuracy: 0.9719 - val_loss: 1.6295 - val_accuracy: 0.7395\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2686 - accuracy: 0.9741 - val_loss: 1.6597 - val_accuracy: 0.7248\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2645 - accuracy: 0.9752 - val_loss: 1.6548 - val_accuracy: 0.7309\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2708 - accuracy: 0.9728 - val_loss: 2.3342 - val_accuracy: 0.6682\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2674 - accuracy: 0.9739 - val_loss: 1.4177 - val_accuracy: 0.7506\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2691 - accuracy: 0.9746 - val_loss: 1.7130 - val_accuracy: 0.7178\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2733 - accuracy: 0.9735 - val_loss: 1.5413 - val_accuracy: 0.7302\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2631 - accuracy: 0.9756 - val_loss: 1.9254 - val_accuracy: 0.7039\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2665 - accuracy: 0.9746 - val_loss: 1.9805 - val_accuracy: 0.6933\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2596 - accuracy: 0.9768 - val_loss: 1.5122 - val_accuracy: 0.7395\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2760 - accuracy: 0.9716 - val_loss: 1.7100 - val_accuracy: 0.7189\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2665 - accuracy: 0.9751 - val_loss: 1.7395 - val_accuracy: 0.7350\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2537 - accuracy: 0.9790 - val_loss: 2.0316 - val_accuracy: 0.7147\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2729 - accuracy: 0.9712 - val_loss: 1.5115 - val_accuracy: 0.7449\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2613 - accuracy: 0.9768 - val_loss: 1.6983 - val_accuracy: 0.7214\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.2611 - accuracy: 0.9766 - val_loss: 1.3802 - val_accuracy: 0.7648\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2699 - accuracy: 0.9737 - val_loss: 1.9281 - val_accuracy: 0.6907\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2654 - accuracy: 0.9752 - val_loss: 1.7599 - val_accuracy: 0.7180\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2605 - accuracy: 0.9773 - val_loss: 1.6978 - val_accuracy: 0.7347\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2620 - accuracy: 0.9754 - val_loss: 2.0784 - val_accuracy: 0.6992\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2643 - accuracy: 0.9758 - val_loss: 1.7244 - val_accuracy: 0.7290\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2635 - accuracy: 0.9750 - val_loss: 2.1788 - val_accuracy: 0.6918\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2615 - accuracy: 0.9766 - val_loss: 1.9085 - val_accuracy: 0.7117\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2557 - accuracy: 0.9784 - val_loss: 2.2645 - val_accuracy: 0.6964\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2629 - accuracy: 0.9751 - val_loss: 1.5577 - val_accuracy: 0.7540\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2608 - accuracy: 0.9757 - val_loss: 1.7562 - val_accuracy: 0.7305\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2656 - accuracy: 0.9748 - val_loss: 1.5249 - val_accuracy: 0.7479\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2588 - accuracy: 0.9774 - val_loss: 1.4886 - val_accuracy: 0.7552\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2584 - accuracy: 0.9771 - val_loss: 1.3542 - val_accuracy: 0.7630\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2510 - accuracy: 0.9803 - val_loss: 1.7910 - val_accuracy: 0.7243\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2592 - accuracy: 0.9763 - val_loss: 1.4108 - val_accuracy: 0.7527\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2683 - accuracy: 0.9734 - val_loss: 1.8328 - val_accuracy: 0.7162\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2529 - accuracy: 0.9791 - val_loss: 1.8784 - val_accuracy: 0.7337\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2539 - accuracy: 0.9781 - val_loss: 1.8436 - val_accuracy: 0.7219\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2579 - accuracy: 0.9772 - val_loss: 2.4532 - val_accuracy: 0.6764\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2506 - accuracy: 0.9795 - val_loss: 1.8433 - val_accuracy: 0.7025\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2580 - accuracy: 0.9768 - val_loss: 1.6744 - val_accuracy: 0.7240\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2590 - accuracy: 0.9761 - val_loss: 1.6353 - val_accuracy: 0.7485\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2561 - accuracy: 0.9781 - val_loss: 1.8505 - val_accuracy: 0.7056\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2530 - accuracy: 0.9787 - val_loss: 1.5285 - val_accuracy: 0.7542\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2619 - accuracy: 0.9755 - val_loss: 1.5763 - val_accuracy: 0.7562\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2459 - accuracy: 0.9810 - val_loss: 1.5869 - val_accuracy: 0.7556\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2575 - accuracy: 0.9767 - val_loss: 2.5074 - val_accuracy: 0.6523\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2584 - accuracy: 0.9766 - val_loss: 1.4829 - val_accuracy: 0.7552\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2533 - accuracy: 0.9777 - val_loss: 2.0660 - val_accuracy: 0.7051\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2513 - accuracy: 0.9784 - val_loss: 1.9111 - val_accuracy: 0.7223\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2441 - accuracy: 0.9809 - val_loss: 1.4953 - val_accuracy: 0.7653\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2634 - accuracy: 0.9746 - val_loss: 2.5895 - val_accuracy: 0.6625\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - 34s 86ms/step - loss: 0.2469 - accuracy: 0.9804 - val_loss: 1.4088 - val_accuracy: 0.7621\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2442 - accuracy: 0.9804 - val_loss: 1.5777 - val_accuracy: 0.7359\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2563 - accuracy: 0.9763 - val_loss: 1.7270 - val_accuracy: 0.7244\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.2519 - accuracy: 0.9783 - val_loss: 1.6988 - val_accuracy: 0.7268\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2469 - accuracy: 0.9793 - val_loss: 1.4854 - val_accuracy: 0.7560\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2517 - accuracy: 0.9783 - val_loss: 1.8128 - val_accuracy: 0.7132\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2587 - accuracy: 0.9752 - val_loss: 1.7268 - val_accuracy: 0.7481\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2406 - accuracy: 0.9816 - val_loss: 1.2816 - val_accuracy: 0.7754\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2425 - accuracy: 0.9798 - val_loss: 1.8953 - val_accuracy: 0.7208\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2540 - accuracy: 0.9769 - val_loss: 1.6514 - val_accuracy: 0.7505\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2566 - accuracy: 0.9767 - val_loss: 1.6503 - val_accuracy: 0.7347\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2479 - accuracy: 0.9794 - val_loss: 1.6242 - val_accuracy: 0.7557\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2464 - accuracy: 0.9788 - val_loss: 1.6537 - val_accuracy: 0.7378\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2532 - accuracy: 0.9774 - val_loss: 1.6731 - val_accuracy: 0.7456\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2377 - accuracy: 0.9830 - val_loss: 1.7422 - val_accuracy: 0.7249\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2518 - accuracy: 0.9772 - val_loss: 1.8806 - val_accuracy: 0.7094\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2479 - accuracy: 0.9787 - val_loss: 1.6383 - val_accuracy: 0.7480\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2497 - accuracy: 0.9777 - val_loss: 2.3686 - val_accuracy: 0.7003\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2440 - accuracy: 0.9809 - val_loss: 1.6252 - val_accuracy: 0.7446\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2553 - accuracy: 0.9750 - val_loss: 2.1807 - val_accuracy: 0.7014\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2480 - accuracy: 0.9789 - val_loss: 1.6404 - val_accuracy: 0.7510\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2354 - accuracy: 0.9832 - val_loss: 2.0645 - val_accuracy: 0.7240\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2440 - accuracy: 0.9799 - val_loss: 1.6435 - val_accuracy: 0.7351\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.2497 - accuracy: 0.9778 - val_loss: 1.6483 - val_accuracy: 0.7455\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2438 - accuracy: 0.9796 - val_loss: 2.5750 - val_accuracy: 0.6728\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2454 - accuracy: 0.9795 - val_loss: 2.0199 - val_accuracy: 0.7030\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2479 - accuracy: 0.9779 - val_loss: 1.4275 - val_accuracy: 0.7581\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2425 - accuracy: 0.9803 - val_loss: 1.9386 - val_accuracy: 0.7230\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2358 - accuracy: 0.9819 - val_loss: 1.7195 - val_accuracy: 0.7413\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2470 - accuracy: 0.9782 - val_loss: 1.4850 - val_accuracy: 0.7471\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2529 - accuracy: 0.9768 - val_loss: 1.5591 - val_accuracy: 0.7549\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2407 - accuracy: 0.9810 - val_loss: 1.7466 - val_accuracy: 0.7460\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2468 - accuracy: 0.9786 - val_loss: 1.5862 - val_accuracy: 0.7445\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2463 - accuracy: 0.9792 - val_loss: 2.0102 - val_accuracy: 0.7091\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2380 - accuracy: 0.9813 - val_loss: 1.7211 - val_accuracy: 0.7333\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2431 - accuracy: 0.9803 - val_loss: 1.6213 - val_accuracy: 0.7520\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2441 - accuracy: 0.9790 - val_loss: 1.6122 - val_accuracy: 0.7359\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2396 - accuracy: 0.9810 - val_loss: 2.1901 - val_accuracy: 0.6942\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2397 - accuracy: 0.9803 - val_loss: 1.5862 - val_accuracy: 0.7603\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2492 - accuracy: 0.9772 - val_loss: 1.9243 - val_accuracy: 0.7133\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2384 - accuracy: 0.9808 - val_loss: 1.6807 - val_accuracy: 0.7486\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2424 - accuracy: 0.9798 - val_loss: 1.7915 - val_accuracy: 0.7283\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2313 - accuracy: 0.9828 - val_loss: 1.6222 - val_accuracy: 0.7474\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2353 - accuracy: 0.9818 - val_loss: 1.6746 - val_accuracy: 0.7423\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2465 - accuracy: 0.9782 - val_loss: 1.4054 - val_accuracy: 0.7740\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2358 - accuracy: 0.9813 - val_loss: 1.5242 - val_accuracy: 0.7682\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2345 - accuracy: 0.9810 - val_loss: 1.7931 - val_accuracy: 0.7440\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2372 - accuracy: 0.9805 - val_loss: 1.5695 - val_accuracy: 0.7518\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2427 - accuracy: 0.9787 - val_loss: 1.5431 - val_accuracy: 0.7557\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2420 - accuracy: 0.9796 - val_loss: 1.5563 - val_accuracy: 0.7424\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - 33s 85ms/step - loss: 0.2405 - accuracy: 0.9796 - val_loss: 1.7344 - val_accuracy: 0.7321\n",
      "✅ Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "history = train_model(sparsity=0.9, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5be666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
