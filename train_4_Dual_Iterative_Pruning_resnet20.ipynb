{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from models.resnet20 import resnet20  # Ensure this is in your Python path\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------\n",
    "# Dual Gradient-Based Iterative Pruning\n",
    "# -------------------------\n",
    "def dual_gradient_iterative_pruning(model, x_train, y_train, sparsity, iterations=10):\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_train[:256], training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y_train[:256], logits)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "        scores = [tf.abs(g * w) for g, w in zip(grads, model.trainable_weights) if g is not None]\n",
    "        all_scores = tf.concat([tf.reshape(score, [-1]) for score in scores], axis=0)\n",
    "        k = int((1 - sparsity) * tf.size(all_scores).numpy())\n",
    "        threshold = tf.sort(all_scores)[k].numpy()\n",
    "\n",
    "        masks = [(tf.abs(g * w) > threshold) if g is not None else tf.ones_like(w) for g, w in zip(grads, model.trainable_weights)]\n",
    "\n",
    "        # Apply masks to weights\n",
    "        for var, mask in zip(model.trainable_weights, masks):\n",
    "            var.assign(var * tf.cast(mask, tf.float32))\n",
    "\n",
    "    print(f\"✅ Applied Dual Gradient Iterative Pruning at sparsity {sparsity}\")\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Training Function\n",
    "# -------------------------\n",
    "def train_model(sparsity=0.5, batch_size=128, epochs=120):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "    model = resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    # Prune the model\n",
    "    model = dual_gradient_iterative_pruning(model, x_train, y_train, sparsity)\n",
    "\n",
    "    # Compile and train\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    # Save model\n",
    "    model.save(f'models/pruned_resnet20_dual_iterative_sparsity_{sparsity}.h5')\n",
    "    print(\"✅ Model training complete and saved.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(sparsity=0.4, batch_size=64, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
