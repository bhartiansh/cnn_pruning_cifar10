{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c91c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from models.resnet20 import build_resnet20\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "# --------------------------\n",
    "# Global Magnitude Pruning\n",
    "# --------------------------\n",
    "def global_magnitude_pruning(model, sparsity):\n",
    "    print(f\"[INFO] Applying global magnitude pruning with sparsity: {sparsity}\")\n",
    "    all_weights = []\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            weights = layer.get_weights()\n",
    "            if weights:\n",
    "                all_weights.append(weights[0].flatten())\n",
    "    \n",
    "    all_weights = np.concatenate(all_weights)\n",
    "    threshold = np.percentile(np.abs(all_weights), sparsity * 100)\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "            weights = layer.get_weights()\n",
    "            if weights:\n",
    "                w, b = weights\n",
    "                mask = np.abs(w) >= threshold\n",
    "                layer.set_weights([w * mask, b])\n",
    "\n",
    "# --------------------------\n",
    "# Load CIFAR-10 Dataset\n",
    "# --------------------------\n",
    "def load_cifar10():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# --------------------------\n",
    "# Training Function\n",
    "# --------------------------\n",
    "def train(model, x_train, y_train, x_test, y_test, epochs, batch_size, checkpoint_path):\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                           monitor='val_accuracy',\n",
    "                                           save_best_only=True,\n",
    "                                           verbose=1)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    return history\n",
    "\n",
    "# --------------------------\n",
    "# Main Training Execution (No Iterations)\n",
    "# --------------------------\n",
    "def run_pruning_training(sparsity=0.5, epochs=150, batch_size=128):\n",
    "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "    model = build_resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    global_magnitude_pruning(model, sparsity)\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    checkpoint_path = f\"results/pruned_resnet20_global_magnitude_sparsity_{sparsity}.h5\"\n",
    "\n",
    "    train(model, x_train, y_train, x_test, y_test, epochs, batch_size, checkpoint_path)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb7e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_18[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "[INFO] Applying global magnitude pruning with sparsity: 0.3\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6519 - accuracy: 0.4609\n",
      "Epoch 1: val_accuracy improved from -inf to 0.45550, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 41s 80ms/step - loss: 1.6519 - accuracy: 0.4609 - val_loss: 1.6756 - val_accuracy: 0.4555\n",
      "Epoch 2/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.1899 - accuracy: 0.6276\n",
      "Epoch 2: val_accuracy improved from 0.45550 to 0.58040, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 1.1900 - accuracy: 0.6276 - val_loss: 1.3251 - val_accuracy: 0.5804\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9982 - accuracy: 0.6984\n",
      "Epoch 3: val_accuracy did not improve from 0.58040\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.9982 - accuracy: 0.6984 - val_loss: 2.0416 - val_accuracy: 0.4942\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8667 - accuracy: 0.7474\n",
      "Epoch 4: val_accuracy improved from 0.58040 to 0.64860, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.8667 - accuracy: 0.7474 - val_loss: 1.1843 - val_accuracy: 0.6486\n",
      "Epoch 5/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7756 - accuracy: 0.7781\n",
      "Epoch 5: val_accuracy improved from 0.64860 to 0.73730, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.7757 - accuracy: 0.7781 - val_loss: 0.9135 - val_accuracy: 0.7373\n",
      "Epoch 6/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7032 - accuracy: 0.8065\n",
      "Epoch 6: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.7032 - accuracy: 0.8066 - val_loss: 1.0421 - val_accuracy: 0.7031\n",
      "Epoch 7/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6431 - accuracy: 0.8275\n",
      "Epoch 7: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.6431 - accuracy: 0.8274 - val_loss: 1.1308 - val_accuracy: 0.6743\n",
      "Epoch 8/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5920 - accuracy: 0.8455\n",
      "Epoch 8: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.5920 - accuracy: 0.8455 - val_loss: 0.9900 - val_accuracy: 0.7341\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.8635\n",
      "Epoch 9: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.5422 - accuracy: 0.8635 - val_loss: 1.2908 - val_accuracy: 0.6680\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5051 - accuracy: 0.8786\n",
      "Epoch 10: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.5050 - accuracy: 0.8786 - val_loss: 1.3271 - val_accuracy: 0.6665\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4649 - accuracy: 0.8902\n",
      "Epoch 11: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.4649 - accuracy: 0.8902 - val_loss: 1.6255 - val_accuracy: 0.6458\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.9032\n",
      "Epoch 12: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.4387 - accuracy: 0.9032 - val_loss: 1.3768 - val_accuracy: 0.6814\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4113 - accuracy: 0.9119\n",
      "Epoch 13: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.4112 - accuracy: 0.9119 - val_loss: 1.3248 - val_accuracy: 0.7008\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.9239\n",
      "Epoch 14: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3819 - accuracy: 0.9239 - val_loss: 1.5191 - val_accuracy: 0.6687\n",
      "Epoch 15/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.9307\n",
      "Epoch 15: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3645 - accuracy: 0.9307 - val_loss: 1.2886 - val_accuracy: 0.7154\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.9356\n",
      "Epoch 16: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3509 - accuracy: 0.9356 - val_loss: 1.2775 - val_accuracy: 0.7147\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.9397\n",
      "Epoch 17: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3430 - accuracy: 0.9397 - val_loss: 1.3364 - val_accuracy: 0.7135\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3209 - accuracy: 0.9473\n",
      "Epoch 18: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3213 - accuracy: 0.9471 - val_loss: 1.4229 - val_accuracy: 0.7067\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9489\n",
      "Epoch 19: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3204 - accuracy: 0.9489 - val_loss: 2.1198 - val_accuracy: 0.6413\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.9515\n",
      "Epoch 20: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3125 - accuracy: 0.9515 - val_loss: 1.2539 - val_accuracy: 0.7237\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.9523\n",
      "Epoch 21: val_accuracy did not improve from 0.73730\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3130 - accuracy: 0.9522 - val_loss: 1.3269 - val_accuracy: 0.7329\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3017 - accuracy: 0.9573\n",
      "Epoch 22: val_accuracy improved from 0.73730 to 0.74430, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3020 - accuracy: 0.9572 - val_loss: 1.3897 - val_accuracy: 0.7443\n",
      "Epoch 23/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.9567\n",
      "Epoch 23: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3014 - accuracy: 0.9566 - val_loss: 1.5093 - val_accuracy: 0.7062\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9616\n",
      "Epoch 24: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.2911 - accuracy: 0.9616 - val_loss: 1.4174 - val_accuracy: 0.7250\n",
      "Epoch 25/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.9612\n",
      "Epoch 25: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2942 - accuracy: 0.9612 - val_loss: 1.8731 - val_accuracy: 0.6746\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.9608\n",
      "Epoch 26: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2940 - accuracy: 0.9608 - val_loss: 1.8026 - val_accuracy: 0.6839\n",
      "Epoch 27/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9630\n",
      "Epoch 27: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2907 - accuracy: 0.9630 - val_loss: 2.1208 - val_accuracy: 0.6642\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9641\n",
      "Epoch 28: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2860 - accuracy: 0.9641 - val_loss: 1.4810 - val_accuracy: 0.7175\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2967 - accuracy: 0.9613\n",
      "Epoch 29: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2967 - accuracy: 0.9613 - val_loss: 1.4077 - val_accuracy: 0.7305\n",
      "Epoch 30/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9647\n",
      "Epoch 30: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2853 - accuracy: 0.9647 - val_loss: 1.9819 - val_accuracy: 0.6780\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9650\n",
      "Epoch 31: val_accuracy did not improve from 0.74430\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2860 - accuracy: 0.9650 - val_loss: 2.2682 - val_accuracy: 0.6423\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9661\n",
      "Epoch 32: val_accuracy improved from 0.74430 to 0.75060, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2845 - accuracy: 0.9661 - val_loss: 1.3062 - val_accuracy: 0.7506\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9667\n",
      "Epoch 33: val_accuracy did not improve from 0.75060\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2811 - accuracy: 0.9667 - val_loss: 1.9288 - val_accuracy: 0.6701\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9662\n",
      "Epoch 34: val_accuracy did not improve from 0.75060\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2853 - accuracy: 0.9662 - val_loss: 1.7411 - val_accuracy: 0.7004\n",
      "Epoch 35/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.9686\n",
      "Epoch 35: val_accuracy did not improve from 0.75060\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2800 - accuracy: 0.9686 - val_loss: 1.4341 - val_accuracy: 0.7445\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9681\n",
      "Epoch 36: val_accuracy did not improve from 0.75060\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2811 - accuracy: 0.9681 - val_loss: 1.7348 - val_accuracy: 0.7134\n",
      "Epoch 37/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9677\n",
      "Epoch 37: val_accuracy did not improve from 0.75060\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2825 - accuracy: 0.9677 - val_loss: 1.4766 - val_accuracy: 0.7364\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9676\n",
      "Epoch 38: val_accuracy improved from 0.75060 to 0.75110, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2834 - accuracy: 0.9676 - val_loss: 1.2999 - val_accuracy: 0.7511\n",
      "Epoch 39/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9723\n",
      "Epoch 39: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2718 - accuracy: 0.9722 - val_loss: 1.6377 - val_accuracy: 0.7103\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.9660\n",
      "Epoch 40: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2887 - accuracy: 0.9660 - val_loss: 1.6550 - val_accuracy: 0.7124\n",
      "Epoch 41/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9711\n",
      "Epoch 41: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2762 - accuracy: 0.9711 - val_loss: 1.4644 - val_accuracy: 0.7327\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.9716\n",
      "Epoch 42: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2737 - accuracy: 0.9716 - val_loss: 1.7023 - val_accuracy: 0.7242\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9711\n",
      "Epoch 43: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2761 - accuracy: 0.9711 - val_loss: 1.4771 - val_accuracy: 0.7371\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9710\n",
      "Epoch 44: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2747 - accuracy: 0.9710 - val_loss: 1.5476 - val_accuracy: 0.7385\n",
      "Epoch 45/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.9704\n",
      "Epoch 45: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2807 - accuracy: 0.9705 - val_loss: 1.6298 - val_accuracy: 0.7131\n",
      "Epoch 46/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2755 - accuracy: 0.9712\n",
      "Epoch 46: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2754 - accuracy: 0.9712 - val_loss: 1.8328 - val_accuracy: 0.6964\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.9743\n",
      "Epoch 47: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2704 - accuracy: 0.9743 - val_loss: 1.7409 - val_accuracy: 0.7123\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.9686\n",
      "Epoch 48: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2820 - accuracy: 0.9686 - val_loss: 1.6094 - val_accuracy: 0.7082\n",
      "Epoch 49/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.9730\n",
      "Epoch 49: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2711 - accuracy: 0.9730 - val_loss: 1.6087 - val_accuracy: 0.7318\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9735\n",
      "Epoch 50: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2703 - accuracy: 0.9735 - val_loss: 1.4733 - val_accuracy: 0.7356\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9724\n",
      "Epoch 51: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2749 - accuracy: 0.9724 - val_loss: 1.7003 - val_accuracy: 0.7274\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.9715\n",
      "Epoch 52: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2759 - accuracy: 0.9715 - val_loss: 1.5754 - val_accuracy: 0.7362\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.9724\n",
      "Epoch 53: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2743 - accuracy: 0.9724 - val_loss: 1.8101 - val_accuracy: 0.7117\n",
      "Epoch 54/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.9746\n",
      "Epoch 54: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2681 - accuracy: 0.9746 - val_loss: 2.1846 - val_accuracy: 0.6767\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9733\n",
      "Epoch 55: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2716 - accuracy: 0.9733 - val_loss: 1.9871 - val_accuracy: 0.7004\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9747\n",
      "Epoch 56: val_accuracy did not improve from 0.75110\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2703 - accuracy: 0.9747 - val_loss: 2.3748 - val_accuracy: 0.6695\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9736\n",
      "Epoch 57: val_accuracy improved from 0.75110 to 0.76270, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2717 - accuracy: 0.9736 - val_loss: 1.3132 - val_accuracy: 0.7627\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9766\n",
      "Epoch 58: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2623 - accuracy: 0.9766 - val_loss: 1.5634 - val_accuracy: 0.7301\n",
      "Epoch 59/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2734 - accuracy: 0.9725\n",
      "Epoch 59: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2735 - accuracy: 0.9725 - val_loss: 2.0175 - val_accuracy: 0.7169\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9760\n",
      "Epoch 60: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2659 - accuracy: 0.9760 - val_loss: 1.5039 - val_accuracy: 0.7544\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2805 - accuracy: 0.9703\n",
      "Epoch 61: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2806 - accuracy: 0.9703 - val_loss: 1.7207 - val_accuracy: 0.7188\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9731\n",
      "Epoch 62: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2744 - accuracy: 0.9731 - val_loss: 2.0399 - val_accuracy: 0.6907\n",
      "Epoch 63/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.9767\n",
      "Epoch 63: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2628 - accuracy: 0.9767 - val_loss: 1.7163 - val_accuracy: 0.7263\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9780\n",
      "Epoch 64: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2588 - accuracy: 0.9780 - val_loss: 1.4874 - val_accuracy: 0.7484\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9731\n",
      "Epoch 65: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2713 - accuracy: 0.9731 - val_loss: 1.4468 - val_accuracy: 0.7604\n",
      "Epoch 66/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2667 - accuracy: 0.9740\n",
      "Epoch 66: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2667 - accuracy: 0.9740 - val_loss: 1.7355 - val_accuracy: 0.7306\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.9743\n",
      "Epoch 67: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2697 - accuracy: 0.9743 - val_loss: 2.3273 - val_accuracy: 0.6622\n",
      "Epoch 68/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.9759\n",
      "Epoch 68: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2645 - accuracy: 0.9759 - val_loss: 1.6366 - val_accuracy: 0.7527\n",
      "Epoch 69/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9775\n",
      "Epoch 69: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2612 - accuracy: 0.9775 - val_loss: 1.4148 - val_accuracy: 0.7492\n",
      "Epoch 70/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.9769\n",
      "Epoch 70: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2606 - accuracy: 0.9769 - val_loss: 2.4889 - val_accuracy: 0.6493\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9728\n",
      "Epoch 71: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2722 - accuracy: 0.9728 - val_loss: 1.4540 - val_accuracy: 0.7465\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9767\n",
      "Epoch 72: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2608 - accuracy: 0.9767 - val_loss: 1.6074 - val_accuracy: 0.7383\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9756\n",
      "Epoch 73: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2642 - accuracy: 0.9756 - val_loss: 1.6922 - val_accuracy: 0.7354\n",
      "Epoch 74/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9760\n",
      "Epoch 74: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2649 - accuracy: 0.9759 - val_loss: 1.7929 - val_accuracy: 0.7286\n",
      "Epoch 75/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.9743\n",
      "Epoch 75: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2696 - accuracy: 0.9743 - val_loss: 1.7277 - val_accuracy: 0.7355\n",
      "Epoch 76/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9768\n",
      "Epoch 76: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2613 - accuracy: 0.9767 - val_loss: 1.9204 - val_accuracy: 0.7038\n",
      "Epoch 77/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9757\n",
      "Epoch 77: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2647 - accuracy: 0.9757 - val_loss: 2.1856 - val_accuracy: 0.6954\n",
      "Epoch 78/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9756\n",
      "Epoch 78: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2639 - accuracy: 0.9756 - val_loss: 1.6582 - val_accuracy: 0.7268\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9779\n",
      "Epoch 79: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2578 - accuracy: 0.9779 - val_loss: 1.6209 - val_accuracy: 0.7385\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9766\n",
      "Epoch 80: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2626 - accuracy: 0.9766 - val_loss: 1.5227 - val_accuracy: 0.7548\n",
      "Epoch 81/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.9751\n",
      "Epoch 81: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2653 - accuracy: 0.9751 - val_loss: 1.7707 - val_accuracy: 0.7381\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.9758\n",
      "Epoch 82: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2631 - accuracy: 0.9758 - val_loss: 1.5172 - val_accuracy: 0.7385\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9763\n",
      "Epoch 83: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2620 - accuracy: 0.9763 - val_loss: 1.8709 - val_accuracy: 0.7123\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9783\n",
      "Epoch 84: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2578 - accuracy: 0.9783 - val_loss: 1.6278 - val_accuracy: 0.7356\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9789\n",
      "Epoch 85: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2537 - accuracy: 0.9789 - val_loss: 1.7237 - val_accuracy: 0.7340\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9770\n",
      "Epoch 86: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2600 - accuracy: 0.9770 - val_loss: 1.6641 - val_accuracy: 0.7373\n",
      "Epoch 87/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.9733\n",
      "Epoch 87: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2711 - accuracy: 0.9733 - val_loss: 1.6903 - val_accuracy: 0.7237\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9803\n",
      "Epoch 88: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2514 - accuracy: 0.9803 - val_loss: 1.4412 - val_accuracy: 0.7585\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9796\n",
      "Epoch 89: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2516 - accuracy: 0.9796 - val_loss: 1.9957 - val_accuracy: 0.7125\n",
      "Epoch 90/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2570 - accuracy: 0.9774\n",
      "Epoch 90: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2574 - accuracy: 0.9773 - val_loss: 1.6661 - val_accuracy: 0.7435\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9739\n",
      "Epoch 91: val_accuracy did not improve from 0.76270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2681 - accuracy: 0.9739 - val_loss: 1.5068 - val_accuracy: 0.7376\n",
      "Epoch 92/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9784\n",
      "Epoch 92: val_accuracy improved from 0.76270 to 0.76420, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2551 - accuracy: 0.9783 - val_loss: 1.3760 - val_accuracy: 0.7642\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9799\n",
      "Epoch 93: val_accuracy did not improve from 0.76420\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2492 - accuracy: 0.9799 - val_loss: 1.7834 - val_accuracy: 0.7285\n",
      "Epoch 94/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9752\n",
      "Epoch 94: val_accuracy did not improve from 0.76420\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2633 - accuracy: 0.9752 - val_loss: 1.8114 - val_accuracy: 0.7251\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9772\n",
      "Epoch 95: val_accuracy did not improve from 0.76420\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2554 - accuracy: 0.9772 - val_loss: 2.0438 - val_accuracy: 0.7119\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9755\n",
      "Epoch 96: val_accuracy did not improve from 0.76420\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2626 - accuracy: 0.9755 - val_loss: 1.6089 - val_accuracy: 0.7452\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9775\n",
      "Epoch 97: val_accuracy improved from 0.76420 to 0.77560, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2570 - accuracy: 0.9775 - val_loss: 1.3088 - val_accuracy: 0.7756\n",
      "Epoch 98/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9795\n",
      "Epoch 98: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2508 - accuracy: 0.9795 - val_loss: 1.5441 - val_accuracy: 0.7418\n",
      "Epoch 99/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2545 - accuracy: 0.9789\n",
      "Epoch 99: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2547 - accuracy: 0.9788 - val_loss: 1.5904 - val_accuracy: 0.7465\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9773\n",
      "Epoch 100: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2588 - accuracy: 0.9773 - val_loss: 1.7340 - val_accuracy: 0.7332\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9794\n",
      "Epoch 101: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2505 - accuracy: 0.9794 - val_loss: 1.9287 - val_accuracy: 0.7139\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.9790\n",
      "Epoch 102: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2530 - accuracy: 0.9790 - val_loss: 2.0155 - val_accuracy: 0.7004\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9778\n",
      "Epoch 103: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2555 - accuracy: 0.9778 - val_loss: 1.9940 - val_accuracy: 0.7082\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9758\n",
      "Epoch 104: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2604 - accuracy: 0.9758 - val_loss: 1.5172 - val_accuracy: 0.7598\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9783\n",
      "Epoch 105: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2545 - accuracy: 0.9783 - val_loss: 1.5225 - val_accuracy: 0.7667\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9797\n",
      "Epoch 106: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2477 - accuracy: 0.9797 - val_loss: 1.7752 - val_accuracy: 0.7245\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9770\n",
      "Epoch 107: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2550 - accuracy: 0.9770 - val_loss: 1.9644 - val_accuracy: 0.7085\n",
      "Epoch 108/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.9777\n",
      "Epoch 108: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2528 - accuracy: 0.9777 - val_loss: 1.4200 - val_accuracy: 0.7629\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2598 - accuracy: 0.9762\n",
      "Epoch 109: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2598 - accuracy: 0.9762 - val_loss: 1.4759 - val_accuracy: 0.7601\n",
      "Epoch 110/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9810\n",
      "Epoch 110: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2461 - accuracy: 0.9810 - val_loss: 1.7351 - val_accuracy: 0.7335\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9767\n",
      "Epoch 111: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2560 - accuracy: 0.9767 - val_loss: 1.4932 - val_accuracy: 0.7458\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9777\n",
      "Epoch 112: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2535 - accuracy: 0.9777 - val_loss: 1.7539 - val_accuracy: 0.7173\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9794\n",
      "Epoch 113: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2491 - accuracy: 0.9794 - val_loss: 1.8569 - val_accuracy: 0.7393\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9785\n",
      "Epoch 114: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 30s 75ms/step - loss: 0.2532 - accuracy: 0.9785 - val_loss: 1.3456 - val_accuracy: 0.7648\n",
      "Epoch 115/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9798\n",
      "Epoch 115: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2493 - accuracy: 0.9798 - val_loss: 1.8211 - val_accuracy: 0.7167\n",
      "Epoch 116/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9779\n",
      "Epoch 116: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2532 - accuracy: 0.9779 - val_loss: 1.4528 - val_accuracy: 0.7488\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9798\n",
      "Epoch 117: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2465 - accuracy: 0.9798 - val_loss: 1.5380 - val_accuracy: 0.7498\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9776\n",
      "Epoch 118: val_accuracy did not improve from 0.77560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2545 - accuracy: 0.9776 - val_loss: 1.7416 - val_accuracy: 0.7102\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9794\n",
      "Epoch 119: val_accuracy improved from 0.77560 to 0.78550, saving model to results\\pruned_resnet20_global_magnitude_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2485 - accuracy: 0.9794 - val_loss: 1.3180 - val_accuracy: 0.7855\n",
      "Epoch 120/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9808\n",
      "Epoch 120: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2437 - accuracy: 0.9808 - val_loss: 1.5250 - val_accuracy: 0.7566\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9756\n",
      "Epoch 121: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2567 - accuracy: 0.9756 - val_loss: 1.5149 - val_accuracy: 0.7411\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9818\n",
      "Epoch 122: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2436 - accuracy: 0.9818 - val_loss: 1.7105 - val_accuracy: 0.7463\n",
      "Epoch 123/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9799\n",
      "Epoch 123: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2461 - accuracy: 0.9799 - val_loss: 1.6456 - val_accuracy: 0.7296\n",
      "Epoch 124/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9755\n",
      "Epoch 124: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2590 - accuracy: 0.9754 - val_loss: 1.5904 - val_accuracy: 0.7414\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9796\n",
      "Epoch 125: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2485 - accuracy: 0.9796 - val_loss: 1.5704 - val_accuracy: 0.7495\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9807\n",
      "Epoch 126: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2444 - accuracy: 0.9807 - val_loss: 1.2848 - val_accuracy: 0.7786\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9791\n",
      "Epoch 127: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2461 - accuracy: 0.9791 - val_loss: 2.2042 - val_accuracy: 0.6867\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9790\n",
      "Epoch 128: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2472 - accuracy: 0.9790 - val_loss: 2.0802 - val_accuracy: 0.6850\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9796\n",
      "Epoch 129: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2464 - accuracy: 0.9796 - val_loss: 1.5545 - val_accuracy: 0.7499\n",
      "Epoch 130/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9802\n",
      "Epoch 130: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2420 - accuracy: 0.9802 - val_loss: 1.4350 - val_accuracy: 0.7643\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9776\n",
      "Epoch 131: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2544 - accuracy: 0.9776 - val_loss: 1.7595 - val_accuracy: 0.7385\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9805\n",
      "Epoch 132: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2461 - accuracy: 0.9805 - val_loss: 1.5460 - val_accuracy: 0.7441\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9808\n",
      "Epoch 133: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2430 - accuracy: 0.9808 - val_loss: 1.5450 - val_accuracy: 0.7457\n",
      "Epoch 134/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9804\n",
      "Epoch 134: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2422 - accuracy: 0.9804 - val_loss: 1.8345 - val_accuracy: 0.7440\n",
      "Epoch 135/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9783\n",
      "Epoch 135: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2482 - accuracy: 0.9782 - val_loss: 1.6919 - val_accuracy: 0.7473\n",
      "Epoch 136/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9793\n",
      "Epoch 136: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2470 - accuracy: 0.9792 - val_loss: 1.6170 - val_accuracy: 0.7387\n",
      "Epoch 137/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9810\n",
      "Epoch 137: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2409 - accuracy: 0.9810 - val_loss: 1.2340 - val_accuracy: 0.7840\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9808\n",
      "Epoch 138: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2415 - accuracy: 0.9808 - val_loss: 1.6501 - val_accuracy: 0.7433\n",
      "Epoch 139/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9814\n",
      "Epoch 139: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2390 - accuracy: 0.9814 - val_loss: 2.0218 - val_accuracy: 0.7029\n",
      "Epoch 140/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9773\n",
      "Epoch 140: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2510 - accuracy: 0.9773 - val_loss: 1.4596 - val_accuracy: 0.7732\n",
      "Epoch 141/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.9777\n",
      "Epoch 141: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2500 - accuracy: 0.9777 - val_loss: 1.6939 - val_accuracy: 0.7435\n",
      "Epoch 142/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9823\n",
      "Epoch 142: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2369 - accuracy: 0.9824 - val_loss: 1.4787 - val_accuracy: 0.7539\n",
      "Epoch 143/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9797\n",
      "Epoch 143: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2428 - accuracy: 0.9797 - val_loss: 1.6281 - val_accuracy: 0.7519\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9800\n",
      "Epoch 144: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2416 - accuracy: 0.9800 - val_loss: 1.5276 - val_accuracy: 0.7659\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9792\n",
      "Epoch 145: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2450 - accuracy: 0.9792 - val_loss: 2.0195 - val_accuracy: 0.6981\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.9799\n",
      "Epoch 146: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2418 - accuracy: 0.9799 - val_loss: 1.3003 - val_accuracy: 0.7710\n",
      "Epoch 147/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9796\n",
      "Epoch 147: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2431 - accuracy: 0.9796 - val_loss: 1.3599 - val_accuracy: 0.7746\n",
      "Epoch 148/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9800\n",
      "Epoch 148: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2424 - accuracy: 0.9800 - val_loss: 1.4408 - val_accuracy: 0.7654\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9809\n",
      "Epoch 149: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2403 - accuracy: 0.9809 - val_loss: 1.3888 - val_accuracy: 0.7772\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2383 - accuracy: 0.9803\n",
      "Epoch 150: val_accuracy did not improve from 0.78550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2383 - accuracy: 0.9803 - val_loss: 2.0741 - val_accuracy: 0.7311\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Run the code\n",
    "# --------------------------\n",
    "run_pruning_training(sparsity=0.3, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5510f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22360b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4e8cfc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ae43da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36d32d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 16)  64          ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 16)  64          ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 32, 32, 16)   0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 32, 32, 16)   0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32, 32, 16)  64          ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 32, 32, 16)   0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 32, 32, 16)   0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, 32, 16)  64          ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 32, 32, 16)   0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 16)   0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 16, 16, 32)   544         ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 16, 16, 32)   0           ['conv2d_51[0][0]',              \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 32)   0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 32)  128         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 16, 16, 32)   0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 32)   0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 32)  128         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 16, 16, 32)  128         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 16, 16, 32)   0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 16, 16, 32)   0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['conv2d_58[0][0]',              \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_54[0][0]',          \n",
      "                                                                  'batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 64)          0           ['activation_56[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           650         ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "[INFO] Applying global magnitude pruning with sparsity: 0.5\n",
      "Epoch 1/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.6203 - accuracy: 0.4710\n",
      "Epoch 1: val_accuracy improved from -inf to 0.40030, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 32s 75ms/step - loss: 1.6201 - accuracy: 0.4711 - val_loss: 2.0136 - val_accuracy: 0.4003\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1770 - accuracy: 0.6286\n",
      "Epoch 2: val_accuracy improved from 0.40030 to 0.60920, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.1770 - accuracy: 0.6286 - val_loss: 1.2547 - val_accuracy: 0.6092\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9864 - accuracy: 0.7023\n",
      "Epoch 3: val_accuracy did not improve from 0.60920\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.9864 - accuracy: 0.7023 - val_loss: 1.4432 - val_accuracy: 0.5673\n",
      "Epoch 4/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8604 - accuracy: 0.7459\n",
      "Epoch 4: val_accuracy improved from 0.60920 to 0.61800, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.8601 - accuracy: 0.7460 - val_loss: 1.3153 - val_accuracy: 0.6180\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7711 - accuracy: 0.7791\n",
      "Epoch 5: val_accuracy improved from 0.61800 to 0.62740, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.7711 - accuracy: 0.7791 - val_loss: 1.2744 - val_accuracy: 0.6274\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.8049\n",
      "Epoch 6: val_accuracy improved from 0.62740 to 0.68870, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6951 - accuracy: 0.8049 - val_loss: 1.0960 - val_accuracy: 0.6887\n",
      "Epoch 7/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6375 - accuracy: 0.8270\n",
      "Epoch 7: val_accuracy improved from 0.68870 to 0.72280, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6375 - accuracy: 0.8270 - val_loss: 0.9953 - val_accuracy: 0.7228\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.8481\n",
      "Epoch 8: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.5874 - accuracy: 0.8481 - val_loss: 1.1869 - val_accuracy: 0.6882\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5374 - accuracy: 0.8641\n",
      "Epoch 9: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.5374 - accuracy: 0.8641 - val_loss: 1.1148 - val_accuracy: 0.7105\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.8803\n",
      "Epoch 10: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.4941 - accuracy: 0.8804 - val_loss: 1.2217 - val_accuracy: 0.6896\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.8917\n",
      "Epoch 11: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.4616 - accuracy: 0.8917 - val_loss: 1.7090 - val_accuracy: 0.6315\n",
      "Epoch 12/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.9072\n",
      "Epoch 12: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.4221 - accuracy: 0.9072 - val_loss: 1.3757 - val_accuracy: 0.6584\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4001 - accuracy: 0.9164\n",
      "Epoch 13: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4001 - accuracy: 0.9164 - val_loss: 2.0514 - val_accuracy: 0.5766\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.9273\n",
      "Epoch 14: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3681 - accuracy: 0.9273 - val_loss: 1.2828 - val_accuracy: 0.7080\n",
      "Epoch 15/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.9339\n",
      "Epoch 15: val_accuracy did not improve from 0.72280\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3537 - accuracy: 0.9339 - val_loss: 1.5396 - val_accuracy: 0.6817\n",
      "Epoch 16/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9400\n",
      "Epoch 16: val_accuracy improved from 0.72280 to 0.72720, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3400 - accuracy: 0.9400 - val_loss: 1.2406 - val_accuracy: 0.7272\n",
      "Epoch 17/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.9437\n",
      "Epoch 17: val_accuracy did not improve from 0.72720\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3304 - accuracy: 0.9437 - val_loss: 1.9093 - val_accuracy: 0.6480\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.9518\n",
      "Epoch 18: val_accuracy did not improve from 0.72720\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3087 - accuracy: 0.9518 - val_loss: 1.3436 - val_accuracy: 0.7196\n",
      "Epoch 19/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9533\n",
      "Epoch 19: val_accuracy did not improve from 0.72720\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3056 - accuracy: 0.9533 - val_loss: 1.9586 - val_accuracy: 0.6514\n",
      "Epoch 20/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3134 - accuracy: 0.9513\n",
      "Epoch 20: val_accuracy improved from 0.72720 to 0.76690, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3132 - accuracy: 0.9513 - val_loss: 1.0975 - val_accuracy: 0.7669\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.9554\n",
      "Epoch 21: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3032 - accuracy: 0.9554 - val_loss: 1.7149 - val_accuracy: 0.6727\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9585\n",
      "Epoch 22: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2968 - accuracy: 0.9585 - val_loss: 1.5751 - val_accuracy: 0.6909\n",
      "Epoch 23/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9595\n",
      "Epoch 23: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2913 - accuracy: 0.9595 - val_loss: 1.3571 - val_accuracy: 0.7232\n",
      "Epoch 24/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2883 - accuracy: 0.9624\n",
      "Epoch 24: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2882 - accuracy: 0.9624 - val_loss: 1.3278 - val_accuracy: 0.7448\n",
      "Epoch 25/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.9595\n",
      "Epoch 25: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2949 - accuracy: 0.9595 - val_loss: 1.4312 - val_accuracy: 0.7262\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.9614\n",
      "Epoch 26: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2896 - accuracy: 0.9614 - val_loss: 1.4323 - val_accuracy: 0.7342\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9627\n",
      "Epoch 27: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2903 - accuracy: 0.9627 - val_loss: 2.0888 - val_accuracy: 0.6629\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9667\n",
      "Epoch 28: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2821 - accuracy: 0.9666 - val_loss: 1.5820 - val_accuracy: 0.7136\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9641\n",
      "Epoch 29: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2862 - accuracy: 0.9641 - val_loss: 1.4729 - val_accuracy: 0.7380\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9644\n",
      "Epoch 30: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2876 - accuracy: 0.9644 - val_loss: 1.5259 - val_accuracy: 0.7129\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2805 - accuracy: 0.9674\n",
      "Epoch 31: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2805 - accuracy: 0.9674 - val_loss: 1.6726 - val_accuracy: 0.7110\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9663\n",
      "Epoch 32: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2850 - accuracy: 0.9663 - val_loss: 1.9328 - val_accuracy: 0.6783\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2801 - accuracy: 0.9677\n",
      "Epoch 33: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2801 - accuracy: 0.9677 - val_loss: 1.3529 - val_accuracy: 0.7523\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9668\n",
      "Epoch 34: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2838 - accuracy: 0.9668 - val_loss: 1.5344 - val_accuracy: 0.7169\n",
      "Epoch 35/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.9714\n",
      "Epoch 35: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2733 - accuracy: 0.9714 - val_loss: 1.6730 - val_accuracy: 0.7259\n",
      "Epoch 36/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.9681\n",
      "Epoch 36: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2827 - accuracy: 0.9681 - val_loss: 1.4790 - val_accuracy: 0.7358\n",
      "Epoch 37/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.9692\n",
      "Epoch 37: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2812 - accuracy: 0.9692 - val_loss: 2.1880 - val_accuracy: 0.6573\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9726\n",
      "Epoch 38: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2714 - accuracy: 0.9726 - val_loss: 1.6513 - val_accuracy: 0.7154\n",
      "Epoch 39/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2781 - accuracy: 0.9697\n",
      "Epoch 39: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2782 - accuracy: 0.9697 - val_loss: 1.5126 - val_accuracy: 0.7345\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.9693\n",
      "Epoch 40: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2803 - accuracy: 0.9693 - val_loss: 2.1924 - val_accuracy: 0.6843\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9722\n",
      "Epoch 41: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2720 - accuracy: 0.9722 - val_loss: 1.4841 - val_accuracy: 0.7326\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9706\n",
      "Epoch 42: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2750 - accuracy: 0.9705 - val_loss: 1.8925 - val_accuracy: 0.7095\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9698\n",
      "Epoch 43: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2797 - accuracy: 0.9698 - val_loss: 1.4328 - val_accuracy: 0.7380\n",
      "Epoch 44/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9758\n",
      "Epoch 44: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2648 - accuracy: 0.9758 - val_loss: 1.7740 - val_accuracy: 0.7229\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9691\n",
      "Epoch 45: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2818 - accuracy: 0.9691 - val_loss: 2.7172 - val_accuracy: 0.6466\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9695\n",
      "Epoch 46: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2848 - accuracy: 0.9695 - val_loss: 2.0872 - val_accuracy: 0.7004\n",
      "Epoch 47/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.9746\n",
      "Epoch 47: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2673 - accuracy: 0.9745 - val_loss: 1.6372 - val_accuracy: 0.7206\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9738\n",
      "Epoch 48: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2700 - accuracy: 0.9738 - val_loss: 1.4281 - val_accuracy: 0.7484\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.9758\n",
      "Epoch 49: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2648 - accuracy: 0.9758 - val_loss: 1.8008 - val_accuracy: 0.7099\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2882 - accuracy: 0.9670\n",
      "Epoch 50: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2882 - accuracy: 0.9670 - val_loss: 1.8511 - val_accuracy: 0.6882\n",
      "Epoch 51/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2668 - accuracy: 0.9756\n",
      "Epoch 51: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2669 - accuracy: 0.9755 - val_loss: 1.4321 - val_accuracy: 0.7421\n",
      "Epoch 52/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9714\n",
      "Epoch 52: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2766 - accuracy: 0.9714 - val_loss: 1.5789 - val_accuracy: 0.7404\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9745\n",
      "Epoch 53: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2708 - accuracy: 0.9745 - val_loss: 1.7228 - val_accuracy: 0.7182\n",
      "Epoch 54/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9738\n",
      "Epoch 54: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2698 - accuracy: 0.9737 - val_loss: 1.7744 - val_accuracy: 0.7175\n",
      "Epoch 55/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.9744\n",
      "Epoch 55: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2684 - accuracy: 0.9744 - val_loss: 1.4507 - val_accuracy: 0.7549\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9734\n",
      "Epoch 56: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2731 - accuracy: 0.9734 - val_loss: 1.5138 - val_accuracy: 0.7476\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9754\n",
      "Epoch 57: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2677 - accuracy: 0.9754 - val_loss: 2.3778 - val_accuracy: 0.6533\n",
      "Epoch 58/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9746\n",
      "Epoch 58: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2698 - accuracy: 0.9746 - val_loss: 1.7912 - val_accuracy: 0.7073\n",
      "Epoch 59/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.9745\n",
      "Epoch 59: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2711 - accuracy: 0.9746 - val_loss: 1.5807 - val_accuracy: 0.7324\n",
      "Epoch 60/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.9746\n",
      "Epoch 60: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2698 - accuracy: 0.9746 - val_loss: 1.5815 - val_accuracy: 0.7347\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9736\n",
      "Epoch 61: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2719 - accuracy: 0.9736 - val_loss: 1.7155 - val_accuracy: 0.7370\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9763\n",
      "Epoch 62: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2645 - accuracy: 0.9763 - val_loss: 1.9594 - val_accuracy: 0.7037\n",
      "Epoch 63/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9738\n",
      "Epoch 63: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2709 - accuracy: 0.9737 - val_loss: 1.6358 - val_accuracy: 0.7372\n",
      "Epoch 64/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2649 - accuracy: 0.9756\n",
      "Epoch 64: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2649 - accuracy: 0.9756 - val_loss: 1.7044 - val_accuracy: 0.7257\n",
      "Epoch 65/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9754\n",
      "Epoch 65: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2676 - accuracy: 0.9754 - val_loss: 1.6989 - val_accuracy: 0.7180\n",
      "Epoch 66/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9777\n",
      "Epoch 66: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2599 - accuracy: 0.9777 - val_loss: 2.1525 - val_accuracy: 0.6977\n",
      "Epoch 67/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9740\n",
      "Epoch 67: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2710 - accuracy: 0.9739 - val_loss: 1.7759 - val_accuracy: 0.7270\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9742\n",
      "Epoch 68: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2702 - accuracy: 0.9742 - val_loss: 2.0193 - val_accuracy: 0.6827\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9767\n",
      "Epoch 69: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2605 - accuracy: 0.9767 - val_loss: 1.6246 - val_accuracy: 0.7242\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9773\n",
      "Epoch 70: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2606 - accuracy: 0.9773 - val_loss: 1.8434 - val_accuracy: 0.7143\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9772\n",
      "Epoch 71: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2620 - accuracy: 0.9772 - val_loss: 1.8395 - val_accuracy: 0.6941\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9750\n",
      "Epoch 72: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2670 - accuracy: 0.9750 - val_loss: 1.5747 - val_accuracy: 0.7416\n",
      "Epoch 73/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.9767\n",
      "Epoch 73: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2639 - accuracy: 0.9767 - val_loss: 1.4895 - val_accuracy: 0.7541\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9761\n",
      "Epoch 74: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2634 - accuracy: 0.9761 - val_loss: 1.5856 - val_accuracy: 0.7343\n",
      "Epoch 75/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2632 - accuracy: 0.9763\n",
      "Epoch 75: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2632 - accuracy: 0.9763 - val_loss: 1.8994 - val_accuracy: 0.7024\n",
      "Epoch 76/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.9757\n",
      "Epoch 76: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2662 - accuracy: 0.9757 - val_loss: 1.5728 - val_accuracy: 0.7440\n",
      "Epoch 77/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.9767\n",
      "Epoch 77: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2622 - accuracy: 0.9766 - val_loss: 1.7214 - val_accuracy: 0.7304\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9773\n",
      "Epoch 78: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2630 - accuracy: 0.9773 - val_loss: 1.6933 - val_accuracy: 0.7259\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9762\n",
      "Epoch 79: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2645 - accuracy: 0.9762 - val_loss: 1.4397 - val_accuracy: 0.7528\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.9760\n",
      "Epoch 80: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2650 - accuracy: 0.9760 - val_loss: 1.6626 - val_accuracy: 0.7373\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9786\n",
      "Epoch 81: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2570 - accuracy: 0.9786 - val_loss: 1.4734 - val_accuracy: 0.7474\n",
      "Epoch 82/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9750\n",
      "Epoch 82: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2671 - accuracy: 0.9750 - val_loss: 1.6898 - val_accuracy: 0.7388\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9783\n",
      "Epoch 83: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2579 - accuracy: 0.9783 - val_loss: 1.7882 - val_accuracy: 0.7373\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.9778\n",
      "Epoch 84: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2583 - accuracy: 0.9778 - val_loss: 2.1310 - val_accuracy: 0.6825\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9764\n",
      "Epoch 85: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2610 - accuracy: 0.9764 - val_loss: 2.0258 - val_accuracy: 0.7143\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9770\n",
      "Epoch 86: val_accuracy did not improve from 0.76690\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2625 - accuracy: 0.9770 - val_loss: 1.7414 - val_accuracy: 0.7259\n",
      "Epoch 87/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9799\n",
      "Epoch 87: val_accuracy improved from 0.76690 to 0.77170, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2522 - accuracy: 0.9799 - val_loss: 1.4276 - val_accuracy: 0.7717\n",
      "Epoch 88/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9763\n",
      "Epoch 88: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2601 - accuracy: 0.9763 - val_loss: 1.6565 - val_accuracy: 0.7311\n",
      "Epoch 89/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2559 - accuracy: 0.9776\n",
      "Epoch 89: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2559 - accuracy: 0.9776 - val_loss: 1.9196 - val_accuracy: 0.7172\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9758\n",
      "Epoch 90: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2619 - accuracy: 0.9758 - val_loss: 2.3889 - val_accuracy: 0.6601\n",
      "Epoch 91/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.9768\n",
      "Epoch 91: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2594 - accuracy: 0.9768 - val_loss: 1.4970 - val_accuracy: 0.7481\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9775\n",
      "Epoch 92: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2568 - accuracy: 0.9775 - val_loss: 1.7416 - val_accuracy: 0.7184\n",
      "Epoch 93/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.9798\n",
      "Epoch 93: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2504 - accuracy: 0.9798 - val_loss: 2.2408 - val_accuracy: 0.6945\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9760\n",
      "Epoch 94: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2596 - accuracy: 0.9760 - val_loss: 1.8997 - val_accuracy: 0.7055\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9749\n",
      "Epoch 95: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2657 - accuracy: 0.9749 - val_loss: 1.8163 - val_accuracy: 0.7284\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9791\n",
      "Epoch 96: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2543 - accuracy: 0.9791 - val_loss: 1.6176 - val_accuracy: 0.7501\n",
      "Epoch 97/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.9804\n",
      "Epoch 97: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2477 - accuracy: 0.9804 - val_loss: 1.5937 - val_accuracy: 0.7349\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9787\n",
      "Epoch 98: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2525 - accuracy: 0.9787 - val_loss: 1.5030 - val_accuracy: 0.7572\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9766\n",
      "Epoch 99: val_accuracy did not improve from 0.77170\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2571 - accuracy: 0.9766 - val_loss: 1.8346 - val_accuracy: 0.7288\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9799\n",
      "Epoch 100: val_accuracy improved from 0.77170 to 0.77690, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2495 - accuracy: 0.9799 - val_loss: 1.4133 - val_accuracy: 0.7769\n",
      "Epoch 101/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2574 - accuracy: 0.9765\n",
      "Epoch 101: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2575 - accuracy: 0.9764 - val_loss: 1.9201 - val_accuracy: 0.6991\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9794\n",
      "Epoch 102: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2509 - accuracy: 0.9794 - val_loss: 1.6977 - val_accuracy: 0.7395\n",
      "Epoch 103/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9765\n",
      "Epoch 103: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2566 - accuracy: 0.9765 - val_loss: 1.5376 - val_accuracy: 0.7349\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9781\n",
      "Epoch 104: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2548 - accuracy: 0.9781 - val_loss: 1.8368 - val_accuracy: 0.7302\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9788\n",
      "Epoch 105: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2521 - accuracy: 0.9788 - val_loss: 2.0003 - val_accuracy: 0.6996\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9789\n",
      "Epoch 106: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2508 - accuracy: 0.9789 - val_loss: 1.5212 - val_accuracy: 0.7704\n",
      "Epoch 107/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9790\n",
      "Epoch 107: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2519 - accuracy: 0.9791 - val_loss: 1.6746 - val_accuracy: 0.7439\n",
      "Epoch 108/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9764\n",
      "Epoch 108: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2582 - accuracy: 0.9764 - val_loss: 2.0724 - val_accuracy: 0.7124\n",
      "Epoch 109/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2549 - accuracy: 0.9773\n",
      "Epoch 109: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2549 - accuracy: 0.9772 - val_loss: 1.5994 - val_accuracy: 0.7330\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9797\n",
      "Epoch 110: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2506 - accuracy: 0.9797 - val_loss: 2.0458 - val_accuracy: 0.6973\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2413 - accuracy: 0.9824\n",
      "Epoch 111: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2413 - accuracy: 0.9824 - val_loss: 1.8456 - val_accuracy: 0.7328\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9771\n",
      "Epoch 112: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2567 - accuracy: 0.9771 - val_loss: 1.9554 - val_accuracy: 0.7094\n",
      "Epoch 113/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.9789\n",
      "Epoch 113: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2501 - accuracy: 0.9788 - val_loss: 2.0136 - val_accuracy: 0.7097\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9766\n",
      "Epoch 114: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2551 - accuracy: 0.9766 - val_loss: 1.3968 - val_accuracy: 0.7680\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9813\n",
      "Epoch 115: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2447 - accuracy: 0.9813 - val_loss: 1.4511 - val_accuracy: 0.7548\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9802\n",
      "Epoch 116: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2450 - accuracy: 0.9802 - val_loss: 2.1980 - val_accuracy: 0.6915\n",
      "Epoch 117/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2556 - accuracy: 0.9769\n",
      "Epoch 117: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2557 - accuracy: 0.9769 - val_loss: 2.5066 - val_accuracy: 0.6820\n",
      "Epoch 118/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.9792\n",
      "Epoch 118: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2511 - accuracy: 0.9792 - val_loss: 2.1135 - val_accuracy: 0.7048\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.9832\n",
      "Epoch 119: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2380 - accuracy: 0.9832 - val_loss: 1.8985 - val_accuracy: 0.7294\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9774\n",
      "Epoch 120: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2537 - accuracy: 0.9774 - val_loss: 1.5429 - val_accuracy: 0.7535\n",
      "Epoch 121/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9767\n",
      "Epoch 121: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2557 - accuracy: 0.9767 - val_loss: 1.9308 - val_accuracy: 0.7047\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9812\n",
      "Epoch 122: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2421 - accuracy: 0.9812 - val_loss: 1.6016 - val_accuracy: 0.7515\n",
      "Epoch 123/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9782\n",
      "Epoch 123: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2522 - accuracy: 0.9781 - val_loss: 1.8631 - val_accuracy: 0.7127\n",
      "Epoch 124/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.9773\n",
      "Epoch 124: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2546 - accuracy: 0.9772 - val_loss: 2.3004 - val_accuracy: 0.6880\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9813\n",
      "Epoch 125: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2409 - accuracy: 0.9813 - val_loss: 1.9966 - val_accuracy: 0.7213\n",
      "Epoch 126/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9771\n",
      "Epoch 126: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2533 - accuracy: 0.9771 - val_loss: 1.5100 - val_accuracy: 0.7629\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9789\n",
      "Epoch 127: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2484 - accuracy: 0.9789 - val_loss: 1.7156 - val_accuracy: 0.7374\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2457 - accuracy: 0.9805\n",
      "Epoch 128: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2457 - accuracy: 0.9805 - val_loss: 1.7999 - val_accuracy: 0.7305\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9777\n",
      "Epoch 129: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2521 - accuracy: 0.9777 - val_loss: 2.2039 - val_accuracy: 0.6988\n",
      "Epoch 130/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9817\n",
      "Epoch 130: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2411 - accuracy: 0.9816 - val_loss: 1.5399 - val_accuracy: 0.7552\n",
      "Epoch 131/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9779\n",
      "Epoch 131: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2525 - accuracy: 0.9779 - val_loss: 1.5961 - val_accuracy: 0.7339\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9819\n",
      "Epoch 132: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2396 - accuracy: 0.9819 - val_loss: 1.7748 - val_accuracy: 0.7394\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9793\n",
      "Epoch 133: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2469 - accuracy: 0.9793 - val_loss: 1.3831 - val_accuracy: 0.7729\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9748\n",
      "Epoch 134: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2559 - accuracy: 0.9748 - val_loss: 1.4806 - val_accuracy: 0.7546\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.9821\n",
      "Epoch 135: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2395 - accuracy: 0.9821 - val_loss: 2.4128 - val_accuracy: 0.6724\n",
      "Epoch 136/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9792\n",
      "Epoch 136: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2470 - accuracy: 0.9792 - val_loss: 1.8554 - val_accuracy: 0.7263\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9780\n",
      "Epoch 137: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2521 - accuracy: 0.9780 - val_loss: 1.8136 - val_accuracy: 0.7176\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.9824\n",
      "Epoch 138: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2375 - accuracy: 0.9824 - val_loss: 1.5115 - val_accuracy: 0.7614\n",
      "Epoch 139/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9794\n",
      "Epoch 139: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2447 - accuracy: 0.9794 - val_loss: 1.7673 - val_accuracy: 0.7205\n",
      "Epoch 140/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9796\n",
      "Epoch 140: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2443 - accuracy: 0.9796 - val_loss: 1.4920 - val_accuracy: 0.7635\n",
      "Epoch 141/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.9804\n",
      "Epoch 141: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2423 - accuracy: 0.9804 - val_loss: 1.7013 - val_accuracy: 0.7322\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.9776\n",
      "Epoch 142: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2520 - accuracy: 0.9776 - val_loss: 1.7288 - val_accuracy: 0.7533\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9811\n",
      "Epoch 143: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2415 - accuracy: 0.9811 - val_loss: 1.4156 - val_accuracy: 0.7627\n",
      "Epoch 144/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.9806\n",
      "Epoch 144: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2414 - accuracy: 0.9806 - val_loss: 1.9654 - val_accuracy: 0.7225\n",
      "Epoch 145/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9806\n",
      "Epoch 145: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2416 - accuracy: 0.9806 - val_loss: 1.7695 - val_accuracy: 0.7302\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9801\n",
      "Epoch 146: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2414 - accuracy: 0.9801 - val_loss: 1.5571 - val_accuracy: 0.7647\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9785\n",
      "Epoch 147: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2456 - accuracy: 0.9785 - val_loss: 1.6213 - val_accuracy: 0.7313\n",
      "Epoch 148/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9799\n",
      "Epoch 148: val_accuracy did not improve from 0.77690\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2439 - accuracy: 0.9799 - val_loss: 1.5377 - val_accuracy: 0.7486\n",
      "Epoch 149/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9823\n",
      "Epoch 149: val_accuracy improved from 0.77690 to 0.77920, saving model to results\\pruned_resnet20_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2379 - accuracy: 0.9823 - val_loss: 1.3743 - val_accuracy: 0.7792\n",
      "Epoch 150/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9810\n",
      "Epoch 150: val_accuracy did not improve from 0.77920\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2404 - accuracy: 0.9809 - val_loss: 1.9315 - val_accuracy: 0.7171\n",
      " Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Run the code\n",
    "# --------------------------\n",
    "run_pruning_training(sparsity=0.5, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d529ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   448         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   544         ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 32)  128         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 32)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 32)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 16, 16, 32)  128         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 8, 8, 64)     0           ['conv2d_79[0][0]',              \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 64)     0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 64)    256         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 8, 8, 64)     0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 64)     0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 64)    256         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['activation_73[0][0]',          \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 64)     0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 64)          0           ['activation_75[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "[INFO] Applying global magnitude pruning with sparsity: 0.7\n",
      "Epoch 1/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.5635 - accuracy: 0.4804\n",
      "Epoch 1: val_accuracy improved from -inf to 0.37380, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 30s 72ms/step - loss: 1.5630 - accuracy: 0.4805 - val_loss: 2.0074 - val_accuracy: 0.3738\n",
      "Epoch 2/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.1560 - accuracy: 0.6315\n",
      "Epoch 2: val_accuracy improved from 0.37380 to 0.60250, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.1556 - accuracy: 0.6317 - val_loss: 1.2784 - val_accuracy: 0.6025\n",
      "Epoch 3/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9533 - accuracy: 0.7076\n",
      "Epoch 3: val_accuracy did not improve from 0.60250\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.9533 - accuracy: 0.7076 - val_loss: 1.8608 - val_accuracy: 0.5253\n",
      "Epoch 4/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8254 - accuracy: 0.7544\n",
      "Epoch 4: val_accuracy improved from 0.60250 to 0.63700, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.8254 - accuracy: 0.7543 - val_loss: 1.1689 - val_accuracy: 0.6370\n",
      "Epoch 5/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7293 - accuracy: 0.7919\n",
      "Epoch 5: val_accuracy improved from 0.63700 to 0.65410, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.7293 - accuracy: 0.7919 - val_loss: 1.1858 - val_accuracy: 0.6541\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.8165\n",
      "Epoch 6: val_accuracy improved from 0.65410 to 0.68800, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6619 - accuracy: 0.8165 - val_loss: 1.0593 - val_accuracy: 0.6880\n",
      "Epoch 7/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6060 - accuracy: 0.8352\n",
      "Epoch 7: val_accuracy improved from 0.68800 to 0.70860, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6063 - accuracy: 0.8351 - val_loss: 1.0464 - val_accuracy: 0.7086\n",
      "Epoch 8/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5493 - accuracy: 0.8565\n",
      "Epoch 8: val_accuracy did not improve from 0.70860\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.5495 - accuracy: 0.8566 - val_loss: 1.4282 - val_accuracy: 0.6496\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5112 - accuracy: 0.8710\n",
      "Epoch 9: val_accuracy did not improve from 0.70860\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.5112 - accuracy: 0.8710 - val_loss: 1.2748 - val_accuracy: 0.6842\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4637 - accuracy: 0.8877\n",
      "Epoch 10: val_accuracy improved from 0.70860 to 0.75290, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.4636 - accuracy: 0.8877 - val_loss: 0.9205 - val_accuracy: 0.7529\n",
      "Epoch 11/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.9000\n",
      "Epoch 11: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.4324 - accuracy: 0.8999 - val_loss: 1.2964 - val_accuracy: 0.6904\n",
      "Epoch 12/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.9138\n",
      "Epoch 12: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.4019 - accuracy: 0.9137 - val_loss: 1.3869 - val_accuracy: 0.6753\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3765 - accuracy: 0.9219\n",
      "Epoch 13: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3765 - accuracy: 0.9219 - val_loss: 1.2362 - val_accuracy: 0.7107\n",
      "Epoch 14/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.9300\n",
      "Epoch 14: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.3558 - accuracy: 0.9299 - val_loss: 1.1785 - val_accuracy: 0.7346\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.9332\n",
      "Epoch 15: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.3479 - accuracy: 0.9332 - val_loss: 1.2337 - val_accuracy: 0.7301\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.9432\n",
      "Epoch 16: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3257 - accuracy: 0.9432 - val_loss: 1.4243 - val_accuracy: 0.7127\n",
      "Epoch 17/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3192 - accuracy: 0.9434\n",
      "Epoch 17: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3194 - accuracy: 0.9434 - val_loss: 1.3197 - val_accuracy: 0.7217\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.9513\n",
      "Epoch 18: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3043 - accuracy: 0.9513 - val_loss: 1.5794 - val_accuracy: 0.7068\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.9526\n",
      "Epoch 19: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3014 - accuracy: 0.9526 - val_loss: 1.3235 - val_accuracy: 0.7318\n",
      "Epoch 20/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2919 - accuracy: 0.9575\n",
      "Epoch 20: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2920 - accuracy: 0.9574 - val_loss: 1.5043 - val_accuracy: 0.7116\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9599\n",
      "Epoch 21: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2865 - accuracy: 0.9599 - val_loss: 1.6128 - val_accuracy: 0.7102\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2947 - accuracy: 0.9571\n",
      "Epoch 22: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2947 - accuracy: 0.9571 - val_loss: 1.5946 - val_accuracy: 0.7151\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9581\n",
      "Epoch 23: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2932 - accuracy: 0.9581 - val_loss: 1.4741 - val_accuracy: 0.7376\n",
      "Epoch 24/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2829 - accuracy: 0.9625\n",
      "Epoch 24: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2830 - accuracy: 0.9625 - val_loss: 1.5043 - val_accuracy: 0.7385\n",
      "Epoch 25/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9597\n",
      "Epoch 25: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2915 - accuracy: 0.9597 - val_loss: 1.5671 - val_accuracy: 0.7235\n",
      "Epoch 26/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2747 - accuracy: 0.9661\n",
      "Epoch 26: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2749 - accuracy: 0.9660 - val_loss: 1.8673 - val_accuracy: 0.6718\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9658\n",
      "Epoch 27: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2779 - accuracy: 0.9658 - val_loss: 1.4048 - val_accuracy: 0.7283\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2798 - accuracy: 0.9652\n",
      "Epoch 28: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2799 - accuracy: 0.9652 - val_loss: 1.4526 - val_accuracy: 0.7391\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9624\n",
      "Epoch 29: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2876 - accuracy: 0.9624 - val_loss: 1.9704 - val_accuracy: 0.6723\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9698\n",
      "Epoch 30: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2685 - accuracy: 0.9698 - val_loss: 1.6593 - val_accuracy: 0.7134\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.9661\n",
      "Epoch 31: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2790 - accuracy: 0.9661 - val_loss: 1.8062 - val_accuracy: 0.7043\n",
      "Epoch 32/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2730 - accuracy: 0.9687\n",
      "Epoch 32: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2733 - accuracy: 0.9686 - val_loss: 1.5682 - val_accuracy: 0.7396\n",
      "Epoch 33/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9656\n",
      "Epoch 33: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2824 - accuracy: 0.9656 - val_loss: 1.5064 - val_accuracy: 0.7370\n",
      "Epoch 34/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.9702\n",
      "Epoch 34: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2725 - accuracy: 0.9702 - val_loss: 1.6853 - val_accuracy: 0.7096\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9690\n",
      "Epoch 35: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2745 - accuracy: 0.9690 - val_loss: 1.7769 - val_accuracy: 0.7101\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9683\n",
      "Epoch 36: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2790 - accuracy: 0.9683 - val_loss: 1.7878 - val_accuracy: 0.7179\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9715\n",
      "Epoch 37: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2705 - accuracy: 0.9715 - val_loss: 1.8990 - val_accuracy: 0.6902\n",
      "Epoch 38/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.9706\n",
      "Epoch 38: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2714 - accuracy: 0.9706 - val_loss: 1.5306 - val_accuracy: 0.7494\n",
      "Epoch 39/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2659 - accuracy: 0.9728\n",
      "Epoch 39: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2659 - accuracy: 0.9728 - val_loss: 1.6828 - val_accuracy: 0.7329\n",
      "Epoch 40/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2753 - accuracy: 0.9696\n",
      "Epoch 40: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2753 - accuracy: 0.9696 - val_loss: 1.9566 - val_accuracy: 0.6901\n",
      "Epoch 41/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.9725\n",
      "Epoch 41: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2680 - accuracy: 0.9725 - val_loss: 1.6227 - val_accuracy: 0.7438\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2704 - accuracy: 0.9712\n",
      "Epoch 42: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2705 - accuracy: 0.9712 - val_loss: 2.2216 - val_accuracy: 0.6918\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9716\n",
      "Epoch 43: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2716 - accuracy: 0.9716 - val_loss: 1.9112 - val_accuracy: 0.6928\n",
      "Epoch 44/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2760 - accuracy: 0.9693\n",
      "Epoch 44: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2760 - accuracy: 0.9693 - val_loss: 1.5175 - val_accuracy: 0.7247\n",
      "Epoch 45/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9753\n",
      "Epoch 45: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2598 - accuracy: 0.9753 - val_loss: 1.4936 - val_accuracy: 0.7312\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9721\n",
      "Epoch 46: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2711 - accuracy: 0.9721 - val_loss: 1.5018 - val_accuracy: 0.7502\n",
      "Epoch 47/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2708 - accuracy: 0.9726\n",
      "Epoch 47: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2711 - accuracy: 0.9725 - val_loss: 1.6036 - val_accuracy: 0.7324\n",
      "Epoch 48/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.9728\n",
      "Epoch 48: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2689 - accuracy: 0.9728 - val_loss: 2.2996 - val_accuracy: 0.6667\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9717\n",
      "Epoch 49: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2710 - accuracy: 0.9717 - val_loss: 1.5447 - val_accuracy: 0.7368\n",
      "Epoch 50/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.9746\n",
      "Epoch 50: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2637 - accuracy: 0.9746 - val_loss: 1.6558 - val_accuracy: 0.7255\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9730\n",
      "Epoch 51: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2705 - accuracy: 0.9730 - val_loss: 2.2773 - val_accuracy: 0.6680\n",
      "Epoch 52/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.9733\n",
      "Epoch 52: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2694 - accuracy: 0.9733 - val_loss: 1.7087 - val_accuracy: 0.7116\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9764\n",
      "Epoch 53: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2589 - accuracy: 0.9763 - val_loss: 1.4773 - val_accuracy: 0.7474\n",
      "Epoch 54/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9745\n",
      "Epoch 54: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2622 - accuracy: 0.9745 - val_loss: 2.0666 - val_accuracy: 0.6937\n",
      "Epoch 55/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2740 - accuracy: 0.9714\n",
      "Epoch 55: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2743 - accuracy: 0.9713 - val_loss: 2.0566 - val_accuracy: 0.6927\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2661 - accuracy: 0.9744\n",
      "Epoch 56: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2661 - accuracy: 0.9744 - val_loss: 1.8298 - val_accuracy: 0.7095\n",
      "Epoch 57/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9785\n",
      "Epoch 57: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2526 - accuracy: 0.9785 - val_loss: 1.6792 - val_accuracy: 0.7332\n",
      "Epoch 58/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2737 - accuracy: 0.9713\n",
      "Epoch 58: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2737 - accuracy: 0.9713 - val_loss: 1.7114 - val_accuracy: 0.7331\n",
      "Epoch 59/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.9733\n",
      "Epoch 59: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2683 - accuracy: 0.9732 - val_loss: 1.6755 - val_accuracy: 0.7281\n",
      "Epoch 60/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.9790\n",
      "Epoch 60: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2555 - accuracy: 0.9790 - val_loss: 2.1083 - val_accuracy: 0.7068\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.9752\n",
      "Epoch 61: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2636 - accuracy: 0.9752 - val_loss: 2.1231 - val_accuracy: 0.6667\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9752\n",
      "Epoch 62: val_accuracy did not improve from 0.75290\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2647 - accuracy: 0.9752 - val_loss: 3.1486 - val_accuracy: 0.6059\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9745\n",
      "Epoch 63: val_accuracy improved from 0.75290 to 0.76930, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2645 - accuracy: 0.9745 - val_loss: 1.3181 - val_accuracy: 0.7693\n",
      "Epoch 64/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9764\n",
      "Epoch 64: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2605 - accuracy: 0.9763 - val_loss: 1.5460 - val_accuracy: 0.7376\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9736\n",
      "Epoch 65: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2663 - accuracy: 0.9736 - val_loss: 2.1246 - val_accuracy: 0.6985\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9769\n",
      "Epoch 66: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2589 - accuracy: 0.9769 - val_loss: 1.5828 - val_accuracy: 0.7418\n",
      "Epoch 67/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9765\n",
      "Epoch 67: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2601 - accuracy: 0.9765 - val_loss: 1.7324 - val_accuracy: 0.7415\n",
      "Epoch 68/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2593 - accuracy: 0.9769\n",
      "Epoch 68: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2593 - accuracy: 0.9769 - val_loss: 1.7627 - val_accuracy: 0.7282\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9758\n",
      "Epoch 69: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2608 - accuracy: 0.9758 - val_loss: 2.1579 - val_accuracy: 0.6990\n",
      "Epoch 70/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9762\n",
      "Epoch 70: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2596 - accuracy: 0.9762 - val_loss: 1.8464 - val_accuracy: 0.7264\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9762\n",
      "Epoch 71: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2601 - accuracy: 0.9762 - val_loss: 2.5402 - val_accuracy: 0.6593\n",
      "Epoch 72/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9759\n",
      "Epoch 72: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2606 - accuracy: 0.9759 - val_loss: 1.8696 - val_accuracy: 0.7064\n",
      "Epoch 73/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9760\n",
      "Epoch 73: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2612 - accuracy: 0.9760 - val_loss: 1.6518 - val_accuracy: 0.7440\n",
      "Epoch 74/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9760\n",
      "Epoch 74: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2602 - accuracy: 0.9760 - val_loss: 1.8075 - val_accuracy: 0.7263\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9796\n",
      "Epoch 75: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2517 - accuracy: 0.9796 - val_loss: 1.5112 - val_accuracy: 0.7634\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9750\n",
      "Epoch 76: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2630 - accuracy: 0.9750 - val_loss: 1.7880 - val_accuracy: 0.7195\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9728\n",
      "Epoch 77: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2699 - accuracy: 0.9728 - val_loss: 1.4241 - val_accuracy: 0.7642\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9833\n",
      "Epoch 78: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 38s 97ms/step - loss: 0.2415 - accuracy: 0.9833 - val_loss: 2.2950 - val_accuracy: 0.6934\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9780\n",
      "Epoch 79: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 52s 134ms/step - loss: 0.2545 - accuracy: 0.9780 - val_loss: 1.9015 - val_accuracy: 0.7191\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9753\n",
      "Epoch 80: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2640 - accuracy: 0.9753 - val_loss: 2.2521 - val_accuracy: 0.7031\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9752\n",
      "Epoch 81: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2602 - accuracy: 0.9752 - val_loss: 1.9764 - val_accuracy: 0.7176\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9791\n",
      "Epoch 82: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2532 - accuracy: 0.9791 - val_loss: 1.8553 - val_accuracy: 0.7104\n",
      "Epoch 83/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9788\n",
      "Epoch 83: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2519 - accuracy: 0.9788 - val_loss: 1.4447 - val_accuracy: 0.7640\n",
      "Epoch 84/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.9755\n",
      "Epoch 84: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2608 - accuracy: 0.9755 - val_loss: 1.6645 - val_accuracy: 0.7285\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9762\n",
      "Epoch 85: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2574 - accuracy: 0.9762 - val_loss: 1.4711 - val_accuracy: 0.7654\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9787\n",
      "Epoch 86: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2512 - accuracy: 0.9787 - val_loss: 1.6267 - val_accuracy: 0.7458\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9783\n",
      "Epoch 87: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2529 - accuracy: 0.9783 - val_loss: 1.5381 - val_accuracy: 0.7516\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9765\n",
      "Epoch 88: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2560 - accuracy: 0.9765 - val_loss: 1.8781 - val_accuracy: 0.7203\n",
      "Epoch 89/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9784\n",
      "Epoch 89: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2506 - accuracy: 0.9784 - val_loss: 1.9453 - val_accuracy: 0.7231\n",
      "Epoch 90/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2533 - accuracy: 0.9783\n",
      "Epoch 90: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2533 - accuracy: 0.9783 - val_loss: 1.6732 - val_accuracy: 0.7378\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9754\n",
      "Epoch 91: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2594 - accuracy: 0.9754 - val_loss: 1.9142 - val_accuracy: 0.7151\n",
      "Epoch 92/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9813\n",
      "Epoch 92: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2449 - accuracy: 0.9812 - val_loss: 1.5216 - val_accuracy: 0.7582\n",
      "Epoch 93/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9780\n",
      "Epoch 93: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2522 - accuracy: 0.9780 - val_loss: 1.7428 - val_accuracy: 0.7375\n",
      "Epoch 94/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9781\n",
      "Epoch 94: val_accuracy did not improve from 0.76930\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2522 - accuracy: 0.9780 - val_loss: 1.4122 - val_accuracy: 0.7519\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9781\n",
      "Epoch 95: val_accuracy improved from 0.76930 to 0.77640, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2510 - accuracy: 0.9781 - val_loss: 1.3474 - val_accuracy: 0.7764\n",
      "Epoch 96/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.9778\n",
      "Epoch 96: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2529 - accuracy: 0.9778 - val_loss: 1.7116 - val_accuracy: 0.7463\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9771\n",
      "Epoch 97: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2528 - accuracy: 0.9771 - val_loss: 1.7234 - val_accuracy: 0.7469\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9793\n",
      "Epoch 98: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2488 - accuracy: 0.9793 - val_loss: 1.5788 - val_accuracy: 0.7631\n",
      "Epoch 99/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.9785\n",
      "Epoch 99: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2499 - accuracy: 0.9785 - val_loss: 1.4912 - val_accuracy: 0.7706\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9772\n",
      "Epoch 100: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2539 - accuracy: 0.9772 - val_loss: 1.4406 - val_accuracy: 0.7675\n",
      "Epoch 101/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.9805\n",
      "Epoch 101: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2456 - accuracy: 0.9805 - val_loss: 1.5662 - val_accuracy: 0.7613\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.9781\n",
      "Epoch 102: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2501 - accuracy: 0.9781 - val_loss: 1.7413 - val_accuracy: 0.7337\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9785\n",
      "Epoch 103: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2494 - accuracy: 0.9785 - val_loss: 2.0145 - val_accuracy: 0.6905\n",
      "Epoch 104/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9775\n",
      "Epoch 104: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2519 - accuracy: 0.9775 - val_loss: 1.7848 - val_accuracy: 0.7338\n",
      "Epoch 105/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9806\n",
      "Epoch 105: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2443 - accuracy: 0.9806 - val_loss: 1.4419 - val_accuracy: 0.7568\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2458 - accuracy: 0.9797\n",
      "Epoch 106: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2458 - accuracy: 0.9797 - val_loss: 1.6206 - val_accuracy: 0.7438\n",
      "Epoch 107/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9769\n",
      "Epoch 107: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2539 - accuracy: 0.9769 - val_loss: 1.5270 - val_accuracy: 0.7620\n",
      "Epoch 108/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9769\n",
      "Epoch 108: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2526 - accuracy: 0.9769 - val_loss: 1.6145 - val_accuracy: 0.7472\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9830\n",
      "Epoch 109: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2372 - accuracy: 0.9830 - val_loss: 1.7241 - val_accuracy: 0.7455\n",
      "Epoch 110/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9791\n",
      "Epoch 110: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2470 - accuracy: 0.9791 - val_loss: 1.8093 - val_accuracy: 0.7306\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9787\n",
      "Epoch 111: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2462 - accuracy: 0.9787 - val_loss: 1.4858 - val_accuracy: 0.7692\n",
      "Epoch 112/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9771\n",
      "Epoch 112: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2526 - accuracy: 0.9771 - val_loss: 1.8248 - val_accuracy: 0.7230\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9786\n",
      "Epoch 113: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2485 - accuracy: 0.9786 - val_loss: 1.3703 - val_accuracy: 0.7744\n",
      "Epoch 114/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9803\n",
      "Epoch 114: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2449 - accuracy: 0.9803 - val_loss: 1.7968 - val_accuracy: 0.7175\n",
      "Epoch 115/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9803\n",
      "Epoch 115: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2430 - accuracy: 0.9803 - val_loss: 1.5371 - val_accuracy: 0.7557\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9784\n",
      "Epoch 116: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2471 - accuracy: 0.9784 - val_loss: 1.3709 - val_accuracy: 0.7748\n",
      "Epoch 117/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9794\n",
      "Epoch 117: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2437 - accuracy: 0.9794 - val_loss: 2.0164 - val_accuracy: 0.7201\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9786\n",
      "Epoch 118: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2472 - accuracy: 0.9786 - val_loss: 1.4062 - val_accuracy: 0.7728\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9791\n",
      "Epoch 119: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2445 - accuracy: 0.9791 - val_loss: 1.5128 - val_accuracy: 0.7550\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2423 - accuracy: 0.9803\n",
      "Epoch 120: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2423 - accuracy: 0.9803 - val_loss: 2.4470 - val_accuracy: 0.6730\n",
      "Epoch 121/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9817\n",
      "Epoch 121: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2393 - accuracy: 0.9817 - val_loss: 1.8391 - val_accuracy: 0.7332\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9802\n",
      "Epoch 122: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2414 - accuracy: 0.9802 - val_loss: 1.3614 - val_accuracy: 0.7680\n",
      "Epoch 123/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9778\n",
      "Epoch 123: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2487 - accuracy: 0.9779 - val_loss: 1.4493 - val_accuracy: 0.7595\n",
      "Epoch 124/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9759\n",
      "Epoch 124: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2521 - accuracy: 0.9759 - val_loss: 1.9626 - val_accuracy: 0.7008\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9817\n",
      "Epoch 125: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2382 - accuracy: 0.9817 - val_loss: 1.4897 - val_accuracy: 0.7501\n",
      "Epoch 126/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9800\n",
      "Epoch 126: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2408 - accuracy: 0.9800 - val_loss: 1.7226 - val_accuracy: 0.7374\n",
      "Epoch 127/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9794\n",
      "Epoch 127: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2441 - accuracy: 0.9794 - val_loss: 1.5135 - val_accuracy: 0.7575\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9828\n",
      "Epoch 128: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2335 - accuracy: 0.9828 - val_loss: 1.6139 - val_accuracy: 0.7473\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9786\n",
      "Epoch 129: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2450 - accuracy: 0.9786 - val_loss: 2.8175 - val_accuracy: 0.6450\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9783\n",
      "Epoch 130: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2449 - accuracy: 0.9783 - val_loss: 2.1974 - val_accuracy: 0.6971\n",
      "Epoch 131/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9793\n",
      "Epoch 131: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2406 - accuracy: 0.9793 - val_loss: 1.3942 - val_accuracy: 0.7710\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9797\n",
      "Epoch 132: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2424 - accuracy: 0.9797 - val_loss: 1.4442 - val_accuracy: 0.7703\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9797\n",
      "Epoch 133: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2419 - accuracy: 0.9797 - val_loss: 1.6487 - val_accuracy: 0.7491\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9805\n",
      "Epoch 134: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2396 - accuracy: 0.9805 - val_loss: 1.7949 - val_accuracy: 0.7317\n",
      "Epoch 135/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9796\n",
      "Epoch 135: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2424 - accuracy: 0.9796 - val_loss: 1.5147 - val_accuracy: 0.7550\n",
      "Epoch 136/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9791\n",
      "Epoch 136: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2405 - accuracy: 0.9791 - val_loss: 1.5978 - val_accuracy: 0.7493\n",
      "Epoch 137/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9801\n",
      "Epoch 137: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2402 - accuracy: 0.9802 - val_loss: 1.3955 - val_accuracy: 0.7757\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.9789\n",
      "Epoch 138: val_accuracy did not improve from 0.77640\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2424 - accuracy: 0.9789 - val_loss: 1.3940 - val_accuracy: 0.7713\n",
      "Epoch 139/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2346 - accuracy: 0.9820\n",
      "Epoch 139: val_accuracy improved from 0.77640 to 0.78380, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2348 - accuracy: 0.9820 - val_loss: 1.4662 - val_accuracy: 0.7838\n",
      "Epoch 140/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9802\n",
      "Epoch 140: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2367 - accuracy: 0.9802 - val_loss: 1.6342 - val_accuracy: 0.7497\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9758\n",
      "Epoch 141: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9758 - val_loss: 1.4153 - val_accuracy: 0.7596\n",
      "Epoch 142/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9805\n",
      "Epoch 142: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2383 - accuracy: 0.9805 - val_loss: 2.1667 - val_accuracy: 0.6850\n",
      "Epoch 143/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2293 - accuracy: 0.9836\n",
      "Epoch 143: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2293 - accuracy: 0.9836 - val_loss: 1.5877 - val_accuracy: 0.7426\n",
      "Epoch 144/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9796\n",
      "Epoch 144: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2388 - accuracy: 0.9797 - val_loss: 1.5863 - val_accuracy: 0.7510\n",
      "Epoch 145/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9790\n",
      "Epoch 145: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2431 - accuracy: 0.9789 - val_loss: 1.6034 - val_accuracy: 0.7590\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9790\n",
      "Epoch 146: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2415 - accuracy: 0.9790 - val_loss: 1.5539 - val_accuracy: 0.7582\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9823\n",
      "Epoch 147: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2326 - accuracy: 0.9823 - val_loss: 1.6946 - val_accuracy: 0.7311\n",
      "Epoch 148/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2420 - accuracy: 0.9788\n",
      "Epoch 148: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2420 - accuracy: 0.9787 - val_loss: 1.5801 - val_accuracy: 0.7573\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9811\n",
      "Epoch 149: val_accuracy did not improve from 0.78380\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2351 - accuracy: 0.9811 - val_loss: 1.5220 - val_accuracy: 0.7625\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9811\n",
      "Epoch 150: val_accuracy improved from 0.78380 to 0.78460, saving model to results\\pruned_resnet20_sparsity_0.7.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2340 - accuracy: 0.9811 - val_loss: 1.2852 - val_accuracy: 0.7846\n",
      " Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Run the code\n",
    "# --------------------------\n",
    "run_pruning_training(sparsity=0.7, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6638883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 32, 32, 16)   448         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32, 32, 16)  64          ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 32, 32, 16)  64          ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 32, 32, 16)  64          ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 32, 32, 16)   0           ['activation_76[0][0]',          \n",
      "                                                                  'batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 32, 32, 16)   0           ['add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 32, 32, 16)  64          ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 32, 32, 16)  64          ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 32, 32, 16)   0           ['activation_78[0][0]',          \n",
      "                                                                  'batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 32, 32, 16)   0           ['add_37[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 32, 32, 16)  64          ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 32, 32, 16)  64          ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 32, 32, 16)   0           ['activation_80[0][0]',          \n",
      "                                                                  'batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 32, 32, 16)   0           ['add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 16, 16, 32)  128         ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 16, 16, 32)   544         ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 16, 16, 32)  128         ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 16, 16, 32)   0           ['conv2d_93[0][0]',              \n",
      "                                                                  'batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 16, 16, 32)   0           ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 16, 16, 32)  128         ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_85[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 16, 16, 32)  128         ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 16, 16, 32)   0           ['activation_84[0][0]',          \n",
      "                                                                  'batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 16, 16, 32)   0           ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 16, 16, 32)  128         ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_87[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 16, 16, 32)  128         ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 16, 16, 32)   0           ['activation_86[0][0]',          \n",
      "                                                                  'batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 16, 16, 32)   0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 8, 8, 64)    256         ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 8, 8, 64)     2112        ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 8, 8, 64)    256         ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 8, 8, 64)     0           ['conv2d_100[0][0]',             \n",
      "                                                                  'batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 8, 8, 64)     0           ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 8, 8, 64)    256         ['conv2d_101[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 8, 8, 64)    256         ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 8, 8, 64)     0           ['activation_90[0][0]',          \n",
      "                                                                  'batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 8, 8, 64)     0           ['add_43[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 8, 8, 64)    256         ['conv2d_103[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 8, 8, 64)    256         ['conv2d_104[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 8, 8, 64)     0           ['activation_92[0][0]',          \n",
      "                                                                  'batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 8, 8, 64)     0           ['add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 64)          0           ['activation_94[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 10)           650         ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "[INFO] Applying global magnitude pruning with sparsity: 0.9\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5000 - accuracy: 0.4890\n",
      "Epoch 1: val_accuracy improved from -inf to 0.42720, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 31s 74ms/step - loss: 1.5000 - accuracy: 0.4890 - val_loss: 1.7011 - val_accuracy: 0.4272\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0607 - accuracy: 0.6514\n",
      "Epoch 2: val_accuracy improved from 0.42720 to 0.65030, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.0607 - accuracy: 0.6514 - val_loss: 1.0632 - val_accuracy: 0.6503\n",
      "Epoch 3/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8809 - accuracy: 0.7202\n",
      "Epoch 3: val_accuracy did not improve from 0.65030\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.8810 - accuracy: 0.7201 - val_loss: 1.4248 - val_accuracy: 0.5814\n",
      "Epoch 4/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7727 - accuracy: 0.7619\n",
      "Epoch 4: val_accuracy did not improve from 0.65030\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.7726 - accuracy: 0.7619 - val_loss: 1.7117 - val_accuracy: 0.5363\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6864 - accuracy: 0.7938\n",
      "Epoch 5: val_accuracy improved from 0.65030 to 0.68440, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6864 - accuracy: 0.7938 - val_loss: 1.0745 - val_accuracy: 0.6844\n",
      "Epoch 6/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.8184\n",
      "Epoch 6: val_accuracy did not improve from 0.68440\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6218 - accuracy: 0.8184 - val_loss: 1.1755 - val_accuracy: 0.6631\n",
      "Epoch 7/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5690 - accuracy: 0.8380\n",
      "Epoch 7: val_accuracy did not improve from 0.68440\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.5689 - accuracy: 0.8380 - val_loss: 1.1686 - val_accuracy: 0.6657\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.8567\n",
      "Epoch 8: val_accuracy improved from 0.68440 to 0.74040, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5268 - accuracy: 0.8567 - val_loss: 0.9881 - val_accuracy: 0.7404\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4871 - accuracy: 0.8702\n",
      "Epoch 9: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4874 - accuracy: 0.8700 - val_loss: 0.9694 - val_accuracy: 0.7272\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4486 - accuracy: 0.8853\n",
      "Epoch 10: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4486 - accuracy: 0.8852 - val_loss: 1.1760 - val_accuracy: 0.6998\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8999\n",
      "Epoch 11: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.4102 - accuracy: 0.8999 - val_loss: 1.1662 - val_accuracy: 0.7110\n",
      "Epoch 12/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.9106\n",
      "Epoch 12: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.3822 - accuracy: 0.9107 - val_loss: 1.0951 - val_accuracy: 0.7354\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3637 - accuracy: 0.9204\n",
      "Epoch 13: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3637 - accuracy: 0.9204 - val_loss: 1.4821 - val_accuracy: 0.6696\n",
      "Epoch 14/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3395 - accuracy: 0.9285\n",
      "Epoch 14: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3395 - accuracy: 0.9284 - val_loss: 1.3144 - val_accuracy: 0.7171\n",
      "Epoch 15/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.9353\n",
      "Epoch 15: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3274 - accuracy: 0.9352 - val_loss: 1.1743 - val_accuracy: 0.7391\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.9446\n",
      "Epoch 16: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3036 - accuracy: 0.9446 - val_loss: 1.4238 - val_accuracy: 0.7251\n",
      "Epoch 17/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9444\n",
      "Epoch 17: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3037 - accuracy: 0.9443 - val_loss: 1.8893 - val_accuracy: 0.6544\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.9485\n",
      "Epoch 18: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2947 - accuracy: 0.9485 - val_loss: 1.6347 - val_accuracy: 0.6671\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.9492\n",
      "Epoch 19: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2944 - accuracy: 0.9492 - val_loss: 1.8358 - val_accuracy: 0.6473\n",
      "Epoch 20/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.9579\n",
      "Epoch 20: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2792 - accuracy: 0.9578 - val_loss: 1.4274 - val_accuracy: 0.7185\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.9567\n",
      "Epoch 21: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2788 - accuracy: 0.9567 - val_loss: 1.6625 - val_accuracy: 0.7038\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.9603\n",
      "Epoch 22: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2736 - accuracy: 0.9603 - val_loss: 1.4901 - val_accuracy: 0.7204\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9584\n",
      "Epoch 23: val_accuracy did not improve from 0.74040\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2782 - accuracy: 0.9584 - val_loss: 1.5254 - val_accuracy: 0.7238\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2744 - accuracy: 0.9607\n",
      "Epoch 24: val_accuracy improved from 0.74040 to 0.75470, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2744 - accuracy: 0.9607 - val_loss: 1.2950 - val_accuracy: 0.7547\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9644\n",
      "Epoch 25: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2666 - accuracy: 0.9644 - val_loss: 2.1592 - val_accuracy: 0.6689\n",
      "Epoch 26/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9644\n",
      "Epoch 26: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2677 - accuracy: 0.9644 - val_loss: 1.5756 - val_accuracy: 0.7078\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.9633\n",
      "Epoch 27: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2741 - accuracy: 0.9633 - val_loss: 1.4056 - val_accuracy: 0.7448\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2711 - accuracy: 0.9642\n",
      "Epoch 28: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2710 - accuracy: 0.9643 - val_loss: 1.8493 - val_accuracy: 0.6789\n",
      "Epoch 29/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9669\n",
      "Epoch 29: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2640 - accuracy: 0.9668 - val_loss: 1.4343 - val_accuracy: 0.7492\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9678\n",
      "Epoch 30: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2637 - accuracy: 0.9678 - val_loss: 1.5317 - val_accuracy: 0.7367\n",
      "Epoch 31/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9696\n",
      "Epoch 31: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2594 - accuracy: 0.9696 - val_loss: 1.5145 - val_accuracy: 0.7270\n",
      "Epoch 32/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9651\n",
      "Epoch 32: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2716 - accuracy: 0.9651 - val_loss: 1.8894 - val_accuracy: 0.6846\n",
      "Epoch 33/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.9664\n",
      "Epoch 33: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2693 - accuracy: 0.9664 - val_loss: 1.9768 - val_accuracy: 0.6791\n",
      "Epoch 34/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.9706\n",
      "Epoch 34: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2595 - accuracy: 0.9706 - val_loss: 1.8690 - val_accuracy: 0.6938\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9719\n",
      "Epoch 35: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2543 - accuracy: 0.9719 - val_loss: 1.5293 - val_accuracy: 0.7199\n",
      "Epoch 36/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.9678\n",
      "Epoch 36: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2718 - accuracy: 0.9678 - val_loss: 1.6069 - val_accuracy: 0.7220\n",
      "Epoch 37/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9709\n",
      "Epoch 37: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2622 - accuracy: 0.9709 - val_loss: 1.8786 - val_accuracy: 0.6893\n",
      "Epoch 38/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2624 - accuracy: 0.9709\n",
      "Epoch 38: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2624 - accuracy: 0.9709 - val_loss: 1.8266 - val_accuracy: 0.6979\n",
      "Epoch 39/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.9725\n",
      "Epoch 39: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2586 - accuracy: 0.9726 - val_loss: 1.4655 - val_accuracy: 0.7435\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9710\n",
      "Epoch 40: val_accuracy did not improve from 0.75470\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2641 - accuracy: 0.9710 - val_loss: 1.8917 - val_accuracy: 0.6916\n",
      "Epoch 41/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.9692\n",
      "Epoch 41: val_accuracy improved from 0.75470 to 0.76000, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2672 - accuracy: 0.9692 - val_loss: 1.3668 - val_accuracy: 0.7600\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.9755\n",
      "Epoch 42: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2531 - accuracy: 0.9755 - val_loss: 1.5494 - val_accuracy: 0.7514\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9746\n",
      "Epoch 43: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2535 - accuracy: 0.9746 - val_loss: 1.5569 - val_accuracy: 0.7400\n",
      "Epoch 44/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.9675\n",
      "Epoch 44: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2737 - accuracy: 0.9675 - val_loss: 1.6456 - val_accuracy: 0.7228\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9742\n",
      "Epoch 45: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2578 - accuracy: 0.9742 - val_loss: 1.4457 - val_accuracy: 0.7595\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9722\n",
      "Epoch 46: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2614 - accuracy: 0.9722 - val_loss: 1.3813 - val_accuracy: 0.7449\n",
      "Epoch 47/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9748\n",
      "Epoch 47: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2566 - accuracy: 0.9748 - val_loss: 1.6868 - val_accuracy: 0.7357\n",
      "Epoch 48/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.9758\n",
      "Epoch 48: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2525 - accuracy: 0.9758 - val_loss: 1.5615 - val_accuracy: 0.7415\n",
      "Epoch 49/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.9717\n",
      "Epoch 49: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2645 - accuracy: 0.9717 - val_loss: 1.9272 - val_accuracy: 0.7068\n",
      "Epoch 50/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.9722\n",
      "Epoch 50: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2644 - accuracy: 0.9722 - val_loss: 1.6686 - val_accuracy: 0.7271\n",
      "Epoch 51/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2528 - accuracy: 0.9766\n",
      "Epoch 51: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2529 - accuracy: 0.9766 - val_loss: 1.5635 - val_accuracy: 0.7426\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9744\n",
      "Epoch 52: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2588 - accuracy: 0.9744 - val_loss: 1.6641 - val_accuracy: 0.7238\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9744\n",
      "Epoch 53: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2605 - accuracy: 0.9744 - val_loss: 1.8067 - val_accuracy: 0.7165\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.9728\n",
      "Epoch 54: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2621 - accuracy: 0.9728 - val_loss: 1.8794 - val_accuracy: 0.7063\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9728\n",
      "Epoch 55: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2623 - accuracy: 0.9728 - val_loss: 1.3932 - val_accuracy: 0.7544\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9767\n",
      "Epoch 56: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2549 - accuracy: 0.9767 - val_loss: 1.7153 - val_accuracy: 0.7273\n",
      "Epoch 57/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9755\n",
      "Epoch 57: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2575 - accuracy: 0.9755 - val_loss: 1.6037 - val_accuracy: 0.7559\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9768\n",
      "Epoch 58: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2545 - accuracy: 0.9768 - val_loss: 1.5993 - val_accuracy: 0.7432\n",
      "Epoch 59/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9742\n",
      "Epoch 59: val_accuracy did not improve from 0.76000\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2592 - accuracy: 0.9742 - val_loss: 1.7625 - val_accuracy: 0.7244\n",
      "Epoch 60/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9773\n",
      "Epoch 60: val_accuracy improved from 0.76000 to 0.76760, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2505 - accuracy: 0.9773 - val_loss: 1.4358 - val_accuracy: 0.7676\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9761\n",
      "Epoch 61: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2553 - accuracy: 0.9761 - val_loss: 2.0232 - val_accuracy: 0.7073\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2505 - accuracy: 0.9777\n",
      "Epoch 62: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2505 - accuracy: 0.9777 - val_loss: 1.8003 - val_accuracy: 0.7224\n",
      "Epoch 63/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2665 - accuracy: 0.9712\n",
      "Epoch 63: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2666 - accuracy: 0.9711 - val_loss: 1.5501 - val_accuracy: 0.7525\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9773\n",
      "Epoch 64: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2523 - accuracy: 0.9773 - val_loss: 1.4713 - val_accuracy: 0.7659\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2553 - accuracy: 0.9762\n",
      "Epoch 65: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2553 - accuracy: 0.9762 - val_loss: 1.5844 - val_accuracy: 0.7389\n",
      "Epoch 66/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9754\n",
      "Epoch 66: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2575 - accuracy: 0.9754 - val_loss: 1.6477 - val_accuracy: 0.7336\n",
      "Epoch 67/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.9777\n",
      "Epoch 67: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2504 - accuracy: 0.9777 - val_loss: 2.2578 - val_accuracy: 0.6866\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9744\n",
      "Epoch 68: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2596 - accuracy: 0.9744 - val_loss: 1.8578 - val_accuracy: 0.7186\n",
      "Epoch 69/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2519 - accuracy: 0.9769\n",
      "Epoch 69: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9770 - val_loss: 1.6123 - val_accuracy: 0.7434\n",
      "Epoch 70/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.9787\n",
      "Epoch 70: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2477 - accuracy: 0.9787 - val_loss: 2.1967 - val_accuracy: 0.6799\n",
      "Epoch 71/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.9757\n",
      "Epoch 71: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2578 - accuracy: 0.9757 - val_loss: 1.6617 - val_accuracy: 0.7321\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9787\n",
      "Epoch 72: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2489 - accuracy: 0.9786 - val_loss: 1.5824 - val_accuracy: 0.7289\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9760\n",
      "Epoch 73: val_accuracy did not improve from 0.76760\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2548 - accuracy: 0.9760 - val_loss: 1.8844 - val_accuracy: 0.7293\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9753\n",
      "Epoch 74: val_accuracy improved from 0.76760 to 0.77850, saving model to results\\pruned_resnet20_sparsity_0.9.h5\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2543 - accuracy: 0.9753 - val_loss: 1.3725 - val_accuracy: 0.7785\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9781\n",
      "Epoch 75: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2508 - accuracy: 0.9781 - val_loss: 1.3909 - val_accuracy: 0.7687\n",
      "Epoch 76/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9762\n",
      "Epoch 76: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2567 - accuracy: 0.9762 - val_loss: 2.0020 - val_accuracy: 0.7101\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.9772\n",
      "Epoch 77: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2548 - accuracy: 0.9772 - val_loss: 1.3704 - val_accuracy: 0.7726\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9791\n",
      "Epoch 78: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2473 - accuracy: 0.9791 - val_loss: 1.6890 - val_accuracy: 0.7395\n",
      "Epoch 79/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2563 - accuracy: 0.9756\n",
      "Epoch 79: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2566 - accuracy: 0.9756 - val_loss: 2.4037 - val_accuracy: 0.6775\n",
      "Epoch 80/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.9735\n",
      "Epoch 80: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2620 - accuracy: 0.9735 - val_loss: 1.9271 - val_accuracy: 0.7321\n",
      "Epoch 81/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9805\n",
      "Epoch 81: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2448 - accuracy: 0.9805 - val_loss: 1.5441 - val_accuracy: 0.7491\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9791\n",
      "Epoch 82: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2443 - accuracy: 0.9791 - val_loss: 2.0537 - val_accuracy: 0.6997\n",
      "Epoch 83/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9757\n",
      "Epoch 83: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2581 - accuracy: 0.9757 - val_loss: 1.7381 - val_accuracy: 0.7304\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9795\n",
      "Epoch 84: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2467 - accuracy: 0.9795 - val_loss: 1.5359 - val_accuracy: 0.7469\n",
      "Epoch 85/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9769\n",
      "Epoch 85: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2526 - accuracy: 0.9769 - val_loss: 1.3835 - val_accuracy: 0.7713\n",
      "Epoch 86/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9786\n",
      "Epoch 86: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2485 - accuracy: 0.9786 - val_loss: 1.5024 - val_accuracy: 0.7596\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.9786\n",
      "Epoch 87: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2469 - accuracy: 0.9786 - val_loss: 1.9312 - val_accuracy: 0.7179\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9790\n",
      "Epoch 88: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2467 - accuracy: 0.9790 - val_loss: 1.3581 - val_accuracy: 0.7674\n",
      "Epoch 89/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.9758\n",
      "Epoch 89: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2529 - accuracy: 0.9758 - val_loss: 1.7778 - val_accuracy: 0.7304\n",
      "Epoch 90/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2456 - accuracy: 0.9792\n",
      "Epoch 90: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2456 - accuracy: 0.9792 - val_loss: 1.4940 - val_accuracy: 0.7527\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9799\n",
      "Epoch 91: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2426 - accuracy: 0.9799 - val_loss: 1.4327 - val_accuracy: 0.7608\n",
      "Epoch 92/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9780\n",
      "Epoch 92: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2496 - accuracy: 0.9779 - val_loss: 2.3404 - val_accuracy: 0.6849\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9773\n",
      "Epoch 93: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2512 - accuracy: 0.9773 - val_loss: 1.6010 - val_accuracy: 0.7408\n",
      "Epoch 94/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9793\n",
      "Epoch 94: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2444 - accuracy: 0.9792 - val_loss: 1.6418 - val_accuracy: 0.7334\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9755\n",
      "Epoch 95: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2551 - accuracy: 0.9755 - val_loss: 1.5272 - val_accuracy: 0.7519\n",
      "Epoch 96/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9801\n",
      "Epoch 96: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2443 - accuracy: 0.9801 - val_loss: 1.6827 - val_accuracy: 0.7436\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2442 - accuracy: 0.9799\n",
      "Epoch 97: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2442 - accuracy: 0.9799 - val_loss: 1.7125 - val_accuracy: 0.7487\n",
      "Epoch 98/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9804\n",
      "Epoch 98: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2412 - accuracy: 0.9804 - val_loss: 1.5885 - val_accuracy: 0.7415\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9780\n",
      "Epoch 99: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2463 - accuracy: 0.9780 - val_loss: 1.8505 - val_accuracy: 0.7234\n",
      "Epoch 100/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9768\n",
      "Epoch 100: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2506 - accuracy: 0.9768 - val_loss: 1.6071 - val_accuracy: 0.7351\n",
      "Epoch 101/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9807\n",
      "Epoch 101: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2399 - accuracy: 0.9807 - val_loss: 1.6393 - val_accuracy: 0.7381\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9785\n",
      "Epoch 102: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2472 - accuracy: 0.9784 - val_loss: 1.4879 - val_accuracy: 0.7571\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9781\n",
      "Epoch 103: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2480 - accuracy: 0.9781 - val_loss: 1.4492 - val_accuracy: 0.7619\n",
      "Epoch 104/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9817\n",
      "Epoch 104: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2369 - accuracy: 0.9818 - val_loss: 1.4338 - val_accuracy: 0.7662\n",
      "Epoch 105/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.9762\n",
      "Epoch 105: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2510 - accuracy: 0.9762 - val_loss: 1.7687 - val_accuracy: 0.7244\n",
      "Epoch 106/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9787\n",
      "Epoch 106: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2445 - accuracy: 0.9787 - val_loss: 2.2519 - val_accuracy: 0.7106\n",
      "Epoch 107/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9807\n",
      "Epoch 107: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2399 - accuracy: 0.9807 - val_loss: 1.4391 - val_accuracy: 0.7577\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9784\n",
      "Epoch 108: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2459 - accuracy: 0.9784 - val_loss: 1.3208 - val_accuracy: 0.7759\n",
      "Epoch 109/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9782\n",
      "Epoch 109: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2468 - accuracy: 0.9782 - val_loss: 1.3369 - val_accuracy: 0.7656\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.9792\n",
      "Epoch 110: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2429 - accuracy: 0.9792 - val_loss: 2.3193 - val_accuracy: 0.6844\n",
      "Epoch 111/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9795\n",
      "Epoch 111: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2417 - accuracy: 0.9795 - val_loss: 1.5773 - val_accuracy: 0.7371\n",
      "Epoch 112/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9796\n",
      "Epoch 112: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2418 - accuracy: 0.9795 - val_loss: 1.9739 - val_accuracy: 0.7198\n",
      "Epoch 113/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9817\n",
      "Epoch 113: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2359 - accuracy: 0.9817 - val_loss: 1.8873 - val_accuracy: 0.7335\n",
      "Epoch 114/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9800\n",
      "Epoch 114: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2411 - accuracy: 0.9800 - val_loss: 1.5981 - val_accuracy: 0.7458\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9786\n",
      "Epoch 115: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2467 - accuracy: 0.9786 - val_loss: 1.4887 - val_accuracy: 0.7549\n",
      "Epoch 116/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9827\n",
      "Epoch 116: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2317 - accuracy: 0.9827 - val_loss: 1.4043 - val_accuracy: 0.7737\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.9793\n",
      "Epoch 117: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2404 - accuracy: 0.9793 - val_loss: 1.9686 - val_accuracy: 0.7228\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2428 - accuracy: 0.9786\n",
      "Epoch 118: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2428 - accuracy: 0.9786 - val_loss: 1.9927 - val_accuracy: 0.7117\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9783\n",
      "Epoch 119: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2444 - accuracy: 0.9783 - val_loss: 1.3842 - val_accuracy: 0.7704\n",
      "Epoch 120/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9803\n",
      "Epoch 120: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2415 - accuracy: 0.9802 - val_loss: 1.4490 - val_accuracy: 0.7606\n",
      "Epoch 121/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9809\n",
      "Epoch 121: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2383 - accuracy: 0.9808 - val_loss: 1.6570 - val_accuracy: 0.7578\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9814\n",
      "Epoch 122: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2362 - accuracy: 0.9814 - val_loss: 1.4963 - val_accuracy: 0.7723\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9778\n",
      "Epoch 123: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2440 - accuracy: 0.9778 - val_loss: 1.6452 - val_accuracy: 0.7378\n",
      "Epoch 124/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9809\n",
      "Epoch 124: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2385 - accuracy: 0.9809 - val_loss: 1.3151 - val_accuracy: 0.7716\n",
      "Epoch 125/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9794\n",
      "Epoch 125: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2398 - accuracy: 0.9793 - val_loss: 1.8156 - val_accuracy: 0.7499\n",
      "Epoch 126/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9807\n",
      "Epoch 126: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2352 - accuracy: 0.9807 - val_loss: 1.9604 - val_accuracy: 0.7226\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9784\n",
      "Epoch 127: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2440 - accuracy: 0.9784 - val_loss: 2.2104 - val_accuracy: 0.6986\n",
      "Epoch 128/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9796\n",
      "Epoch 128: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2403 - accuracy: 0.9796 - val_loss: 1.6909 - val_accuracy: 0.7332\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9810\n",
      "Epoch 129: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2358 - accuracy: 0.9810 - val_loss: 1.7430 - val_accuracy: 0.7242\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9812\n",
      "Epoch 130: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2365 - accuracy: 0.9812 - val_loss: 1.6678 - val_accuracy: 0.7401\n",
      "Epoch 131/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2409 - accuracy: 0.9786\n",
      "Epoch 131: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2411 - accuracy: 0.9786 - val_loss: 1.5491 - val_accuracy: 0.7487\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9808\n",
      "Epoch 132: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2363 - accuracy: 0.9808 - val_loss: 1.4230 - val_accuracy: 0.7646\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2293 - accuracy: 0.9830\n",
      "Epoch 133: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2292 - accuracy: 0.9830 - val_loss: 1.5251 - val_accuracy: 0.7502\n",
      "Epoch 134/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9794\n",
      "Epoch 134: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2378 - accuracy: 0.9793 - val_loss: 1.5795 - val_accuracy: 0.7558\n",
      "Epoch 135/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2468 - accuracy: 0.9765\n",
      "Epoch 135: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2469 - accuracy: 0.9765 - val_loss: 1.7659 - val_accuracy: 0.7455\n",
      "Epoch 136/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9804\n",
      "Epoch 136: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2358 - accuracy: 0.9804 - val_loss: 1.6719 - val_accuracy: 0.7421\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.9822\n",
      "Epoch 137: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2306 - accuracy: 0.9822 - val_loss: 1.4884 - val_accuracy: 0.7642\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9794\n",
      "Epoch 138: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2387 - accuracy: 0.9794 - val_loss: 1.7924 - val_accuracy: 0.7383\n",
      "Epoch 139/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2429 - accuracy: 0.9782\n",
      "Epoch 139: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2432 - accuracy: 0.9781 - val_loss: 2.6039 - val_accuracy: 0.6496\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9808\n",
      "Epoch 140: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2349 - accuracy: 0.9808 - val_loss: 1.4413 - val_accuracy: 0.7579\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9830\n",
      "Epoch 141: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2281 - accuracy: 0.9830 - val_loss: 1.7397 - val_accuracy: 0.7465\n",
      "Epoch 142/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9832\n",
      "Epoch 142: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2272 - accuracy: 0.9832 - val_loss: 1.7151 - val_accuracy: 0.7436\n",
      "Epoch 143/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2386 - accuracy: 0.9792\n",
      "Epoch 143: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2387 - accuracy: 0.9792 - val_loss: 1.4636 - val_accuracy: 0.7685\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.9794\n",
      "Epoch 144: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2382 - accuracy: 0.9794 - val_loss: 1.6573 - val_accuracy: 0.7433\n",
      "Epoch 145/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9785\n",
      "Epoch 145: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2421 - accuracy: 0.9785 - val_loss: 1.4697 - val_accuracy: 0.7593\n",
      "Epoch 146/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9819\n",
      "Epoch 146: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2324 - accuracy: 0.9819 - val_loss: 1.5911 - val_accuracy: 0.7586\n",
      "Epoch 147/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9814\n",
      "Epoch 147: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2317 - accuracy: 0.9814 - val_loss: 1.7355 - val_accuracy: 0.7426\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9796\n",
      "Epoch 148: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2372 - accuracy: 0.9796 - val_loss: 1.7496 - val_accuracy: 0.7289\n",
      "Epoch 149/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2409 - accuracy: 0.9790\n",
      "Epoch 149: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2409 - accuracy: 0.9790 - val_loss: 1.4052 - val_accuracy: 0.7685\n",
      "Epoch 150/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9834\n",
      "Epoch 150: val_accuracy did not improve from 0.77850\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2272 - accuracy: 0.9834 - val_loss: 1.4099 - val_accuracy: 0.7709\n",
      " Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Run the code\n",
    "# --------------------------\n",
    "run_pruning_training(sparsity=0.9, epochs=150, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8680b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
