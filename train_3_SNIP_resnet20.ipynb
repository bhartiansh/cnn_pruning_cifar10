{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaa0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from models.resnet20 import build_resnet20\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# -----------------------------\n",
    "# SNIP Pruning Function\n",
    "# -----------------------------\n",
    "def snip_prune(model, x_batch, y_batch, sparsity):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x_batch, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_batch, preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    snip_scores = [tf.abs(w * g) for w, g in zip(model.trainable_variables, grads) if g is not None and 'kernel' in w.name]\n",
    "    all_scores = tf.concat([tf.reshape(score, [-1]) for score in snip_scores], axis=0)\n",
    "\n",
    "    k = int((1 - sparsity) * tf.size(all_scores).numpy())\n",
    "    threshold = tf.sort(all_scores)[k].numpy()\n",
    "\n",
    "    masks = [(tf.abs(score) > threshold).numpy().astype(np.float32) for score in snip_scores]\n",
    "    mask_idx = 0\n",
    "\n",
    "    for i, var in enumerate(model.trainable_variables):\n",
    "        if 'kernel' in var.name:\n",
    "            var.assign(var * masks[mask_idx])\n",
    "            mask_idx += 1\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def run_snip_training(sparsity=0.5, batch_size=128, epochs=150):\n",
    "    # Load and preprocess data\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "    y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "    # Build and initialize model\n",
    "    model = build_resnet20()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    # SNIP pruning on small batch\n",
    "    x_sample = x_train[:batch_size]\n",
    "    y_sample = y_train[:batch_size]\n",
    "    snip_prune(model, x_sample, y_sample, sparsity)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Prepare saving\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    checkpoint_path = f'results/pruned_resnet20_SNIP_sparsity_{sparsity}.h5'\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[checkpoint_callback],\n",
    "                        verbose=1)\n",
    "\n",
    "    print(\"Training complete. Best model saved.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f7ab000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 64)          0           ['activation_18[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5359 - accuracy: 0.4822\n",
      "Epoch 1: val_accuracy improved from -inf to 0.46940, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 35s 82ms/step - loss: 1.5359 - accuracy: 0.4822 - val_loss: 1.6513 - val_accuracy: 0.4694\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0984 - accuracy: 0.6465\n",
      "Epoch 2: val_accuracy improved from 0.46940 to 0.50230, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 1.0984 - accuracy: 0.6465 - val_loss: 1.7595 - val_accuracy: 0.5023\n",
      "Epoch 3/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.7138\n",
      "Epoch 3: val_accuracy improved from 0.50230 to 0.65440, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.9190 - accuracy: 0.7139 - val_loss: 1.0959 - val_accuracy: 0.6544\n",
      "Epoch 4/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8048 - accuracy: 0.7557\n",
      "Epoch 4: val_accuracy improved from 0.65440 to 0.67970, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.8044 - accuracy: 0.7558 - val_loss: 1.0231 - val_accuracy: 0.6797\n",
      "Epoch 5/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7226 - accuracy: 0.7844\n",
      "Epoch 5: val_accuracy did not improve from 0.67970\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.7224 - accuracy: 0.7845 - val_loss: 1.6004 - val_accuracy: 0.5546\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.8140\n",
      "Epoch 6: val_accuracy improved from 0.67970 to 0.69210, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.6505 - accuracy: 0.8140 - val_loss: 1.0572 - val_accuracy: 0.6921\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5994 - accuracy: 0.8338\n",
      "Epoch 7: val_accuracy improved from 0.69210 to 0.70210, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.5994 - accuracy: 0.8338 - val_loss: 1.0387 - val_accuracy: 0.7021\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.8534\n",
      "Epoch 8: val_accuracy improved from 0.70210 to 0.70470, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.5492 - accuracy: 0.8534 - val_loss: 1.1434 - val_accuracy: 0.7047\n",
      "Epoch 9/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5045 - accuracy: 0.8697\n",
      "Epoch 9: val_accuracy improved from 0.70470 to 0.70560, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.5045 - accuracy: 0.8697 - val_loss: 1.1053 - val_accuracy: 0.7056\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4628 - accuracy: 0.8845\n",
      "Epoch 10: val_accuracy did not improve from 0.70560\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.4628 - accuracy: 0.8845 - val_loss: 1.6130 - val_accuracy: 0.6524\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8959\n",
      "Epoch 11: val_accuracy did not improve from 0.70560\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.4336 - accuracy: 0.8959 - val_loss: 1.2485 - val_accuracy: 0.7047\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.9074\n",
      "Epoch 12: val_accuracy did not improve from 0.70560\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.4032 - accuracy: 0.9074 - val_loss: 1.4038 - val_accuracy: 0.6913\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3739 - accuracy: 0.9189\n",
      "Epoch 13: val_accuracy did not improve from 0.70560\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3741 - accuracy: 0.9188 - val_loss: 1.4086 - val_accuracy: 0.6855\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.9287\n",
      "Epoch 14: val_accuracy improved from 0.70560 to 0.71460, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3514 - accuracy: 0.9287 - val_loss: 1.2952 - val_accuracy: 0.7146\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.9353\n",
      "Epoch 15: val_accuracy did not improve from 0.71460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3377 - accuracy: 0.9353 - val_loss: 1.2893 - val_accuracy: 0.7103\n",
      "Epoch 16/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3174 - accuracy: 0.9430\n",
      "Epoch 16: val_accuracy did not improve from 0.71460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3173 - accuracy: 0.9430 - val_loss: 1.3847 - val_accuracy: 0.6975\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9474\n",
      "Epoch 17: val_accuracy did not improve from 0.71460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3098 - accuracy: 0.9474 - val_loss: 1.7136 - val_accuracy: 0.6853\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9470\n",
      "Epoch 18: val_accuracy did not improve from 0.71460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.3075 - accuracy: 0.9470 - val_loss: 2.0861 - val_accuracy: 0.6294\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9535\n",
      "Epoch 19: val_accuracy did not improve from 0.71460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2955 - accuracy: 0.9535 - val_loss: 1.5160 - val_accuracy: 0.7019\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.9542\n",
      "Epoch 20: val_accuracy improved from 0.71460 to 0.73880, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.2929 - accuracy: 0.9542 - val_loss: 1.2321 - val_accuracy: 0.7388\n",
      "Epoch 21/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2947 - accuracy: 0.9554\n",
      "Epoch 21: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2947 - accuracy: 0.9554 - val_loss: 2.0530 - val_accuracy: 0.6549\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9599\n",
      "Epoch 22: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2829 - accuracy: 0.9599 - val_loss: 1.5522 - val_accuracy: 0.7156\n",
      "Epoch 23/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.9597\n",
      "Epoch 23: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2826 - accuracy: 0.9597 - val_loss: 1.6489 - val_accuracy: 0.7080\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9610\n",
      "Epoch 24: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2822 - accuracy: 0.9610 - val_loss: 2.3435 - val_accuracy: 0.6422\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9602\n",
      "Epoch 25: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2855 - accuracy: 0.9602 - val_loss: 1.6116 - val_accuracy: 0.7012\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.9618\n",
      "Epoch 26: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2817 - accuracy: 0.9618 - val_loss: 1.4554 - val_accuracy: 0.7273\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9681\n",
      "Epoch 27: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2672 - accuracy: 0.9681 - val_loss: 2.2765 - val_accuracy: 0.6377\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2807 - accuracy: 0.9634\n",
      "Epoch 28: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2807 - accuracy: 0.9634 - val_loss: 1.6964 - val_accuracy: 0.6995\n",
      "Epoch 29/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2768 - accuracy: 0.9655\n",
      "Epoch 29: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2768 - accuracy: 0.9655 - val_loss: 1.6740 - val_accuracy: 0.7018\n",
      "Epoch 30/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2653 - accuracy: 0.9697\n",
      "Epoch 30: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 78ms/step - loss: 0.2653 - accuracy: 0.9697 - val_loss: 2.0249 - val_accuracy: 0.6566\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2695 - accuracy: 0.9684\n",
      "Epoch 31: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2695 - accuracy: 0.9684 - val_loss: 1.7536 - val_accuracy: 0.7061\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9667\n",
      "Epoch 32: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2757 - accuracy: 0.9667 - val_loss: 2.0736 - val_accuracy: 0.6550\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.9659\n",
      "Epoch 33: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2798 - accuracy: 0.9659 - val_loss: 1.7033 - val_accuracy: 0.7226\n",
      "Epoch 34/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2650 - accuracy: 0.9719\n",
      "Epoch 34: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2650 - accuracy: 0.9719 - val_loss: 2.2239 - val_accuracy: 0.6284\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9681\n",
      "Epoch 35: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2729 - accuracy: 0.9681 - val_loss: 1.9139 - val_accuracy: 0.7068\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9709\n",
      "Epoch 36: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2670 - accuracy: 0.9709 - val_loss: 2.0362 - val_accuracy: 0.6808\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9684\n",
      "Epoch 37: val_accuracy did not improve from 0.73880\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2739 - accuracy: 0.9684 - val_loss: 1.5861 - val_accuracy: 0.7332\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9683\n",
      "Epoch 38: val_accuracy improved from 0.73880 to 0.74080, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2749 - accuracy: 0.9683 - val_loss: 1.5899 - val_accuracy: 0.7408\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9741\n",
      "Epoch 39: val_accuracy did not improve from 0.74080\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2600 - accuracy: 0.9741 - val_loss: 1.5535 - val_accuracy: 0.7253\n",
      "Epoch 40/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.9732\n",
      "Epoch 40: val_accuracy did not improve from 0.74080\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2657 - accuracy: 0.9732 - val_loss: 2.6050 - val_accuracy: 0.6140\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2764 - accuracy: 0.9689\n",
      "Epoch 41: val_accuracy did not improve from 0.74080\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2764 - accuracy: 0.9689 - val_loss: 2.4454 - val_accuracy: 0.6497\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2739 - accuracy: 0.9694\n",
      "Epoch 42: val_accuracy improved from 0.74080 to 0.74460, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2739 - accuracy: 0.9694 - val_loss: 1.4860 - val_accuracy: 0.7446\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9750\n",
      "Epoch 43: val_accuracy did not improve from 0.74460\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2622 - accuracy: 0.9750 - val_loss: 1.6141 - val_accuracy: 0.7357\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9706\n",
      "Epoch 44: val_accuracy did not improve from 0.74460\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2734 - accuracy: 0.9706 - val_loss: 1.7683 - val_accuracy: 0.7170\n",
      "Epoch 45/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9750\n",
      "Epoch 45: val_accuracy improved from 0.74460 to 0.75480, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2608 - accuracy: 0.9749 - val_loss: 1.3441 - val_accuracy: 0.7548\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9709\n",
      "Epoch 46: val_accuracy did not improve from 0.75480\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2724 - accuracy: 0.9709 - val_loss: 1.3698 - val_accuracy: 0.7476\n",
      "Epoch 47/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.9721\n",
      "Epoch 47: val_accuracy did not improve from 0.75480\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2702 - accuracy: 0.9720 - val_loss: 1.6413 - val_accuracy: 0.7231\n",
      "Epoch 48/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9756\n",
      "Epoch 48: val_accuracy improved from 0.75480 to 0.77270, saving model to results\\pruned_resnet20_SNIP_sparsity_0.3.h5\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2613 - accuracy: 0.9756 - val_loss: 1.2300 - val_accuracy: 0.7727\n",
      "Epoch 49/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9746\n",
      "Epoch 49: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2609 - accuracy: 0.9745 - val_loss: 1.8906 - val_accuracy: 0.7005\n",
      "Epoch 50/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2703 - accuracy: 0.9717\n",
      "Epoch 50: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2702 - accuracy: 0.9718 - val_loss: 1.6136 - val_accuracy: 0.7296\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9723\n",
      "Epoch 51: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2699 - accuracy: 0.9723 - val_loss: 1.5054 - val_accuracy: 0.7466\n",
      "Epoch 52/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9746\n",
      "Epoch 52: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2647 - accuracy: 0.9746 - val_loss: 1.8039 - val_accuracy: 0.7285\n",
      "Epoch 53/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9742\n",
      "Epoch 53: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2637 - accuracy: 0.9742 - val_loss: 1.4922 - val_accuracy: 0.7380\n",
      "Epoch 54/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.9751\n",
      "Epoch 54: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2615 - accuracy: 0.9750 - val_loss: 2.6653 - val_accuracy: 0.6585\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9748\n",
      "Epoch 55: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2645 - accuracy: 0.9748 - val_loss: 1.9991 - val_accuracy: 0.6980\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9722\n",
      "Epoch 56: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2711 - accuracy: 0.9722 - val_loss: 1.6223 - val_accuracy: 0.7302\n",
      "Epoch 57/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.9756\n",
      "Epoch 57: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2606 - accuracy: 0.9757 - val_loss: 1.8023 - val_accuracy: 0.7239\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9766\n",
      "Epoch 58: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2610 - accuracy: 0.9766 - val_loss: 1.5743 - val_accuracy: 0.7464\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9769\n",
      "Epoch 59: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2557 - accuracy: 0.9769 - val_loss: 1.7529 - val_accuracy: 0.7089\n",
      "Epoch 60/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2697 - accuracy: 0.9722\n",
      "Epoch 60: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2698 - accuracy: 0.9722 - val_loss: 1.6283 - val_accuracy: 0.7339\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.9748\n",
      "Epoch 61: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2643 - accuracy: 0.9748 - val_loss: 1.5044 - val_accuracy: 0.7505\n",
      "Epoch 62/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.9768\n",
      "Epoch 62: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2561 - accuracy: 0.9768 - val_loss: 1.5156 - val_accuracy: 0.7510\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9755\n",
      "Epoch 63: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2610 - accuracy: 0.9755 - val_loss: 1.5105 - val_accuracy: 0.7587\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9739\n",
      "Epoch 64: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2680 - accuracy: 0.9739 - val_loss: 1.6015 - val_accuracy: 0.7294\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9786\n",
      "Epoch 65: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2539 - accuracy: 0.9786 - val_loss: 1.8845 - val_accuracy: 0.7144\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9753\n",
      "Epoch 66: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2638 - accuracy: 0.9753 - val_loss: 1.5767 - val_accuracy: 0.7355\n",
      "Epoch 67/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9770\n",
      "Epoch 67: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2598 - accuracy: 0.9769 - val_loss: 1.5283 - val_accuracy: 0.7392\n",
      "Epoch 68/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9743\n",
      "Epoch 68: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2645 - accuracy: 0.9744 - val_loss: 1.7265 - val_accuracy: 0.7422\n",
      "Epoch 69/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9760\n",
      "Epoch 69: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2604 - accuracy: 0.9760 - val_loss: 1.4778 - val_accuracy: 0.7601\n",
      "Epoch 70/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.9778\n",
      "Epoch 70: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2544 - accuracy: 0.9778 - val_loss: 1.8624 - val_accuracy: 0.7129\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9783\n",
      "Epoch 71: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2542 - accuracy: 0.9783 - val_loss: 1.7460 - val_accuracy: 0.7300\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9764\n",
      "Epoch 72: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2579 - accuracy: 0.9764 - val_loss: 1.8595 - val_accuracy: 0.7221\n",
      "Epoch 73/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.9754\n",
      "Epoch 73: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2627 - accuracy: 0.9753 - val_loss: 2.0120 - val_accuracy: 0.6957\n",
      "Epoch 74/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.9771\n",
      "Epoch 74: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2578 - accuracy: 0.9771 - val_loss: 1.6172 - val_accuracy: 0.7512\n",
      "Epoch 75/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.9763\n",
      "Epoch 75: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2582 - accuracy: 0.9763 - val_loss: 2.3253 - val_accuracy: 0.6929\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9783\n",
      "Epoch 76: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2559 - accuracy: 0.9783 - val_loss: 2.2401 - val_accuracy: 0.6705\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9769\n",
      "Epoch 77: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2580 - accuracy: 0.9769 - val_loss: 1.6431 - val_accuracy: 0.7360\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9780\n",
      "Epoch 78: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2554 - accuracy: 0.9780 - val_loss: 1.7741 - val_accuracy: 0.7308\n",
      "Epoch 79/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9760\n",
      "Epoch 79: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2610 - accuracy: 0.9759 - val_loss: 1.6041 - val_accuracy: 0.7476\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9792\n",
      "Epoch 80: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2509 - accuracy: 0.9792 - val_loss: 1.4955 - val_accuracy: 0.7594\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9744\n",
      "Epoch 81: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2612 - accuracy: 0.9744 - val_loss: 1.9276 - val_accuracy: 0.7158\n",
      "Epoch 82/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2583 - accuracy: 0.9770\n",
      "Epoch 82: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2583 - accuracy: 0.9770 - val_loss: 1.5556 - val_accuracy: 0.7532\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9807\n",
      "Epoch 83: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2486 - accuracy: 0.9807 - val_loss: 1.5326 - val_accuracy: 0.7458\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9771\n",
      "Epoch 84: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2571 - accuracy: 0.9771 - val_loss: 1.7811 - val_accuracy: 0.7064\n",
      "Epoch 85/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9759\n",
      "Epoch 85: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2613 - accuracy: 0.9760 - val_loss: 1.8089 - val_accuracy: 0.7205\n",
      "Epoch 86/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9769\n",
      "Epoch 86: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2568 - accuracy: 0.9769 - val_loss: 1.6976 - val_accuracy: 0.7137\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9802\n",
      "Epoch 87: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2478 - accuracy: 0.9802 - val_loss: 1.5386 - val_accuracy: 0.7420\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9782\n",
      "Epoch 88: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2551 - accuracy: 0.9782 - val_loss: 1.5911 - val_accuracy: 0.7321\n",
      "Epoch 89/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.9777\n",
      "Epoch 89: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2555 - accuracy: 0.9777 - val_loss: 1.4597 - val_accuracy: 0.7595\n",
      "Epoch 90/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9776\n",
      "Epoch 90: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2546 - accuracy: 0.9776 - val_loss: 1.5664 - val_accuracy: 0.7474\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9788\n",
      "Epoch 91: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2517 - accuracy: 0.9788 - val_loss: 1.7286 - val_accuracy: 0.7405\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9779\n",
      "Epoch 92: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2550 - accuracy: 0.9779 - val_loss: 1.7070 - val_accuracy: 0.7508\n",
      "Epoch 93/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9797\n",
      "Epoch 93: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2496 - accuracy: 0.9797 - val_loss: 2.0015 - val_accuracy: 0.6979\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2461 - accuracy: 0.9803\n",
      "Epoch 94: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2461 - accuracy: 0.9803 - val_loss: 1.9104 - val_accuracy: 0.7114\n",
      "Epoch 95/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.9760\n",
      "Epoch 95: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2591 - accuracy: 0.9760 - val_loss: 1.8143 - val_accuracy: 0.7332\n",
      "Epoch 96/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9785\n",
      "Epoch 96: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2514 - accuracy: 0.9784 - val_loss: 1.3831 - val_accuracy: 0.7692\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9771\n",
      "Epoch 97: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2558 - accuracy: 0.9771 - val_loss: 1.6177 - val_accuracy: 0.7386\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2467 - accuracy: 0.9804\n",
      "Epoch 98: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2467 - accuracy: 0.9804 - val_loss: 2.1887 - val_accuracy: 0.6807\n",
      "Epoch 99/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9775\n",
      "Epoch 99: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2543 - accuracy: 0.9776 - val_loss: 1.6029 - val_accuracy: 0.7319\n",
      "Epoch 100/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9783\n",
      "Epoch 100: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2494 - accuracy: 0.9783 - val_loss: 1.7482 - val_accuracy: 0.7353\n",
      "Epoch 101/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9777\n",
      "Epoch 101: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2507 - accuracy: 0.9777 - val_loss: 1.6295 - val_accuracy: 0.7466\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9791\n",
      "Epoch 102: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2471 - accuracy: 0.9791 - val_loss: 1.7031 - val_accuracy: 0.7414\n",
      "Epoch 103/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9780\n",
      "Epoch 103: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2502 - accuracy: 0.9780 - val_loss: 1.5574 - val_accuracy: 0.7525\n",
      "Epoch 104/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9781\n",
      "Epoch 104: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2495 - accuracy: 0.9780 - val_loss: 1.7359 - val_accuracy: 0.7397\n",
      "Epoch 105/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9767\n",
      "Epoch 105: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2548 - accuracy: 0.9767 - val_loss: 2.1361 - val_accuracy: 0.7029\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9781\n",
      "Epoch 106: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2512 - accuracy: 0.9781 - val_loss: 1.6923 - val_accuracy: 0.7372\n",
      "Epoch 107/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9808\n",
      "Epoch 107: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2431 - accuracy: 0.9808 - val_loss: 1.4607 - val_accuracy: 0.7542\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9834\n",
      "Epoch 108: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2369 - accuracy: 0.9834 - val_loss: 1.9950 - val_accuracy: 0.7047\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9729\n",
      "Epoch 109: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2653 - accuracy: 0.9729 - val_loss: 1.7599 - val_accuracy: 0.7152\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9804\n",
      "Epoch 110: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2443 - accuracy: 0.9804 - val_loss: 1.4860 - val_accuracy: 0.7568\n",
      "Epoch 111/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9841\n",
      "Epoch 111: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2337 - accuracy: 0.9841 - val_loss: 1.6666 - val_accuracy: 0.7143\n",
      "Epoch 112/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2544 - accuracy: 0.9763\n",
      "Epoch 112: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2544 - accuracy: 0.9763 - val_loss: 1.6496 - val_accuracy: 0.7395\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2520 - accuracy: 0.9771\n",
      "Epoch 113: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 0.2520 - accuracy: 0.9771 - val_loss: 1.9647 - val_accuracy: 0.6892\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9786\n",
      "Epoch 114: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2508 - accuracy: 0.9786 - val_loss: 1.9270 - val_accuracy: 0.7118\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.9830\n",
      "Epoch 115: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2370 - accuracy: 0.9830 - val_loss: 1.4918 - val_accuracy: 0.7664\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9767\n",
      "Epoch 116: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2537 - accuracy: 0.9767 - val_loss: 1.9143 - val_accuracy: 0.7018\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9804\n",
      "Epoch 117: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2431 - accuracy: 0.9804 - val_loss: 2.2086 - val_accuracy: 0.6959\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9786\n",
      "Epoch 118: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2485 - accuracy: 0.9786 - val_loss: 1.5089 - val_accuracy: 0.7590\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9796\n",
      "Epoch 119: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2462 - accuracy: 0.9796 - val_loss: 1.6759 - val_accuracy: 0.7405\n",
      "Epoch 120/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9758\n",
      "Epoch 120: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2568 - accuracy: 0.9758 - val_loss: 1.3979 - val_accuracy: 0.7638\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.9810\n",
      "Epoch 121: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2402 - accuracy: 0.9810 - val_loss: 1.5750 - val_accuracy: 0.7674\n",
      "Epoch 122/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2424 - accuracy: 0.9810\n",
      "Epoch 122: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2424 - accuracy: 0.9810 - val_loss: 2.0959 - val_accuracy: 0.6941\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9802\n",
      "Epoch 123: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2419 - accuracy: 0.9802 - val_loss: 2.3419 - val_accuracy: 0.6816\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9767\n",
      "Epoch 124: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2518 - accuracy: 0.9767 - val_loss: 2.1181 - val_accuracy: 0.6826\n",
      "Epoch 125/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9793\n",
      "Epoch 125: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2445 - accuracy: 0.9793 - val_loss: 1.5342 - val_accuracy: 0.7593\n",
      "Epoch 126/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9822\n",
      "Epoch 126: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2387 - accuracy: 0.9822 - val_loss: 1.4029 - val_accuracy: 0.7672\n",
      "Epoch 127/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9781\n",
      "Epoch 127: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2466 - accuracy: 0.9781 - val_loss: 1.6878 - val_accuracy: 0.7375\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9780\n",
      "Epoch 128: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2464 - accuracy: 0.9780 - val_loss: 1.4240 - val_accuracy: 0.7556\n",
      "Epoch 129/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2478 - accuracy: 0.9784\n",
      "Epoch 129: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2478 - accuracy: 0.9785 - val_loss: 1.6305 - val_accuracy: 0.7508\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9817\n",
      "Epoch 130: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2378 - accuracy: 0.9817 - val_loss: 1.7053 - val_accuracy: 0.7436\n",
      "Epoch 131/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9791\n",
      "Epoch 131: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2466 - accuracy: 0.9791 - val_loss: 1.8006 - val_accuracy: 0.7400\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9809\n",
      "Epoch 132: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2407 - accuracy: 0.9809 - val_loss: 1.9307 - val_accuracy: 0.7278\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9798\n",
      "Epoch 133: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2426 - accuracy: 0.9797 - val_loss: 1.5618 - val_accuracy: 0.7575\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9813\n",
      "Epoch 134: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2396 - accuracy: 0.9813 - val_loss: 1.7636 - val_accuracy: 0.7268\n",
      "Epoch 135/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9821\n",
      "Epoch 135: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2356 - accuracy: 0.9821 - val_loss: 1.8417 - val_accuracy: 0.7259\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9771\n",
      "Epoch 136: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2499 - accuracy: 0.9771 - val_loss: 1.6716 - val_accuracy: 0.7446\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9791\n",
      "Epoch 137: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2440 - accuracy: 0.9791 - val_loss: 1.5766 - val_accuracy: 0.7463\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9811\n",
      "Epoch 138: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2379 - accuracy: 0.9810 - val_loss: 1.6186 - val_accuracy: 0.7471\n",
      "Epoch 139/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9826\n",
      "Epoch 139: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2356 - accuracy: 0.9826 - val_loss: 2.1334 - val_accuracy: 0.6863\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9766\n",
      "Epoch 140: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2504 - accuracy: 0.9766 - val_loss: 1.7952 - val_accuracy: 0.7306\n",
      "Epoch 141/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9812\n",
      "Epoch 141: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2379 - accuracy: 0.9812 - val_loss: 1.4649 - val_accuracy: 0.7588\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2446 - accuracy: 0.9789\n",
      "Epoch 142: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2446 - accuracy: 0.9789 - val_loss: 1.8030 - val_accuracy: 0.7214\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9777\n",
      "Epoch 143: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2481 - accuracy: 0.9777 - val_loss: 1.6576 - val_accuracy: 0.7495\n",
      "Epoch 144/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9844\n",
      "Epoch 144: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2279 - accuracy: 0.9844 - val_loss: 1.6375 - val_accuracy: 0.7473\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9801\n",
      "Epoch 145: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2394 - accuracy: 0.9801 - val_loss: 1.9404 - val_accuracy: 0.7247\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.9776\n",
      "Epoch 146: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2486 - accuracy: 0.9776 - val_loss: 2.4542 - val_accuracy: 0.6528\n",
      "Epoch 147/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9832\n",
      "Epoch 147: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2329 - accuracy: 0.9832 - val_loss: 1.8679 - val_accuracy: 0.7158\n",
      "Epoch 148/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9828\n",
      "Epoch 148: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2321 - accuracy: 0.9827 - val_loss: 1.8162 - val_accuracy: 0.7191\n",
      "Epoch 149/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9786\n",
      "Epoch 149: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2427 - accuracy: 0.9786 - val_loss: 1.5792 - val_accuracy: 0.7581\n",
      "Epoch 150/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9776\n",
      "Epoch 150: val_accuracy did not improve from 0.77270\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2480 - accuracy: 0.9776 - val_loss: 1.7076 - val_accuracy: 0.7235\n",
      "Training complete. Best model saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b5ba49e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.3, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e800df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 16)  64          ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 32, 32, 16)  64          ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 32, 32, 16)  64          ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 32, 16)   0           ['activation_19[0][0]',          \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 32, 32, 16)   0           ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 32, 32, 16)  64          ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 32, 32, 16)  64          ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 32, 32, 16)   0           ['activation_21[0][0]',          \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 16)   0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 32, 32, 16)   0           ['activation_23[0][0]',          \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 16)   0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 16, 16, 32)  128         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 16, 16, 32)   544         ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['conv2d_30[0][0]',              \n",
      "                                                                  'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 16, 16, 32)   0           ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 16, 16, 32)  128         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 16, 16, 32)  128         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['activation_27[0][0]',          \n",
      "                                                                  'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 16, 16, 32)   0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 16, 16, 32)  128         ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 16, 16, 32)  128         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 32)   0           ['activation_29[0][0]',          \n",
      "                                                                  'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 64)    256         ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['conv2d_37[0][0]',              \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 64)     0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 64)    256         ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['activation_33[0][0]',          \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 64)     0           ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 64)    256         ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['activation_35[0][0]',          \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 64)     0           ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 64)          0           ['activation_37[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5869 - accuracy: 0.4745\n",
      "Epoch 1: val_accuracy improved from -inf to 0.49320, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 32s 76ms/step - loss: 1.5869 - accuracy: 0.4745 - val_loss: 1.5475 - val_accuracy: 0.4932\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1853 - accuracy: 0.6213\n",
      "Epoch 2: val_accuracy improved from 0.49320 to 0.58220, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.1853 - accuracy: 0.6213 - val_loss: 1.3171 - val_accuracy: 0.5822\n",
      "Epoch 3/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0167 - accuracy: 0.6801\n",
      "Epoch 3: val_accuracy improved from 0.58220 to 0.61710, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.0170 - accuracy: 0.6801 - val_loss: 1.1901 - val_accuracy: 0.6171\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8923 - accuracy: 0.7298\n",
      "Epoch 4: val_accuracy did not improve from 0.61710\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.8923 - accuracy: 0.7298 - val_loss: 1.3679 - val_accuracy: 0.5871\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7958 - accuracy: 0.7658\n",
      "Epoch 5: val_accuracy did not improve from 0.61710\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.7958 - accuracy: 0.7658 - val_loss: 1.2938 - val_accuracy: 0.6015\n",
      "Epoch 6/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.7191 - accuracy: 0.7946\n",
      "Epoch 6: val_accuracy improved from 0.61710 to 0.70740, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.7190 - accuracy: 0.7947 - val_loss: 0.9987 - val_accuracy: 0.7074\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.8186\n",
      "Epoch 7: val_accuracy did not improve from 0.70740\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.6511 - accuracy: 0.8186 - val_loss: 1.1546 - val_accuracy: 0.6718\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5918 - accuracy: 0.8411\n",
      "Epoch 8: val_accuracy did not improve from 0.70740\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.5918 - accuracy: 0.8411 - val_loss: 1.4063 - val_accuracy: 0.6350\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.8576\n",
      "Epoch 9: val_accuracy improved from 0.70740 to 0.72250, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.5456 - accuracy: 0.8575 - val_loss: 1.0501 - val_accuracy: 0.7225\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5028 - accuracy: 0.8748\n",
      "Epoch 10: val_accuracy did not improve from 0.72250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.5027 - accuracy: 0.8748 - val_loss: 1.1587 - val_accuracy: 0.6883\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.8874\n",
      "Epoch 11: val_accuracy did not improve from 0.72250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.4712 - accuracy: 0.8874 - val_loss: 1.1242 - val_accuracy: 0.7164\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.8995\n",
      "Epoch 12: val_accuracy did not improve from 0.72250\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4373 - accuracy: 0.8995 - val_loss: 1.4483 - val_accuracy: 0.6535\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4074 - accuracy: 0.9115\n",
      "Epoch 13: val_accuracy did not improve from 0.72250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.4075 - accuracy: 0.9115 - val_loss: 1.3271 - val_accuracy: 0.6862\n",
      "Epoch 14/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.9228\n",
      "Epoch 14: val_accuracy did not improve from 0.72250\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3810 - accuracy: 0.9228 - val_loss: 1.6131 - val_accuracy: 0.6763\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.9293\n",
      "Epoch 15: val_accuracy improved from 0.72250 to 0.73550, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3604 - accuracy: 0.9293 - val_loss: 1.1387 - val_accuracy: 0.7355\n",
      "Epoch 16/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.9341\n",
      "Epoch 16: val_accuracy did not improve from 0.73550\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3506 - accuracy: 0.9340 - val_loss: 1.2715 - val_accuracy: 0.7219\n",
      "Epoch 17/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.9421\n",
      "Epoch 17: val_accuracy improved from 0.73550 to 0.73950, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3302 - accuracy: 0.9421 - val_loss: 1.1893 - val_accuracy: 0.7395\n",
      "Epoch 18/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.9467\n",
      "Epoch 18: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3205 - accuracy: 0.9466 - val_loss: 1.4964 - val_accuracy: 0.6954\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9530\n",
      "Epoch 19: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3073 - accuracy: 0.9530 - val_loss: 1.6473 - val_accuracy: 0.6942\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.9473\n",
      "Epoch 20: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3208 - accuracy: 0.9473 - val_loss: 1.3795 - val_accuracy: 0.7130\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.9570\n",
      "Epoch 21: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2977 - accuracy: 0.9569 - val_loss: 1.9821 - val_accuracy: 0.6456\n",
      "Epoch 22/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.9547\n",
      "Epoch 22: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3023 - accuracy: 0.9547 - val_loss: 1.8603 - val_accuracy: 0.6884\n",
      "Epoch 23/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9556\n",
      "Epoch 23: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3031 - accuracy: 0.9556 - val_loss: 1.5086 - val_accuracy: 0.7027\n",
      "Epoch 24/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9612\n",
      "Epoch 24: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2902 - accuracy: 0.9612 - val_loss: 1.3798 - val_accuracy: 0.7306\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9625\n",
      "Epoch 25: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2872 - accuracy: 0.9625 - val_loss: 1.7416 - val_accuracy: 0.6885\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9619\n",
      "Epoch 26: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2895 - accuracy: 0.9619 - val_loss: 2.1194 - val_accuracy: 0.6596\n",
      "Epoch 27/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2907 - accuracy: 0.9615\n",
      "Epoch 27: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2906 - accuracy: 0.9616 - val_loss: 1.6920 - val_accuracy: 0.7147\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9651\n",
      "Epoch 28: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2811 - accuracy: 0.9651 - val_loss: 2.1848 - val_accuracy: 0.6544\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9648\n",
      "Epoch 29: val_accuracy did not improve from 0.73950\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2847 - accuracy: 0.9648 - val_loss: 1.5654 - val_accuracy: 0.7293\n",
      "Epoch 30/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9640\n",
      "Epoch 30: val_accuracy improved from 0.73950 to 0.75070, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2881 - accuracy: 0.9640 - val_loss: 1.3428 - val_accuracy: 0.7507\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9656\n",
      "Epoch 31: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2844 - accuracy: 0.9656 - val_loss: 1.7012 - val_accuracy: 0.7092\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9669\n",
      "Epoch 32: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2845 - accuracy: 0.9669 - val_loss: 1.3946 - val_accuracy: 0.7358\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9656\n",
      "Epoch 33: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2847 - accuracy: 0.9656 - val_loss: 1.5852 - val_accuracy: 0.7168\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9691\n",
      "Epoch 34: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2783 - accuracy: 0.9691 - val_loss: 1.6265 - val_accuracy: 0.7340\n",
      "Epoch 35/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2823 - accuracy: 0.9677\n",
      "Epoch 35: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2822 - accuracy: 0.9678 - val_loss: 1.8177 - val_accuracy: 0.6976\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9697\n",
      "Epoch 36: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2775 - accuracy: 0.9697 - val_loss: 1.4924 - val_accuracy: 0.7423\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9694\n",
      "Epoch 37: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2783 - accuracy: 0.9694 - val_loss: 2.0844 - val_accuracy: 0.6823\n",
      "Epoch 38/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.9697\n",
      "Epoch 38: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2775 - accuracy: 0.9697 - val_loss: 2.3822 - val_accuracy: 0.6157\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.9679\n",
      "Epoch 39: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2840 - accuracy: 0.9679 - val_loss: 1.7025 - val_accuracy: 0.7111\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9708\n",
      "Epoch 40: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2763 - accuracy: 0.9708 - val_loss: 1.7961 - val_accuracy: 0.7267\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.9714\n",
      "Epoch 41: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2752 - accuracy: 0.9714 - val_loss: 2.3774 - val_accuracy: 0.6570\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2774 - accuracy: 0.9708\n",
      "Epoch 42: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2775 - accuracy: 0.9708 - val_loss: 1.9306 - val_accuracy: 0.6917\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9705\n",
      "Epoch 43: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2773 - accuracy: 0.9705 - val_loss: 1.6701 - val_accuracy: 0.7100\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9729\n",
      "Epoch 44: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2727 - accuracy: 0.9729 - val_loss: 1.8461 - val_accuracy: 0.7123\n",
      "Epoch 45/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.9721\n",
      "Epoch 45: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2729 - accuracy: 0.9721 - val_loss: 2.0820 - val_accuracy: 0.6818\n",
      "Epoch 46/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.9710\n",
      "Epoch 46: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2773 - accuracy: 0.9709 - val_loss: 1.6878 - val_accuracy: 0.7119\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9733\n",
      "Epoch 47: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2711 - accuracy: 0.9733 - val_loss: 1.7676 - val_accuracy: 0.6842\n",
      "Epoch 48/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2698 - accuracy: 0.9741\n",
      "Epoch 48: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2697 - accuracy: 0.9741 - val_loss: 1.8661 - val_accuracy: 0.7104\n",
      "Epoch 49/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.9717\n",
      "Epoch 49: val_accuracy did not improve from 0.75070\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2771 - accuracy: 0.9717 - val_loss: 1.6943 - val_accuracy: 0.7264\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.9718\n",
      "Epoch 50: val_accuracy improved from 0.75070 to 0.75530, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2771 - accuracy: 0.9718 - val_loss: 1.4345 - val_accuracy: 0.7553\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9738\n",
      "Epoch 51: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2714 - accuracy: 0.9738 - val_loss: 1.4409 - val_accuracy: 0.7493\n",
      "Epoch 52/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.9733\n",
      "Epoch 52: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2725 - accuracy: 0.9732 - val_loss: 1.6230 - val_accuracy: 0.7425\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.9722\n",
      "Epoch 53: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2773 - accuracy: 0.9722 - val_loss: 1.8122 - val_accuracy: 0.7139\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9771\n",
      "Epoch 54: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2614 - accuracy: 0.9771 - val_loss: 1.7824 - val_accuracy: 0.7160\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9707\n",
      "Epoch 55: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2796 - accuracy: 0.9707 - val_loss: 1.7257 - val_accuracy: 0.7099\n",
      "Epoch 56/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.9724\n",
      "Epoch 56: val_accuracy did not improve from 0.75530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2737 - accuracy: 0.9724 - val_loss: 1.7239 - val_accuracy: 0.7199\n",
      "Epoch 57/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9765\n",
      "Epoch 57: val_accuracy improved from 0.75530 to 0.76450, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2640 - accuracy: 0.9764 - val_loss: 1.4101 - val_accuracy: 0.7645\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9712\n",
      "Epoch 58: val_accuracy did not improve from 0.76450\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2791 - accuracy: 0.9712 - val_loss: 2.4489 - val_accuracy: 0.6638\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9763\n",
      "Epoch 59: val_accuracy did not improve from 0.76450\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2678 - accuracy: 0.9763 - val_loss: 2.1630 - val_accuracy: 0.6812\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9760\n",
      "Epoch 60: val_accuracy did not improve from 0.76450\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2647 - accuracy: 0.9760 - val_loss: 1.8803 - val_accuracy: 0.6858\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.9738\n",
      "Epoch 61: val_accuracy did not improve from 0.76450\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2715 - accuracy: 0.9738 - val_loss: 1.8757 - val_accuracy: 0.6912\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2685 - accuracy: 0.9752\n",
      "Epoch 62: val_accuracy did not improve from 0.76450\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2685 - accuracy: 0.9752 - val_loss: 1.6980 - val_accuracy: 0.7297\n",
      "Epoch 63/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9742\n",
      "Epoch 63: val_accuracy improved from 0.76450 to 0.76970, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2709 - accuracy: 0.9742 - val_loss: 1.3317 - val_accuracy: 0.7697\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9756\n",
      "Epoch 64: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2668 - accuracy: 0.9756 - val_loss: 1.5600 - val_accuracy: 0.7454\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9742\n",
      "Epoch 65: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2706 - accuracy: 0.9742 - val_loss: 1.6369 - val_accuracy: 0.7427\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9741\n",
      "Epoch 66: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2711 - accuracy: 0.9741 - val_loss: 1.5288 - val_accuracy: 0.7374\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9750\n",
      "Epoch 67: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2691 - accuracy: 0.9750 - val_loss: 1.8252 - val_accuracy: 0.7181\n",
      "Epoch 68/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.9774\n",
      "Epoch 68: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2598 - accuracy: 0.9772 - val_loss: 1.5370 - val_accuracy: 0.7375\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9753\n",
      "Epoch 69: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2666 - accuracy: 0.9753 - val_loss: 1.5710 - val_accuracy: 0.7338\n",
      "Epoch 70/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.9761\n",
      "Epoch 70: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2658 - accuracy: 0.9761 - val_loss: 1.9018 - val_accuracy: 0.7235\n",
      "Epoch 71/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2778 - accuracy: 0.9715\n",
      "Epoch 71: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2780 - accuracy: 0.9715 - val_loss: 1.6454 - val_accuracy: 0.7293\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.9771\n",
      "Epoch 72: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2630 - accuracy: 0.9770 - val_loss: 2.0688 - val_accuracy: 0.7121\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9784\n",
      "Epoch 73: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2574 - accuracy: 0.9784 - val_loss: 1.4422 - val_accuracy: 0.7601\n",
      "Epoch 74/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9768\n",
      "Epoch 74: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2620 - accuracy: 0.9768 - val_loss: 2.0434 - val_accuracy: 0.7175\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9730\n",
      "Epoch 75: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2745 - accuracy: 0.9730 - val_loss: 2.0087 - val_accuracy: 0.7153\n",
      "Epoch 76/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2651 - accuracy: 0.9760\n",
      "Epoch 76: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2651 - accuracy: 0.9760 - val_loss: 1.4868 - val_accuracy: 0.7413\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.9757\n",
      "Epoch 77: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2644 - accuracy: 0.9757 - val_loss: 2.4790 - val_accuracy: 0.6670\n",
      "Epoch 78/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.9762\n",
      "Epoch 78: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2656 - accuracy: 0.9762 - val_loss: 2.1044 - val_accuracy: 0.6728\n",
      "Epoch 79/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2613 - accuracy: 0.9775\n",
      "Epoch 79: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2613 - accuracy: 0.9775 - val_loss: 1.7814 - val_accuracy: 0.7198\n",
      "Epoch 80/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9777\n",
      "Epoch 80: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2610 - accuracy: 0.9777 - val_loss: 2.0728 - val_accuracy: 0.7051\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9771\n",
      "Epoch 81: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2625 - accuracy: 0.9771 - val_loss: 1.5844 - val_accuracy: 0.7510\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9768\n",
      "Epoch 82: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2635 - accuracy: 0.9768 - val_loss: 1.5511 - val_accuracy: 0.7490\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9782\n",
      "Epoch 83: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2580 - accuracy: 0.9782 - val_loss: 2.3278 - val_accuracy: 0.6832\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9744\n",
      "Epoch 84: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2675 - accuracy: 0.9744 - val_loss: 1.3814 - val_accuracy: 0.7572\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9796\n",
      "Epoch 85: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2542 - accuracy: 0.9796 - val_loss: 1.7595 - val_accuracy: 0.7249\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9767\n",
      "Epoch 86: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2619 - accuracy: 0.9767 - val_loss: 2.2345 - val_accuracy: 0.6800\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9746\n",
      "Epoch 87: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2667 - accuracy: 0.9746 - val_loss: 1.9382 - val_accuracy: 0.7113\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9788\n",
      "Epoch 88: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2569 - accuracy: 0.9788 - val_loss: 1.7587 - val_accuracy: 0.7244\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9778\n",
      "Epoch 89: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2603 - accuracy: 0.9778 - val_loss: 1.4379 - val_accuracy: 0.7515\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.9789\n",
      "Epoch 90: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2556 - accuracy: 0.9789 - val_loss: 1.4001 - val_accuracy: 0.7667\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9789\n",
      "Epoch 91: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2549 - accuracy: 0.9789 - val_loss: 3.0525 - val_accuracy: 0.6194\n",
      "Epoch 92/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9768\n",
      "Epoch 92: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2621 - accuracy: 0.9768 - val_loss: 1.9527 - val_accuracy: 0.7129\n",
      "Epoch 93/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.9752\n",
      "Epoch 93: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2653 - accuracy: 0.9752 - val_loss: 1.8653 - val_accuracy: 0.6957\n",
      "Epoch 94/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2574 - accuracy: 0.9781\n",
      "Epoch 94: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2575 - accuracy: 0.9781 - val_loss: 1.5115 - val_accuracy: 0.7585\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.9793\n",
      "Epoch 95: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2531 - accuracy: 0.9793 - val_loss: 1.9987 - val_accuracy: 0.7097\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9784\n",
      "Epoch 96: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2565 - accuracy: 0.9784 - val_loss: 1.4782 - val_accuracy: 0.7529\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9761\n",
      "Epoch 97: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2627 - accuracy: 0.9761 - val_loss: 1.6195 - val_accuracy: 0.7475\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9788\n",
      "Epoch 98: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2542 - accuracy: 0.9788 - val_loss: 1.6236 - val_accuracy: 0.7471\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9797\n",
      "Epoch 99: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2513 - accuracy: 0.9797 - val_loss: 2.0552 - val_accuracy: 0.6867\n",
      "Epoch 100/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9778\n",
      "Epoch 100: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2569 - accuracy: 0.9777 - val_loss: 2.7247 - val_accuracy: 0.6074\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9784\n",
      "Epoch 101: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2538 - accuracy: 0.9784 - val_loss: 1.4701 - val_accuracy: 0.7557\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.9756\n",
      "Epoch 102: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2640 - accuracy: 0.9755 - val_loss: 1.5077 - val_accuracy: 0.7436\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9765\n",
      "Epoch 103: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2594 - accuracy: 0.9765 - val_loss: 1.6158 - val_accuracy: 0.7409\n",
      "Epoch 104/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2461 - accuracy: 0.9813\n",
      "Epoch 104: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2461 - accuracy: 0.9813 - val_loss: 1.5395 - val_accuracy: 0.7510\n",
      "Epoch 105/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9788\n",
      "Epoch 105: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2515 - accuracy: 0.9788 - val_loss: 1.6416 - val_accuracy: 0.7407\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9760\n",
      "Epoch 106: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2595 - accuracy: 0.9760 - val_loss: 1.6391 - val_accuracy: 0.7256\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9786\n",
      "Epoch 107: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2550 - accuracy: 0.9786 - val_loss: 1.4906 - val_accuracy: 0.7640\n",
      "Epoch 108/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9801\n",
      "Epoch 108: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2497 - accuracy: 0.9801 - val_loss: 1.5154 - val_accuracy: 0.7516\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9757\n",
      "Epoch 109: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2620 - accuracy: 0.9757 - val_loss: 1.8182 - val_accuracy: 0.7389\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2462 - accuracy: 0.9813\n",
      "Epoch 110: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2462 - accuracy: 0.9813 - val_loss: 2.1112 - val_accuracy: 0.7032\n",
      "Epoch 111/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9774\n",
      "Epoch 111: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2525 - accuracy: 0.9774 - val_loss: 1.5173 - val_accuracy: 0.7474\n",
      "Epoch 112/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9765\n",
      "Epoch 112: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2591 - accuracy: 0.9764 - val_loss: 1.7682 - val_accuracy: 0.7138\n",
      "Epoch 113/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9793\n",
      "Epoch 113: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2516 - accuracy: 0.9793 - val_loss: 1.5446 - val_accuracy: 0.7426\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9785\n",
      "Epoch 114: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2524 - accuracy: 0.9785 - val_loss: 1.8725 - val_accuracy: 0.7160\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9803\n",
      "Epoch 115: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2484 - accuracy: 0.9803 - val_loss: 1.5011 - val_accuracy: 0.7515\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9767\n",
      "Epoch 116: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2579 - accuracy: 0.9767 - val_loss: 1.6321 - val_accuracy: 0.7449\n",
      "Epoch 117/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9772\n",
      "Epoch 117: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2546 - accuracy: 0.9772 - val_loss: 1.9538 - val_accuracy: 0.7271\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2441 - accuracy: 0.9815\n",
      "Epoch 118: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2441 - accuracy: 0.9815 - val_loss: 1.6277 - val_accuracy: 0.7438\n",
      "Epoch 119/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9802\n",
      "Epoch 119: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2474 - accuracy: 0.9801 - val_loss: 2.1403 - val_accuracy: 0.7068\n",
      "Epoch 120/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9811\n",
      "Epoch 120: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2449 - accuracy: 0.9811 - val_loss: 1.7799 - val_accuracy: 0.7283\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9781\n",
      "Epoch 121: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2508 - accuracy: 0.9781 - val_loss: 1.5196 - val_accuracy: 0.7666\n",
      "Epoch 122/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9763\n",
      "Epoch 122: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2587 - accuracy: 0.9764 - val_loss: 1.4564 - val_accuracy: 0.7522\n",
      "Epoch 123/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9782\n",
      "Epoch 123: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2555 - accuracy: 0.9782 - val_loss: 1.7029 - val_accuracy: 0.7146\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9826\n",
      "Epoch 124: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2407 - accuracy: 0.9826 - val_loss: 1.5507 - val_accuracy: 0.7621\n",
      "Epoch 125/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9804\n",
      "Epoch 125: val_accuracy did not improve from 0.76970\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2459 - accuracy: 0.9804 - val_loss: 1.5634 - val_accuracy: 0.7502\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9769\n",
      "Epoch 126: val_accuracy improved from 0.76970 to 0.77460, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2536 - accuracy: 0.9769 - val_loss: 1.4124 - val_accuracy: 0.7746\n",
      "Epoch 127/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9799\n",
      "Epoch 127: val_accuracy did not improve from 0.77460\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2473 - accuracy: 0.9799 - val_loss: 2.4123 - val_accuracy: 0.6810\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9794\n",
      "Epoch 128: val_accuracy did not improve from 0.77460\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2483 - accuracy: 0.9794 - val_loss: 1.4938 - val_accuracy: 0.7595\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9782\n",
      "Epoch 129: val_accuracy improved from 0.77460 to 0.77530, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2519 - accuracy: 0.9782 - val_loss: 1.5005 - val_accuracy: 0.7753\n",
      "Epoch 130/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9791\n",
      "Epoch 130: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2494 - accuracy: 0.9791 - val_loss: 1.4830 - val_accuracy: 0.7662\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9818\n",
      "Epoch 131: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2431 - accuracy: 0.9818 - val_loss: 1.6439 - val_accuracy: 0.7406\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9781\n",
      "Epoch 132: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2502 - accuracy: 0.9781 - val_loss: 1.5124 - val_accuracy: 0.7470\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2475 - accuracy: 0.9802\n",
      "Epoch 133: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2476 - accuracy: 0.9802 - val_loss: 1.7872 - val_accuracy: 0.7240\n",
      "Epoch 134/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.9774\n",
      "Epoch 134: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2513 - accuracy: 0.9774 - val_loss: 1.6419 - val_accuracy: 0.7475\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9809\n",
      "Epoch 135: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2474 - accuracy: 0.9809 - val_loss: 1.6580 - val_accuracy: 0.7342\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9826\n",
      "Epoch 136: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2387 - accuracy: 0.9826 - val_loss: 2.1459 - val_accuracy: 0.6753\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9780\n",
      "Epoch 137: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2506 - accuracy: 0.9780 - val_loss: 2.0700 - val_accuracy: 0.6945\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9782\n",
      "Epoch 138: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2513 - accuracy: 0.9782 - val_loss: 1.6863 - val_accuracy: 0.7393\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9811\n",
      "Epoch 139: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2421 - accuracy: 0.9811 - val_loss: 1.5440 - val_accuracy: 0.7584\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9793\n",
      "Epoch 140: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2493 - accuracy: 0.9793 - val_loss: 1.8885 - val_accuracy: 0.7185\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9791\n",
      "Epoch 141: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2479 - accuracy: 0.9791 - val_loss: 1.7000 - val_accuracy: 0.7236\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9819\n",
      "Epoch 142: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2396 - accuracy: 0.9819 - val_loss: 1.4336 - val_accuracy: 0.7635\n",
      "Epoch 143/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9811\n",
      "Epoch 143: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2404 - accuracy: 0.9811 - val_loss: 1.4070 - val_accuracy: 0.7742\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9779\n",
      "Epoch 144: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2503 - accuracy: 0.9779 - val_loss: 1.7887 - val_accuracy: 0.7309\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9766\n",
      "Epoch 145: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2533 - accuracy: 0.9766 - val_loss: 1.6265 - val_accuracy: 0.7474\n",
      "Epoch 146/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9818\n",
      "Epoch 146: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2404 - accuracy: 0.9817 - val_loss: 1.5133 - val_accuracy: 0.7574\n",
      "Epoch 147/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.9823\n",
      "Epoch 147: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2374 - accuracy: 0.9823 - val_loss: 1.9056 - val_accuracy: 0.7293\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9817\n",
      "Epoch 148: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2396 - accuracy: 0.9817 - val_loss: 1.9380 - val_accuracy: 0.7032\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9773\n",
      "Epoch 149: val_accuracy did not improve from 0.77530\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2496 - accuracy: 0.9773 - val_loss: 2.0033 - val_accuracy: 0.7296\n",
      "Epoch 150/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9800\n",
      "Epoch 150: val_accuracy improved from 0.77530 to 0.77870, saving model to results\\pruned_resnet20_SNIP_sparsity_0.5.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2454 - accuracy: 0.9800 - val_loss: 1.3895 - val_accuracy: 0.7787\n",
      "Training complete. Best model saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b54667b50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.5, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fdf4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 16)  64          ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 16)  64          ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 32, 32, 16)   0           ['activation_38[0][0]',          \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 32, 32, 16)   0           ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32, 32, 16)  64          ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 32, 32, 16)   0           ['activation_40[0][0]',          \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 32, 32, 16)   0           ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, 32, 16)  64          ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 32, 32, 16)   0           ['activation_42[0][0]',          \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 16)   0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 16, 16, 32)   544         ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 16, 16, 32)   0           ['conv2d_51[0][0]',              \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 32)   0           ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 32)  128         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 16, 16, 32)   0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 32)   0           ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 32)  128         ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 16, 16, 32)  128         ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 16, 16, 32)   0           ['activation_48[0][0]',          \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 16, 16, 32)   0           ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 64)     0           ['conv2d_58[0][0]',              \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 64)     0           ['activation_52[0][0]',          \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 64)     0           ['activation_54[0][0]',          \n",
      "                                                                  'batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 64)     0           ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 64)          0           ['activation_56[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           650         ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.5886 - accuracy: 0.4788\n",
      "Epoch 1: val_accuracy improved from -inf to 0.40030, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 31s 76ms/step - loss: 1.5882 - accuracy: 0.4789 - val_loss: 1.9903 - val_accuracy: 0.4003\n",
      "Epoch 2/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.1787 - accuracy: 0.6283\n",
      "Epoch 2: val_accuracy improved from 0.40030 to 0.53220, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.1789 - accuracy: 0.6283 - val_loss: 1.5149 - val_accuracy: 0.5322\n",
      "Epoch 3/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.9933 - accuracy: 0.6985\n",
      "Epoch 3: val_accuracy improved from 0.53220 to 0.57440, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.9931 - accuracy: 0.6985 - val_loss: 1.3978 - val_accuracy: 0.5744\n",
      "Epoch 4/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8714 - accuracy: 0.7418\n",
      "Epoch 4: val_accuracy improved from 0.57440 to 0.69630, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.8713 - accuracy: 0.7419 - val_loss: 1.0063 - val_accuracy: 0.6963\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7782 - accuracy: 0.7776\n",
      "Epoch 5: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.7782 - accuracy: 0.7776 - val_loss: 1.4777 - val_accuracy: 0.5906\n",
      "Epoch 6/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7023 - accuracy: 0.8041\n",
      "Epoch 6: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.7023 - accuracy: 0.8041 - val_loss: 1.1670 - val_accuracy: 0.6651\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6483 - accuracy: 0.8225\n",
      "Epoch 7: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6483 - accuracy: 0.8225 - val_loss: 1.8112 - val_accuracy: 0.5403\n",
      "Epoch 8/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5900 - accuracy: 0.8449\n",
      "Epoch 8: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.5899 - accuracy: 0.8449 - val_loss: 1.5296 - val_accuracy: 0.5707\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5452 - accuracy: 0.8617\n",
      "Epoch 9: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.5453 - accuracy: 0.8616 - val_loss: 1.2422 - val_accuracy: 0.6721\n",
      "Epoch 10/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4968 - accuracy: 0.8783\n",
      "Epoch 10: val_accuracy did not improve from 0.69630\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4968 - accuracy: 0.8783 - val_loss: 1.3632 - val_accuracy: 0.6693\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4704 - accuracy: 0.8900\n",
      "Epoch 11: val_accuracy improved from 0.69630 to 0.72770, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4704 - accuracy: 0.8900 - val_loss: 1.0323 - val_accuracy: 0.7277\n",
      "Epoch 12/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4311 - accuracy: 0.9036\n",
      "Epoch 12: val_accuracy did not improve from 0.72770\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4311 - accuracy: 0.9036 - val_loss: 1.3143 - val_accuracy: 0.6892\n",
      "Epoch 13/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.9130\n",
      "Epoch 13: val_accuracy did not improve from 0.72770\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4070 - accuracy: 0.9130 - val_loss: 1.2839 - val_accuracy: 0.6974\n",
      "Epoch 14/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.9223\n",
      "Epoch 14: val_accuracy did not improve from 0.72770\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3829 - accuracy: 0.9223 - val_loss: 1.2415 - val_accuracy: 0.7147\n",
      "Epoch 15/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3599 - accuracy: 0.9314\n",
      "Epoch 15: val_accuracy improved from 0.72770 to 0.73280, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3601 - accuracy: 0.9313 - val_loss: 1.1709 - val_accuracy: 0.7328\n",
      "Epoch 16/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3474 - accuracy: 0.9356\n",
      "Epoch 16: val_accuracy did not improve from 0.73280\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3476 - accuracy: 0.9355 - val_loss: 2.4854 - val_accuracy: 0.5659\n",
      "Epoch 17/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.9424\n",
      "Epoch 17: val_accuracy did not improve from 0.73280\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3350 - accuracy: 0.9424 - val_loss: 1.3308 - val_accuracy: 0.7127\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.9465\n",
      "Epoch 18: val_accuracy did not improve from 0.73280\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3245 - accuracy: 0.9465 - val_loss: 1.5139 - val_accuracy: 0.7028\n",
      "Epoch 19/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.9484\n",
      "Epoch 19: val_accuracy did not improve from 0.73280\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3218 - accuracy: 0.9484 - val_loss: 1.3034 - val_accuracy: 0.7121\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.9546\n",
      "Epoch 20: val_accuracy did not improve from 0.73280\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3058 - accuracy: 0.9546 - val_loss: 1.7804 - val_accuracy: 0.6890\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3062 - accuracy: 0.9541\n",
      "Epoch 21: val_accuracy improved from 0.73280 to 0.73510, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3063 - accuracy: 0.9541 - val_loss: 1.2882 - val_accuracy: 0.7351\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2991 - accuracy: 0.9566\n",
      "Epoch 22: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2996 - accuracy: 0.9565 - val_loss: 1.8553 - val_accuracy: 0.6613\n",
      "Epoch 23/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9593\n",
      "Epoch 23: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2957 - accuracy: 0.9593 - val_loss: 1.8809 - val_accuracy: 0.6917\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9550\n",
      "Epoch 24: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3071 - accuracy: 0.9550 - val_loss: 1.8497 - val_accuracy: 0.6902\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9640\n",
      "Epoch 25: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2855 - accuracy: 0.9640 - val_loss: 1.5942 - val_accuracy: 0.7325\n",
      "Epoch 26/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9626\n",
      "Epoch 26: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2903 - accuracy: 0.9626 - val_loss: 1.4316 - val_accuracy: 0.7318\n",
      "Epoch 27/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.9604\n",
      "Epoch 27: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2980 - accuracy: 0.9603 - val_loss: 1.5565 - val_accuracy: 0.7157\n",
      "Epoch 28/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9637\n",
      "Epoch 28: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2880 - accuracy: 0.9637 - val_loss: 2.1518 - val_accuracy: 0.6887\n",
      "Epoch 29/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.9669\n",
      "Epoch 29: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2809 - accuracy: 0.9669 - val_loss: 1.7146 - val_accuracy: 0.7059\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9644\n",
      "Epoch 30: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2864 - accuracy: 0.9644 - val_loss: 1.6510 - val_accuracy: 0.7098\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9647\n",
      "Epoch 31: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2863 - accuracy: 0.9646 - val_loss: 1.7410 - val_accuracy: 0.6921\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9668\n",
      "Epoch 32: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2845 - accuracy: 0.9668 - val_loss: 1.5839 - val_accuracy: 0.7092\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9676\n",
      "Epoch 33: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2800 - accuracy: 0.9676 - val_loss: 2.3062 - val_accuracy: 0.6385\n",
      "Epoch 34/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.9690\n",
      "Epoch 34: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2786 - accuracy: 0.9689 - val_loss: 1.7645 - val_accuracy: 0.7060\n",
      "Epoch 35/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.9663\n",
      "Epoch 35: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2865 - accuracy: 0.9663 - val_loss: 1.5139 - val_accuracy: 0.7311\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9692\n",
      "Epoch 36: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2777 - accuracy: 0.9692 - val_loss: 1.7695 - val_accuracy: 0.7081\n",
      "Epoch 37/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2860 - accuracy: 0.9669\n",
      "Epoch 37: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2860 - accuracy: 0.9669 - val_loss: 1.7206 - val_accuracy: 0.7140\n",
      "Epoch 38/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.9715\n",
      "Epoch 38: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2785 - accuracy: 0.9715 - val_loss: 2.1341 - val_accuracy: 0.6674\n",
      "Epoch 39/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2773 - accuracy: 0.9708\n",
      "Epoch 39: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2773 - accuracy: 0.9708 - val_loss: 1.5603 - val_accuracy: 0.7327\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9680\n",
      "Epoch 40: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2848 - accuracy: 0.9680 - val_loss: 1.8287 - val_accuracy: 0.6895\n",
      "Epoch 41/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9735\n",
      "Epoch 41: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2706 - accuracy: 0.9735 - val_loss: 1.6623 - val_accuracy: 0.7115\n",
      "Epoch 42/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9678\n",
      "Epoch 42: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2863 - accuracy: 0.9676 - val_loss: 1.8567 - val_accuracy: 0.6953\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.9697\n",
      "Epoch 43: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2817 - accuracy: 0.9697 - val_loss: 1.7985 - val_accuracy: 0.7170\n",
      "Epoch 44/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9758\n",
      "Epoch 44: val_accuracy did not improve from 0.73510\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2658 - accuracy: 0.9758 - val_loss: 1.7346 - val_accuracy: 0.7118\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9710\n",
      "Epoch 45: val_accuracy improved from 0.73510 to 0.75910, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2766 - accuracy: 0.9710 - val_loss: 1.3515 - val_accuracy: 0.7591\n",
      "Epoch 46/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.9704\n",
      "Epoch 46: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2794 - accuracy: 0.9703 - val_loss: 1.5353 - val_accuracy: 0.7412\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9738\n",
      "Epoch 47: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.2700 - accuracy: 0.9738 - val_loss: 1.7296 - val_accuracy: 0.6939\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.9704\n",
      "Epoch 48: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2827 - accuracy: 0.9704 - val_loss: 1.3463 - val_accuracy: 0.7591\n",
      "Epoch 49/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9740\n",
      "Epoch 49: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2711 - accuracy: 0.9740 - val_loss: 2.0841 - val_accuracy: 0.7063\n",
      "Epoch 50/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.9704\n",
      "Epoch 50: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2821 - accuracy: 0.9704 - val_loss: 1.6501 - val_accuracy: 0.7357\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9736\n",
      "Epoch 51: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2707 - accuracy: 0.9736 - val_loss: 1.7497 - val_accuracy: 0.6998\n",
      "Epoch 52/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.9751\n",
      "Epoch 52: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2681 - accuracy: 0.9751 - val_loss: 1.6040 - val_accuracy: 0.7048\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9684\n",
      "Epoch 53: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2837 - accuracy: 0.9685 - val_loss: 2.2473 - val_accuracy: 0.6521\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9755\n",
      "Epoch 54: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2682 - accuracy: 0.9755 - val_loss: 1.7860 - val_accuracy: 0.7278\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9768\n",
      "Epoch 55: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2634 - accuracy: 0.9768 - val_loss: 1.7076 - val_accuracy: 0.7307\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9691\n",
      "Epoch 56: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2841 - accuracy: 0.9691 - val_loss: 1.5273 - val_accuracy: 0.7445\n",
      "Epoch 57/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.9750\n",
      "Epoch 57: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2687 - accuracy: 0.9750 - val_loss: 2.0112 - val_accuracy: 0.6884\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9732\n",
      "Epoch 58: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2742 - accuracy: 0.9732 - val_loss: 1.5597 - val_accuracy: 0.7300\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9763\n",
      "Epoch 59: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2662 - accuracy: 0.9763 - val_loss: 1.6409 - val_accuracy: 0.7310\n",
      "Epoch 60/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.9749\n",
      "Epoch 60: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2686 - accuracy: 0.9749 - val_loss: 2.5391 - val_accuracy: 0.6148\n",
      "Epoch 61/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9718\n",
      "Epoch 61: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2763 - accuracy: 0.9718 - val_loss: 2.0694 - val_accuracy: 0.6899\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9775\n",
      "Epoch 62: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2609 - accuracy: 0.9775 - val_loss: 1.7720 - val_accuracy: 0.7110\n",
      "Epoch 63/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9723\n",
      "Epoch 63: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2763 - accuracy: 0.9723 - val_loss: 2.2886 - val_accuracy: 0.6552\n",
      "Epoch 64/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9802\n",
      "Epoch 64: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2545 - accuracy: 0.9802 - val_loss: 1.8040 - val_accuracy: 0.7243\n",
      "Epoch 65/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.9721\n",
      "Epoch 65: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2772 - accuracy: 0.9721 - val_loss: 1.8909 - val_accuracy: 0.7083\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.9727\n",
      "Epoch 66: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2755 - accuracy: 0.9727 - val_loss: 1.7623 - val_accuracy: 0.7281\n",
      "Epoch 67/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2678 - accuracy: 0.9759\n",
      "Epoch 67: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2681 - accuracy: 0.9759 - val_loss: 1.7598 - val_accuracy: 0.7039\n",
      "Epoch 68/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9786\n",
      "Epoch 68: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2601 - accuracy: 0.9786 - val_loss: 1.6752 - val_accuracy: 0.7309\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9747\n",
      "Epoch 69: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2696 - accuracy: 0.9747 - val_loss: 2.1856 - val_accuracy: 0.6799\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9752\n",
      "Epoch 70: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2667 - accuracy: 0.9752 - val_loss: 1.7955 - val_accuracy: 0.7067\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9743\n",
      "Epoch 71: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2706 - accuracy: 0.9743 - val_loss: 2.2928 - val_accuracy: 0.6445\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9778\n",
      "Epoch 72: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2603 - accuracy: 0.9778 - val_loss: 1.5241 - val_accuracy: 0.7438\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9759\n",
      "Epoch 73: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2659 - accuracy: 0.9759 - val_loss: 1.6546 - val_accuracy: 0.7258\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9725\n",
      "Epoch 74: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2748 - accuracy: 0.9725 - val_loss: 1.7696 - val_accuracy: 0.7268\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9752\n",
      "Epoch 75: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2649 - accuracy: 0.9752 - val_loss: 2.0599 - val_accuracy: 0.6831\n",
      "Epoch 76/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.9770\n",
      "Epoch 76: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2629 - accuracy: 0.9770 - val_loss: 1.6505 - val_accuracy: 0.7377\n",
      "Epoch 77/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.9752\n",
      "Epoch 77: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2661 - accuracy: 0.9752 - val_loss: 1.6254 - val_accuracy: 0.7384\n",
      "Epoch 78/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2622 - accuracy: 0.9774\n",
      "Epoch 78: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2623 - accuracy: 0.9773 - val_loss: 1.7119 - val_accuracy: 0.7179\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2674 - accuracy: 0.9751\n",
      "Epoch 79: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2674 - accuracy: 0.9751 - val_loss: 1.5519 - val_accuracy: 0.7506\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9780\n",
      "Epoch 80: val_accuracy did not improve from 0.75910\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2619 - accuracy: 0.9780 - val_loss: 1.5160 - val_accuracy: 0.7412\n",
      "Epoch 81/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9757\n",
      "Epoch 81: val_accuracy improved from 0.75910 to 0.76520, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2647 - accuracy: 0.9757 - val_loss: 1.3487 - val_accuracy: 0.7652\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9760\n",
      "Epoch 82: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2646 - accuracy: 0.9760 - val_loss: 2.3275 - val_accuracy: 0.6920\n",
      "Epoch 83/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9750\n",
      "Epoch 83: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2675 - accuracy: 0.9750 - val_loss: 1.4393 - val_accuracy: 0.7586\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9820\n",
      "Epoch 84: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2487 - accuracy: 0.9820 - val_loss: 1.5939 - val_accuracy: 0.7530\n",
      "Epoch 85/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9759\n",
      "Epoch 85: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2613 - accuracy: 0.9759 - val_loss: 1.6472 - val_accuracy: 0.7316\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9732\n",
      "Epoch 86: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2729 - accuracy: 0.9732 - val_loss: 1.7659 - val_accuracy: 0.7301\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9766\n",
      "Epoch 87: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2619 - accuracy: 0.9766 - val_loss: 1.6576 - val_accuracy: 0.7335\n",
      "Epoch 88/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.9764\n",
      "Epoch 88: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2638 - accuracy: 0.9764 - val_loss: 1.7090 - val_accuracy: 0.7191\n",
      "Epoch 89/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.9781\n",
      "Epoch 89: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2596 - accuracy: 0.9781 - val_loss: 2.1177 - val_accuracy: 0.6954\n",
      "Epoch 90/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9779\n",
      "Epoch 90: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2588 - accuracy: 0.9779 - val_loss: 1.4533 - val_accuracy: 0.7492\n",
      "Epoch 91/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9776\n",
      "Epoch 91: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2577 - accuracy: 0.9776 - val_loss: 2.1979 - val_accuracy: 0.6756\n",
      "Epoch 92/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9777\n",
      "Epoch 92: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2585 - accuracy: 0.9777 - val_loss: 1.9343 - val_accuracy: 0.7002\n",
      "Epoch 93/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.9757\n",
      "Epoch 93: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2646 - accuracy: 0.9756 - val_loss: 1.7455 - val_accuracy: 0.7226\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9754\n",
      "Epoch 94: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2635 - accuracy: 0.9754 - val_loss: 1.6542 - val_accuracy: 0.7291\n",
      "Epoch 95/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.9760\n",
      "Epoch 95: val_accuracy did not improve from 0.76520\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2628 - accuracy: 0.9760 - val_loss: 2.0275 - val_accuracy: 0.7097\n",
      "Epoch 96/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9811\n",
      "Epoch 96: val_accuracy improved from 0.76520 to 0.76540, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2509 - accuracy: 0.9811 - val_loss: 1.2986 - val_accuracy: 0.7654\n",
      "Epoch 97/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9768\n",
      "Epoch 97: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2602 - accuracy: 0.9768 - val_loss: 1.9971 - val_accuracy: 0.7101\n",
      "Epoch 98/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.9797\n",
      "Epoch 98: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2523 - accuracy: 0.9797 - val_loss: 1.5392 - val_accuracy: 0.7441\n",
      "Epoch 99/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9743\n",
      "Epoch 99: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2672 - accuracy: 0.9743 - val_loss: 1.6233 - val_accuracy: 0.7489\n",
      "Epoch 100/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9813\n",
      "Epoch 100: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2484 - accuracy: 0.9813 - val_loss: 1.9814 - val_accuracy: 0.7123\n",
      "Epoch 101/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9765\n",
      "Epoch 101: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2589 - accuracy: 0.9765 - val_loss: 2.4497 - val_accuracy: 0.6646\n",
      "Epoch 102/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9792\n",
      "Epoch 102: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2534 - accuracy: 0.9792 - val_loss: 1.3812 - val_accuracy: 0.7620\n",
      "Epoch 103/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9815\n",
      "Epoch 103: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2455 - accuracy: 0.9815 - val_loss: 1.4047 - val_accuracy: 0.7592\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9729\n",
      "Epoch 104: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2677 - accuracy: 0.9729 - val_loss: 1.5964 - val_accuracy: 0.7309\n",
      "Epoch 105/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9783\n",
      "Epoch 105: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2551 - accuracy: 0.9783 - val_loss: 1.7575 - val_accuracy: 0.7318\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2619 - accuracy: 0.9765\n",
      "Epoch 106: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2619 - accuracy: 0.9765 - val_loss: 2.0308 - val_accuracy: 0.6847\n",
      "Epoch 107/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9776\n",
      "Epoch 107: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2571 - accuracy: 0.9776 - val_loss: 1.4495 - val_accuracy: 0.7509\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9788\n",
      "Epoch 108: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2528 - accuracy: 0.9788 - val_loss: 1.8184 - val_accuracy: 0.7065\n",
      "Epoch 109/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9818\n",
      "Epoch 109: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2458 - accuracy: 0.9818 - val_loss: 1.7907 - val_accuracy: 0.7341\n",
      "Epoch 110/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9795\n",
      "Epoch 110: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2496 - accuracy: 0.9795 - val_loss: 1.9109 - val_accuracy: 0.6871\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9763\n",
      "Epoch 111: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2599 - accuracy: 0.9763 - val_loss: 1.5756 - val_accuracy: 0.7465\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9795\n",
      "Epoch 112: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2491 - accuracy: 0.9795 - val_loss: 2.1202 - val_accuracy: 0.6786\n",
      "Epoch 113/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9758\n",
      "Epoch 113: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2605 - accuracy: 0.9758 - val_loss: 1.5535 - val_accuracy: 0.7462\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.9769\n",
      "Epoch 114: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2581 - accuracy: 0.9769 - val_loss: 1.5566 - val_accuracy: 0.7274\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9796\n",
      "Epoch 115: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2499 - accuracy: 0.9796 - val_loss: 1.5967 - val_accuracy: 0.7421\n",
      "Epoch 116/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9791\n",
      "Epoch 116: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2532 - accuracy: 0.9791 - val_loss: 1.9866 - val_accuracy: 0.7261\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9787\n",
      "Epoch 117: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2528 - accuracy: 0.9787 - val_loss: 1.9649 - val_accuracy: 0.6897\n",
      "Epoch 118/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2542 - accuracy: 0.9781\n",
      "Epoch 118: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2542 - accuracy: 0.9781 - val_loss: 1.7009 - val_accuracy: 0.7270\n",
      "Epoch 119/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2471 - accuracy: 0.9806\n",
      "Epoch 119: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2472 - accuracy: 0.9805 - val_loss: 1.4763 - val_accuracy: 0.7617\n",
      "Epoch 120/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9786\n",
      "Epoch 120: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2537 - accuracy: 0.9786 - val_loss: 1.5241 - val_accuracy: 0.7543\n",
      "Epoch 121/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9808\n",
      "Epoch 121: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2440 - accuracy: 0.9808 - val_loss: 1.6289 - val_accuracy: 0.7414\n",
      "Epoch 122/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2479 - accuracy: 0.9798\n",
      "Epoch 122: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2479 - accuracy: 0.9798 - val_loss: 1.6210 - val_accuracy: 0.7235\n",
      "Epoch 123/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.9741\n",
      "Epoch 123: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2645 - accuracy: 0.9741 - val_loss: 1.5076 - val_accuracy: 0.7522\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9817\n",
      "Epoch 124: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2435 - accuracy: 0.9817 - val_loss: 2.2367 - val_accuracy: 0.7027\n",
      "Epoch 125/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2479 - accuracy: 0.9791\n",
      "Epoch 125: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2479 - accuracy: 0.9791 - val_loss: 1.6603 - val_accuracy: 0.7583\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9791\n",
      "Epoch 126: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2489 - accuracy: 0.9791 - val_loss: 1.8249 - val_accuracy: 0.7142\n",
      "Epoch 127/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9811\n",
      "Epoch 127: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2433 - accuracy: 0.9810 - val_loss: 1.3818 - val_accuracy: 0.7619\n",
      "Epoch 128/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9779\n",
      "Epoch 128: val_accuracy did not improve from 0.76540\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2515 - accuracy: 0.9779 - val_loss: 2.1500 - val_accuracy: 0.6869\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9799\n",
      "Epoch 129: val_accuracy improved from 0.76540 to 0.77050, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2490 - accuracy: 0.9799 - val_loss: 1.4885 - val_accuracy: 0.7705\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9811\n",
      "Epoch 130: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2435 - accuracy: 0.9811 - val_loss: 1.7827 - val_accuracy: 0.7189\n",
      "Epoch 131/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2500 - accuracy: 0.9780\n",
      "Epoch 131: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2501 - accuracy: 0.9779 - val_loss: 1.4807 - val_accuracy: 0.7522\n",
      "Epoch 132/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9787\n",
      "Epoch 132: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2494 - accuracy: 0.9786 - val_loss: 1.7783 - val_accuracy: 0.7265\n",
      "Epoch 133/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9806\n",
      "Epoch 133: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2431 - accuracy: 0.9806 - val_loss: 1.7334 - val_accuracy: 0.7360\n",
      "Epoch 134/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9799\n",
      "Epoch 134: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2466 - accuracy: 0.9799 - val_loss: 1.4338 - val_accuracy: 0.7572\n",
      "Epoch 135/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9793\n",
      "Epoch 135: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2455 - accuracy: 0.9793 - val_loss: 1.5790 - val_accuracy: 0.7555\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2512 - accuracy: 0.9774\n",
      "Epoch 136: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2512 - accuracy: 0.9774 - val_loss: 1.6599 - val_accuracy: 0.7370\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9801\n",
      "Epoch 137: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2448 - accuracy: 0.9801 - val_loss: 1.5160 - val_accuracy: 0.7600\n",
      "Epoch 138/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9804\n",
      "Epoch 138: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2426 - accuracy: 0.9804 - val_loss: 1.5653 - val_accuracy: 0.7587\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9779\n",
      "Epoch 139: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2508 - accuracy: 0.9779 - val_loss: 1.7661 - val_accuracy: 0.7284\n",
      "Epoch 140/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2525 - accuracy: 0.9766\n",
      "Epoch 140: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2525 - accuracy: 0.9766 - val_loss: 1.8093 - val_accuracy: 0.7277\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9817\n",
      "Epoch 141: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2416 - accuracy: 0.9817 - val_loss: 1.6148 - val_accuracy: 0.7331\n",
      "Epoch 142/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2374 - accuracy: 0.9831\n",
      "Epoch 142: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2374 - accuracy: 0.9831 - val_loss: 1.5477 - val_accuracy: 0.7514\n",
      "Epoch 143/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9783\n",
      "Epoch 143: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2491 - accuracy: 0.9784 - val_loss: 1.5009 - val_accuracy: 0.7637\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9816\n",
      "Epoch 144: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2394 - accuracy: 0.9816 - val_loss: 1.7160 - val_accuracy: 0.7260\n",
      "Epoch 145/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2475 - accuracy: 0.9788\n",
      "Epoch 145: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2475 - accuracy: 0.9788 - val_loss: 1.7378 - val_accuracy: 0.7356\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9797\n",
      "Epoch 146: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2471 - accuracy: 0.9797 - val_loss: 1.9340 - val_accuracy: 0.7185\n",
      "Epoch 147/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2384 - accuracy: 0.9816\n",
      "Epoch 147: val_accuracy did not improve from 0.77050\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2385 - accuracy: 0.9816 - val_loss: 1.6594 - val_accuracy: 0.7372\n",
      "Epoch 148/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9811\n",
      "Epoch 148: val_accuracy improved from 0.77050 to 0.78560, saving model to results\\pruned_resnet20_SNIP_sparsity_0.7.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2417 - accuracy: 0.9811 - val_loss: 1.2696 - val_accuracy: 0.7856\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9804\n",
      "Epoch 149: val_accuracy did not improve from 0.78560\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2406 - accuracy: 0.9804 - val_loss: 1.7106 - val_accuracy: 0.7307\n",
      "Epoch 150/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9802\n",
      "Epoch 150: val_accuracy did not improve from 0.78560\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2422 - accuracy: 0.9802 - val_loss: 1.4114 - val_accuracy: 0.7668\n",
      "Training complete. Best model saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b64a18430>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.7, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918404b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   448         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 16)  64          ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['activation_57[0][0]',          \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 32, 32, 16)   0           ['activation_59[0][0]',          \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 16)   0           ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 32, 32, 16)   0           ['activation_61[0][0]',          \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   544         ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 32)  128         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 32)   0           ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['activation_65[0][0]',          \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 32)   0           ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 16, 16, 32)  128         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 16, 16, 32)  128         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['activation_67[0][0]',          \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 8, 8, 64)     0           ['conv2d_79[0][0]',              \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 64)     0           ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 8, 8, 64)    256         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 8, 8, 64)     0           ['activation_71[0][0]',          \n",
      "                                                                  'batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 8, 8, 64)     0           ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 8, 8, 64)    256         ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['activation_73[0][0]',          \n",
      "                                                                  'batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 64)     0           ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 64)          0           ['activation_75[0][0]']          \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           650         ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.5643 - accuracy: 0.4937\n",
      "Epoch 1: val_accuracy improved from -inf to 0.44060, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 32s 76ms/step - loss: 1.5643 - accuracy: 0.4937 - val_loss: 1.8419 - val_accuracy: 0.4406\n",
      "Epoch 2/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1489 - accuracy: 0.6464\n",
      "Epoch 2: val_accuracy improved from 0.44060 to 0.62490, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.1489 - accuracy: 0.6464 - val_loss: 1.2772 - val_accuracy: 0.6249\n",
      "Epoch 3/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9506 - accuracy: 0.7183\n",
      "Epoch 3: val_accuracy did not improve from 0.62490\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9506 - accuracy: 0.7183 - val_loss: 1.3227 - val_accuracy: 0.6040\n",
      "Epoch 4/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.7598\n",
      "Epoch 4: val_accuracy improved from 0.62490 to 0.62630, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.8362 - accuracy: 0.7598 - val_loss: 1.2449 - val_accuracy: 0.6263\n",
      "Epoch 5/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.7894\n",
      "Epoch 5: val_accuracy improved from 0.62630 to 0.67670, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.7494 - accuracy: 0.7894 - val_loss: 1.1280 - val_accuracy: 0.6767\n",
      "Epoch 6/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.8134\n",
      "Epoch 6: val_accuracy did not improve from 0.67670\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6844 - accuracy: 0.8134 - val_loss: 1.3491 - val_accuracy: 0.6451\n",
      "Epoch 7/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.8392\n",
      "Epoch 7: val_accuracy did not improve from 0.67670\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6169 - accuracy: 0.8392 - val_loss: 1.2540 - val_accuracy: 0.6401\n",
      "Epoch 8/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.8564\n",
      "Epoch 8: val_accuracy improved from 0.67670 to 0.67920, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.5686 - accuracy: 0.8564 - val_loss: 1.2123 - val_accuracy: 0.6792\n",
      "Epoch 9/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8712\n",
      "Epoch 9: val_accuracy improved from 0.67920 to 0.70140, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.5246 - accuracy: 0.8712 - val_loss: 1.1488 - val_accuracy: 0.7014\n",
      "Epoch 10/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.8867\n",
      "Epoch 10: val_accuracy did not improve from 0.70140\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4827 - accuracy: 0.8867 - val_loss: 1.9417 - val_accuracy: 0.5850\n",
      "Epoch 11/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.8991\n",
      "Epoch 11: val_accuracy did not improve from 0.70140\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4510 - accuracy: 0.8991 - val_loss: 1.4358 - val_accuracy: 0.6686\n",
      "Epoch 12/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.9103\n",
      "Epoch 12: val_accuracy improved from 0.70140 to 0.72560, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.4206 - accuracy: 0.9103 - val_loss: 1.0815 - val_accuracy: 0.7256\n",
      "Epoch 13/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3919 - accuracy: 0.9218\n",
      "Epoch 13: val_accuracy improved from 0.72560 to 0.72810, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3919 - accuracy: 0.9218 - val_loss: 1.1997 - val_accuracy: 0.7281\n",
      "Epoch 14/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.9282\n",
      "Epoch 14: val_accuracy did not improve from 0.72810\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.3736 - accuracy: 0.9282 - val_loss: 1.3273 - val_accuracy: 0.7098\n",
      "Epoch 15/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.9369\n",
      "Epoch 15: val_accuracy did not improve from 0.72810\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3539 - accuracy: 0.9369 - val_loss: 2.0964 - val_accuracy: 0.5965\n",
      "Epoch 16/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.9429\n",
      "Epoch 16: val_accuracy improved from 0.72810 to 0.74790, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.3377 - accuracy: 0.9429 - val_loss: 1.1450 - val_accuracy: 0.7479\n",
      "Epoch 17/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9453\n",
      "Epoch 17: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3291 - accuracy: 0.9453 - val_loss: 1.3173 - val_accuracy: 0.7287\n",
      "Epoch 18/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.9487\n",
      "Epoch 18: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3222 - accuracy: 0.9487 - val_loss: 1.2341 - val_accuracy: 0.7336\n",
      "Epoch 19/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3163 - accuracy: 0.9517\n",
      "Epoch 19: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3163 - accuracy: 0.9517 - val_loss: 1.5820 - val_accuracy: 0.7011\n",
      "Epoch 20/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9555\n",
      "Epoch 20: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3071 - accuracy: 0.9555 - val_loss: 1.7966 - val_accuracy: 0.6730\n",
      "Epoch 21/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.9579\n",
      "Epoch 21: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3011 - accuracy: 0.9578 - val_loss: 1.6246 - val_accuracy: 0.6906\n",
      "Epoch 22/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.9567\n",
      "Epoch 22: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3036 - accuracy: 0.9566 - val_loss: 1.8347 - val_accuracy: 0.6833\n",
      "Epoch 23/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2965 - accuracy: 0.9601\n",
      "Epoch 23: val_accuracy did not improve from 0.74790\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2965 - accuracy: 0.9601 - val_loss: 1.5796 - val_accuracy: 0.6950\n",
      "Epoch 24/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.9633\n",
      "Epoch 24: val_accuracy improved from 0.74790 to 0.74840, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2892 - accuracy: 0.9633 - val_loss: 1.3793 - val_accuracy: 0.7484\n",
      "Epoch 25/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.9621\n",
      "Epoch 25: val_accuracy improved from 0.74840 to 0.74930, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2952 - accuracy: 0.9621 - val_loss: 1.2522 - val_accuracy: 0.7493\n",
      "Epoch 26/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2886 - accuracy: 0.9634\n",
      "Epoch 26: val_accuracy improved from 0.74930 to 0.75130, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2886 - accuracy: 0.9634 - val_loss: 1.3241 - val_accuracy: 0.7513\n",
      "Epoch 27/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9606\n",
      "Epoch 27: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2974 - accuracy: 0.9606 - val_loss: 1.5147 - val_accuracy: 0.7149\n",
      "Epoch 28/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2764 - accuracy: 0.9699\n",
      "Epoch 28: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2765 - accuracy: 0.9699 - val_loss: 1.7927 - val_accuracy: 0.6810\n",
      "Epoch 29/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9655\n",
      "Epoch 29: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2868 - accuracy: 0.9655 - val_loss: 1.8428 - val_accuracy: 0.6984\n",
      "Epoch 30/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9626\n",
      "Epoch 30: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2969 - accuracy: 0.9626 - val_loss: 1.3983 - val_accuracy: 0.7425\n",
      "Epoch 31/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2816 - accuracy: 0.9687\n",
      "Epoch 31: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2815 - accuracy: 0.9687 - val_loss: 1.9155 - val_accuracy: 0.6961\n",
      "Epoch 32/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9656\n",
      "Epoch 32: val_accuracy did not improve from 0.75130\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2883 - accuracy: 0.9656 - val_loss: 1.6196 - val_accuracy: 0.7126\n",
      "Epoch 33/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9699\n",
      "Epoch 33: val_accuracy improved from 0.75130 to 0.75250, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2779 - accuracy: 0.9699 - val_loss: 1.3496 - val_accuracy: 0.7525\n",
      "Epoch 34/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9648\n",
      "Epoch 34: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2925 - accuracy: 0.9648 - val_loss: 1.4261 - val_accuracy: 0.7432\n",
      "Epoch 35/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9674\n",
      "Epoch 35: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2866 - accuracy: 0.9674 - val_loss: 1.6865 - val_accuracy: 0.7063\n",
      "Epoch 36/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9717\n",
      "Epoch 36: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2763 - accuracy: 0.9717 - val_loss: 1.6412 - val_accuracy: 0.7317\n",
      "Epoch 37/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.9737\n",
      "Epoch 37: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2708 - accuracy: 0.9736 - val_loss: 1.8743 - val_accuracy: 0.7033\n",
      "Epoch 38/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9636\n",
      "Epoch 38: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2973 - accuracy: 0.9636 - val_loss: 1.5089 - val_accuracy: 0.7253\n",
      "Epoch 39/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.9719\n",
      "Epoch 39: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2759 - accuracy: 0.9719 - val_loss: 1.6475 - val_accuracy: 0.7223\n",
      "Epoch 40/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9723\n",
      "Epoch 40: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2750 - accuracy: 0.9723 - val_loss: 1.7137 - val_accuracy: 0.7270\n",
      "Epoch 41/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9723\n",
      "Epoch 41: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2767 - accuracy: 0.9723 - val_loss: 1.7518 - val_accuracy: 0.6920\n",
      "Epoch 42/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.9685\n",
      "Epoch 42: val_accuracy did not improve from 0.75250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2851 - accuracy: 0.9685 - val_loss: 1.7393 - val_accuracy: 0.6955\n",
      "Epoch 43/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.9719\n",
      "Epoch 43: val_accuracy improved from 0.75250 to 0.76250, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2748 - accuracy: 0.9719 - val_loss: 1.3752 - val_accuracy: 0.7625\n",
      "Epoch 44/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9716\n",
      "Epoch 44: val_accuracy did not improve from 0.76250\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2768 - accuracy: 0.9715 - val_loss: 1.9832 - val_accuracy: 0.6801\n",
      "Epoch 45/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9704\n",
      "Epoch 45: val_accuracy did not improve from 0.76250\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2822 - accuracy: 0.9704 - val_loss: 1.5171 - val_accuracy: 0.7432\n",
      "Epoch 46/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.9722\n",
      "Epoch 46: val_accuracy improved from 0.76250 to 0.76310, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2777 - accuracy: 0.9722 - val_loss: 1.3969 - val_accuracy: 0.7631\n",
      "Epoch 47/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9757\n",
      "Epoch 47: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2677 - accuracy: 0.9757 - val_loss: 1.8605 - val_accuracy: 0.7054\n",
      "Epoch 48/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9729\n",
      "Epoch 48: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2733 - accuracy: 0.9729 - val_loss: 2.3801 - val_accuracy: 0.6615\n",
      "Epoch 49/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9686\n",
      "Epoch 49: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2863 - accuracy: 0.9686 - val_loss: 2.0071 - val_accuracy: 0.6959\n",
      "Epoch 50/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9751\n",
      "Epoch 50: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2698 - accuracy: 0.9751 - val_loss: 2.3345 - val_accuracy: 0.6682\n",
      "Epoch 51/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.9738\n",
      "Epoch 51: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2721 - accuracy: 0.9738 - val_loss: 1.7363 - val_accuracy: 0.7165\n",
      "Epoch 52/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2749 - accuracy: 0.9727\n",
      "Epoch 52: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2749 - accuracy: 0.9727 - val_loss: 1.7368 - val_accuracy: 0.7168\n",
      "Epoch 53/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.9757\n",
      "Epoch 53: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2683 - accuracy: 0.9756 - val_loss: 1.6436 - val_accuracy: 0.7220\n",
      "Epoch 54/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9736\n",
      "Epoch 54: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2729 - accuracy: 0.9736 - val_loss: 1.6975 - val_accuracy: 0.7192\n",
      "Epoch 55/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9730\n",
      "Epoch 55: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2750 - accuracy: 0.9730 - val_loss: 1.9323 - val_accuracy: 0.6966\n",
      "Epoch 56/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9757\n",
      "Epoch 56: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2699 - accuracy: 0.9757 - val_loss: 2.9050 - val_accuracy: 0.6321\n",
      "Epoch 57/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2704 - accuracy: 0.9746\n",
      "Epoch 57: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2705 - accuracy: 0.9746 - val_loss: 1.4568 - val_accuracy: 0.7586\n",
      "Epoch 58/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.9747\n",
      "Epoch 58: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2711 - accuracy: 0.9747 - val_loss: 1.4343 - val_accuracy: 0.7524\n",
      "Epoch 59/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9734\n",
      "Epoch 59: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2767 - accuracy: 0.9734 - val_loss: 1.4338 - val_accuracy: 0.7542\n",
      "Epoch 60/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9772\n",
      "Epoch 60: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2660 - accuracy: 0.9772 - val_loss: 1.9641 - val_accuracy: 0.7072\n",
      "Epoch 61/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2704 - accuracy: 0.9751\n",
      "Epoch 61: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2705 - accuracy: 0.9751 - val_loss: 1.5777 - val_accuracy: 0.7430\n",
      "Epoch 62/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9745\n",
      "Epoch 62: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2722 - accuracy: 0.9745 - val_loss: 1.5448 - val_accuracy: 0.7381\n",
      "Epoch 63/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9753\n",
      "Epoch 63: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2705 - accuracy: 0.9753 - val_loss: 1.5178 - val_accuracy: 0.7445\n",
      "Epoch 64/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9760\n",
      "Epoch 64: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2675 - accuracy: 0.9760 - val_loss: 1.6911 - val_accuracy: 0.7373\n",
      "Epoch 65/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9754\n",
      "Epoch 65: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2686 - accuracy: 0.9754 - val_loss: 1.5905 - val_accuracy: 0.7337\n",
      "Epoch 66/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9755\n",
      "Epoch 66: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2681 - accuracy: 0.9755 - val_loss: 1.8551 - val_accuracy: 0.7229\n",
      "Epoch 67/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9751\n",
      "Epoch 67: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2686 - accuracy: 0.9751 - val_loss: 1.8500 - val_accuracy: 0.7153\n",
      "Epoch 68/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2699 - accuracy: 0.9747\n",
      "Epoch 68: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2702 - accuracy: 0.9746 - val_loss: 1.6383 - val_accuracy: 0.7396\n",
      "Epoch 69/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9753\n",
      "Epoch 69: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2699 - accuracy: 0.9753 - val_loss: 1.4088 - val_accuracy: 0.7588\n",
      "Epoch 70/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9756\n",
      "Epoch 70: val_accuracy did not improve from 0.76310\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2671 - accuracy: 0.9756 - val_loss: 1.4875 - val_accuracy: 0.7296\n",
      "Epoch 71/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9778\n",
      "Epoch 71: val_accuracy improved from 0.76310 to 0.77000, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2618 - accuracy: 0.9778 - val_loss: 1.3646 - val_accuracy: 0.7700\n",
      "Epoch 72/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.9765\n",
      "Epoch 72: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2641 - accuracy: 0.9765 - val_loss: 1.6060 - val_accuracy: 0.7428\n",
      "Epoch 73/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9753\n",
      "Epoch 73: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2681 - accuracy: 0.9753 - val_loss: 1.4871 - val_accuracy: 0.7522\n",
      "Epoch 74/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.9755\n",
      "Epoch 74: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2679 - accuracy: 0.9755 - val_loss: 1.4328 - val_accuracy: 0.7553\n",
      "Epoch 75/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9744\n",
      "Epoch 75: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2724 - accuracy: 0.9744 - val_loss: 1.5001 - val_accuracy: 0.7477\n",
      "Epoch 76/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9796\n",
      "Epoch 76: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2580 - accuracy: 0.9796 - val_loss: 1.6930 - val_accuracy: 0.7253\n",
      "Epoch 77/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.9768\n",
      "Epoch 77: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2621 - accuracy: 0.9768 - val_loss: 1.4249 - val_accuracy: 0.7607\n",
      "Epoch 78/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9739\n",
      "Epoch 78: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2753 - accuracy: 0.9739 - val_loss: 2.1012 - val_accuracy: 0.6886\n",
      "Epoch 79/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9784\n",
      "Epoch 79: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2611 - accuracy: 0.9784 - val_loss: 1.7398 - val_accuracy: 0.7274\n",
      "Epoch 80/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9784\n",
      "Epoch 80: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2590 - accuracy: 0.9784 - val_loss: 1.8233 - val_accuracy: 0.7304\n",
      "Epoch 81/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9735\n",
      "Epoch 81: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2716 - accuracy: 0.9735 - val_loss: 1.5696 - val_accuracy: 0.7463\n",
      "Epoch 82/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9782\n",
      "Epoch 82: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2615 - accuracy: 0.9782 - val_loss: 1.5347 - val_accuracy: 0.7625\n",
      "Epoch 83/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9806\n",
      "Epoch 83: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2550 - accuracy: 0.9806 - val_loss: 1.4042 - val_accuracy: 0.7609\n",
      "Epoch 84/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9766\n",
      "Epoch 84: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2625 - accuracy: 0.9766 - val_loss: 1.7688 - val_accuracy: 0.7222\n",
      "Epoch 85/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9750\n",
      "Epoch 85: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2709 - accuracy: 0.9750 - val_loss: 2.1431 - val_accuracy: 0.7041\n",
      "Epoch 86/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2541 - accuracy: 0.9798\n",
      "Epoch 86: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2541 - accuracy: 0.9798 - val_loss: 1.7351 - val_accuracy: 0.7317\n",
      "Epoch 87/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9766\n",
      "Epoch 87: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2647 - accuracy: 0.9766 - val_loss: 1.6635 - val_accuracy: 0.7402\n",
      "Epoch 88/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9757\n",
      "Epoch 88: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2665 - accuracy: 0.9757 - val_loss: 1.8501 - val_accuracy: 0.7250\n",
      "Epoch 89/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2636 - accuracy: 0.9763\n",
      "Epoch 89: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2636 - accuracy: 0.9763 - val_loss: 1.2827 - val_accuracy: 0.7687\n",
      "Epoch 90/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9790\n",
      "Epoch 90: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2572 - accuracy: 0.9790 - val_loss: 1.5136 - val_accuracy: 0.7617\n",
      "Epoch 91/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9789\n",
      "Epoch 91: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2576 - accuracy: 0.9789 - val_loss: 1.5693 - val_accuracy: 0.7442\n",
      "Epoch 92/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9762\n",
      "Epoch 92: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2634 - accuracy: 0.9762 - val_loss: 1.5795 - val_accuracy: 0.7506\n",
      "Epoch 93/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.9777\n",
      "Epoch 93: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2606 - accuracy: 0.9777 - val_loss: 1.7050 - val_accuracy: 0.7379\n",
      "Epoch 94/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9803\n",
      "Epoch 94: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2526 - accuracy: 0.9803 - val_loss: 1.4883 - val_accuracy: 0.7646\n",
      "Epoch 95/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.9767\n",
      "Epoch 95: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2621 - accuracy: 0.9767 - val_loss: 1.8281 - val_accuracy: 0.7343\n",
      "Epoch 96/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9750\n",
      "Epoch 96: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2682 - accuracy: 0.9750 - val_loss: 1.7699 - val_accuracy: 0.7237\n",
      "Epoch 97/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2471 - accuracy: 0.9826\n",
      "Epoch 97: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2471 - accuracy: 0.9826 - val_loss: 1.5030 - val_accuracy: 0.7595\n",
      "Epoch 98/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.9784\n",
      "Epoch 98: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2554 - accuracy: 0.9784 - val_loss: 1.8110 - val_accuracy: 0.7311\n",
      "Epoch 99/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9776\n",
      "Epoch 99: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2589 - accuracy: 0.9776 - val_loss: 1.6176 - val_accuracy: 0.7437\n",
      "Epoch 100/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9789\n",
      "Epoch 100: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2543 - accuracy: 0.9789 - val_loss: 1.8415 - val_accuracy: 0.6992\n",
      "Epoch 101/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2657 - accuracy: 0.9755\n",
      "Epoch 101: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2659 - accuracy: 0.9754 - val_loss: 1.5008 - val_accuracy: 0.7434\n",
      "Epoch 102/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9781\n",
      "Epoch 102: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2569 - accuracy: 0.9781 - val_loss: 1.6170 - val_accuracy: 0.7442\n",
      "Epoch 103/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.9790\n",
      "Epoch 103: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2562 - accuracy: 0.9790 - val_loss: 1.5191 - val_accuracy: 0.7657\n",
      "Epoch 104/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9812\n",
      "Epoch 104: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2514 - accuracy: 0.9812 - val_loss: 1.8358 - val_accuracy: 0.7383\n",
      "Epoch 105/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9783\n",
      "Epoch 105: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2567 - accuracy: 0.9783 - val_loss: 1.4212 - val_accuracy: 0.7672\n",
      "Epoch 106/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9763\n",
      "Epoch 106: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2593 - accuracy: 0.9763 - val_loss: 1.4214 - val_accuracy: 0.7621\n",
      "Epoch 107/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2566 - accuracy: 0.9781\n",
      "Epoch 107: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2567 - accuracy: 0.9780 - val_loss: 1.7991 - val_accuracy: 0.7343\n",
      "Epoch 108/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9775\n",
      "Epoch 108: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2571 - accuracy: 0.9775 - val_loss: 1.6643 - val_accuracy: 0.7276\n",
      "Epoch 109/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2568 - accuracy: 0.9777\n",
      "Epoch 109: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2568 - accuracy: 0.9777 - val_loss: 1.8183 - val_accuracy: 0.7301\n",
      "Epoch 110/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2573 - accuracy: 0.9775\n",
      "Epoch 110: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2572 - accuracy: 0.9776 - val_loss: 1.6844 - val_accuracy: 0.7286\n",
      "Epoch 111/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9803\n",
      "Epoch 111: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2508 - accuracy: 0.9803 - val_loss: 1.9134 - val_accuracy: 0.7298\n",
      "Epoch 112/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9791\n",
      "Epoch 112: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2534 - accuracy: 0.9791 - val_loss: 1.6028 - val_accuracy: 0.7351\n",
      "Epoch 113/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.9784\n",
      "Epoch 113: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2564 - accuracy: 0.9784 - val_loss: 1.8944 - val_accuracy: 0.7058\n",
      "Epoch 114/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9804\n",
      "Epoch 114: val_accuracy did not improve from 0.77000\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2495 - accuracy: 0.9804 - val_loss: 1.6041 - val_accuracy: 0.7591\n",
      "Epoch 115/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9798\n",
      "Epoch 115: val_accuracy improved from 0.77000 to 0.77410, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.2497 - accuracy: 0.9798 - val_loss: 1.4197 - val_accuracy: 0.7741\n",
      "Epoch 116/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9766\n",
      "Epoch 116: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2595 - accuracy: 0.9767 - val_loss: 1.6593 - val_accuracy: 0.7142\n",
      "Epoch 117/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9809\n",
      "Epoch 117: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2483 - accuracy: 0.9809 - val_loss: 1.6579 - val_accuracy: 0.7541\n",
      "Epoch 118/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9792\n",
      "Epoch 118: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2506 - accuracy: 0.9791 - val_loss: 2.0383 - val_accuracy: 0.7238\n",
      "Epoch 119/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9790\n",
      "Epoch 119: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2529 - accuracy: 0.9790 - val_loss: 1.6336 - val_accuracy: 0.7302\n",
      "Epoch 120/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.9773\n",
      "Epoch 120: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2577 - accuracy: 0.9773 - val_loss: 1.6837 - val_accuracy: 0.7493\n",
      "Epoch 121/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9806\n",
      "Epoch 121: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2489 - accuracy: 0.9806 - val_loss: 1.5473 - val_accuracy: 0.7469\n",
      "Epoch 122/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2465 - accuracy: 0.9814\n",
      "Epoch 122: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2464 - accuracy: 0.9814 - val_loss: 1.4058 - val_accuracy: 0.7714\n",
      "Epoch 123/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9768\n",
      "Epoch 123: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2562 - accuracy: 0.9768 - val_loss: 1.9535 - val_accuracy: 0.7245\n",
      "Epoch 124/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2469 - accuracy: 0.9815\n",
      "Epoch 124: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2469 - accuracy: 0.9815 - val_loss: 1.5906 - val_accuracy: 0.7579\n",
      "Epoch 125/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9790\n",
      "Epoch 125: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2512 - accuracy: 0.9789 - val_loss: 1.4868 - val_accuracy: 0.7469\n",
      "Epoch 126/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9795\n",
      "Epoch 126: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2509 - accuracy: 0.9795 - val_loss: 1.5045 - val_accuracy: 0.7557\n",
      "Epoch 127/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9784\n",
      "Epoch 127: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2515 - accuracy: 0.9784 - val_loss: 1.6782 - val_accuracy: 0.7393\n",
      "Epoch 128/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9764\n",
      "Epoch 128: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2589 - accuracy: 0.9764 - val_loss: 1.4857 - val_accuracy: 0.7650\n",
      "Epoch 129/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9825\n",
      "Epoch 129: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2421 - accuracy: 0.9825 - val_loss: 1.3942 - val_accuracy: 0.7548\n",
      "Epoch 130/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.9819\n",
      "Epoch 130: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2420 - accuracy: 0.9819 - val_loss: 1.4111 - val_accuracy: 0.7693\n",
      "Epoch 131/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9787\n",
      "Epoch 131: val_accuracy did not improve from 0.77410\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2506 - accuracy: 0.9787 - val_loss: 2.3671 - val_accuracy: 0.6815\n",
      "Epoch 132/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9748\n",
      "Epoch 132: val_accuracy improved from 0.77410 to 0.77500, saving model to results\\pruned_resnet20_SNIP_sparsity_0.9.h5\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2609 - accuracy: 0.9748 - val_loss: 1.5474 - val_accuracy: 0.7750\n",
      "Epoch 133/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2439 - accuracy: 0.9816\n",
      "Epoch 133: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2439 - accuracy: 0.9816 - val_loss: 1.7168 - val_accuracy: 0.7417\n",
      "Epoch 134/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9847\n",
      "Epoch 134: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2343 - accuracy: 0.9846 - val_loss: 1.5730 - val_accuracy: 0.7508\n",
      "Epoch 135/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9775\n",
      "Epoch 135: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2554 - accuracy: 0.9775 - val_loss: 1.9292 - val_accuracy: 0.7065\n",
      "Epoch 136/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9803\n",
      "Epoch 136: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2456 - accuracy: 0.9803 - val_loss: 1.5216 - val_accuracy: 0.7524\n",
      "Epoch 137/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9793\n",
      "Epoch 137: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2496 - accuracy: 0.9793 - val_loss: 1.4536 - val_accuracy: 0.7534\n",
      "Epoch 138/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2436 - accuracy: 0.9813\n",
      "Epoch 138: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2436 - accuracy: 0.9813 - val_loss: 1.5915 - val_accuracy: 0.7504\n",
      "Epoch 139/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9787\n",
      "Epoch 139: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2501 - accuracy: 0.9787 - val_loss: 1.9167 - val_accuracy: 0.7225\n",
      "Epoch 140/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.9777\n",
      "Epoch 140: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2527 - accuracy: 0.9777 - val_loss: 1.3976 - val_accuracy: 0.7750\n",
      "Epoch 141/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9813\n",
      "Epoch 141: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2426 - accuracy: 0.9813 - val_loss: 1.4966 - val_accuracy: 0.7712\n",
      "Epoch 142/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9800\n",
      "Epoch 142: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2461 - accuracy: 0.9800 - val_loss: 1.5441 - val_accuracy: 0.7482\n",
      "Epoch 143/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9819\n",
      "Epoch 143: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2405 - accuracy: 0.9819 - val_loss: 2.1580 - val_accuracy: 0.6965\n",
      "Epoch 144/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9784\n",
      "Epoch 144: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2499 - accuracy: 0.9784 - val_loss: 1.9503 - val_accuracy: 0.7138\n",
      "Epoch 145/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9783\n",
      "Epoch 145: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2521 - accuracy: 0.9783 - val_loss: 1.6692 - val_accuracy: 0.7468\n",
      "Epoch 146/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9827\n",
      "Epoch 146: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2377 - accuracy: 0.9827 - val_loss: 1.7069 - val_accuracy: 0.7316\n",
      "Epoch 147/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2428 - accuracy: 0.9806\n",
      "Epoch 147: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2428 - accuracy: 0.9805 - val_loss: 1.7567 - val_accuracy: 0.7362\n",
      "Epoch 148/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9794\n",
      "Epoch 148: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2457 - accuracy: 0.9793 - val_loss: 1.5233 - val_accuracy: 0.7537\n",
      "Epoch 149/150\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9790\n",
      "Epoch 149: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2488 - accuracy: 0.9790 - val_loss: 1.5632 - val_accuracy: 0.7415\n",
      "Epoch 150/150\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9831\n",
      "Epoch 150: val_accuracy did not improve from 0.77500\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.2373 - accuracy: 0.9831 - val_loss: 1.5815 - val_accuracy: 0.7625\n",
      "Training complete. Best model saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b6409ac70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_snip_training(sparsity=0.9, batch_size=128, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e6e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b144343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
